
1. 在Pandas的DataFrame中可以调用Info方法，查看DataFrame的一些信息，在pyspark的DataFrame对象上也可以使用describe方法查看基础的统计量。在机器学习模块中，对于向量或矩阵上的基础统计信息也提供了一些基础的方法。可以使用基于列的方法colStats()来攻击矩阵上的一些常见的信息值。这些值包括：最大值、最小值、均值、方差、非空数据个数、总的数据等信息，这个方法返回MultivariateStatisticalSummary对象

~~~python
mat = sc.parallelize([
    np.array([1.0, 10.0, 100.0]),
    np.array([2.0, 20.0, 200.0]),
    np.array([3.0, 30.0, 300.0])
], 3)
summary = Statistics.colStats(mat)
dir(summary)
>>> 
['__class__','__del__','__delattr__','__dict__','__dir__','__doc__','__eq__','__format__','__ge__','__getattribute__','__gt__','__hash__','__init__','__init_subclass__','__le__','__lt__','__module__','__ne__','__new__','__reduce__','__reduce_ex__','__repr__','__setattr__','__sizeof__','__str__','__subclasshook__','__weakref__','_java_model','_sc','call','count','max','mean','min','normL1','normL2','numNonzeros','variance']
summary.min()
>>>  array([  1.,  10., 100.])
summary.max()
>>>  array([  3.,  30., 300.])
summary.mean()
>>>  array([  2.,  20., 200.])
summary.count()
>>>  3
summary.numNonzeros()
>>>  array([3., 3., 3.])
summary.variance()
>>>  array([1.e+00, 1.e+02, 1.e+04])
summary.normL1()
>>>  array([  6.,  60., 600.])
summary.normL2()
>>> array([  3.74165739,  37.41657387, 374.16573868])
~~~
可以看到这个对象很多方法，例如可以通过normL1得到L1正则项，通过normL2得到L2正则项，L1正则项，L2正则项计算公式如下：  
- L1正则化是指权重向量w中各个元素的绝对值之和，通常表示为||w||1
- L2正则化是指权重向量w中各个元素的平方和然后再求平方根(可以看到Ride回归的L2正则化项有平方符号)，通常表示为||w||2


L1:|1|+|2|+|3|=6  
L2:1^2+2^2+3^2=3.74165739

2. 计算相关系数，在Pandas和pyspark的DataFrame上都可以调用corr这个方法来计算相关的列的相关系数，在spark.mllib中Statistics提供了corr方法，使用这个方法来计算两个RDD代表的列Series的相关系数，可以通过关键字参数指定计算相关系数的算法，可以选pearson由皮尔逊相关系数算法，和spearman斯皮尔曼相关系数算法。皮尔逊相关系数算法是默认的计算相关性的算法。相关系数计算公式：  
D(X) = E [X - E(X)]2  
根号D(X)为X的均方差或标准差  
常用公式D(X) = E(X2) - E2(X)  
协方差  
COV(X,Y) = E([X - E(X)][Y - E(Y)])  
相关系数  
协方差 / [根号D(X) * 根号D(Y)]

~~~python
seriesX = sc.parallelize([1.0, 2.0, 3.0, 3.0, 5.0], 3)
seriesY = sc.parallelize([11.0, 22.0, 33.0, 33.0,555.0], 3)
print('person' + str(Statistics.corr(seriesX, seriesY, method='pearson')))
>>> person0.8500286768773001
data = sc.parallelize([
    np.array([1.0, 10.0, 100.0]),
    np.array([2.0, 20.0, 200.0]),
    np.array([5.0, 33.0, 366.0])
], 3)
print('spearman' + str(Statistics.corr(data, method='spearman')))
>>> 
spearman[[1. 1. 1.]
 [1. 1. 1.]
 [1. 1. 1.]]
~~~
除了在RDD上计算相关系数，实际上在ml模块基于DataFrame的数据集上也可以计算相关系数。需要使用到的是pyspark.ml.stat模块里面的Corrlation类，默认使用皮尔逊相关系数，可以指定使用斯皮尔曼相关系数

~~~python
from pyspark.ml.linalg import Vectors
from pyspark.ml.stat import Correlation

data = [(Vectors.sparse(4, [(0, 10), (3, -2.0)]), ),
        (Vectors.dense([4.0, 5.0, 0.0, 3.0]), ),
        (Vectors.dense([6.0, 7.0, 0.0, 8.0]), ),
        (Vectors.sparse(4, [(0, 9.0), (3, 1.0)]), )]
df = spark.createDataFrame(data, ['features'])
r1 = Correlation.corr(df, 'features').head()
r2 = Correlation.corr(df, 'features', 'spearman').head()
print(u'皮尔逊相关系数：\n' + str(r1[0]))
print(u'斯皮尔曼相关系数：\n' + str(r2[0]))
>>> 
DenseMatrix([[ 1.        , -0.85027128,         nan, -0.67676708],
             [-0.85027128,  1.        ,         nan,  0.91359586],
             [        nan,         nan,  1.        ,         nan],
             [-0.67676708,  0.91359586,         nan,  1.        ]])
斯皮尔曼相关系数：
DenseMatrix([[ 1.        , -0.73786479,         nan, -0.8       ],
             [-0.73786479,  1.        ,         nan,  0.9486833 ],
             [        nan,         nan,  1.        ,         nan],
             [-0.8       ,  0.9486833 ,         nan,  1.        ]])
~~~

4. 分层抽样，在RDD和DataFrame上都有sample和sampleByKey方法，其中sampleByKey方法适用对于key，value类型的数据进行抽样，可以之地哦那个fractions对key里面对应不同类型分层抽取。例如对于人口数据，key为male和female，可以适用sampleByKey方法对男女分别采样0.3和0.35

~~~python
data = sc.parallelize([
    ('male', 'tom'),
    ('male', 'jerry'),
    ('male', 'jick'),
    ('female', 'jean'),
    ('female', 'hua'),
    ('female', 'min')
], 3)
fractions = {'male':0.5, 'female': 0.4}
approxSample = data.sampleByKey(False, fractions)
approxSample.collect()
>>>  [('male', 'jerry'), ('female', 'jean'), ('female', 'hua'), ('female', 'min')]
~~~

5. 假设检验在统计学中是一个强有力的方法，用于判断某个结果是否是偶然出现的。在pyspark机器学习中，现在支持卡方检验，它用来检验集合的优度及独立性。检验拟合的优度输入的参数为Vector向量，对于独立性的检验传入的参数是Matrix矩阵。【p说个sig表示的是显著性检验，T检验代表的是均值检验，F检验代表的是方差齐次性检验，df表示自度degree of freedom】显著性检验表示检验表示在非常罕见的情况下事件发生了，说明事件的发生很大程度上不是偶然发生的。虽然说也有出错的情况，但是出错的情况相比于很容易发生的情况下事件发生出错的置信度要高，我们更有把握认为此次时间的发生不是偶然，虽然这也有判断失误的情况，但这样的情况的概率是非常低的，这个概率有p或sig指定，通常0.05是显著性检测的边界值

~~~python
from pyspark.mllib.linalg import Matrices, Vectors
from pyspark.mllib.regression import LabeledPoint
from pyspark.mllib.stat import Statistics


# 由时间发生的概率组成的向量
vec = Vectors.dense(0.1,0.15,0.2,0.3,0.25)
# 测试拟合优度
goodnessOfFitTestResult = Statistics.chiSqTest(vec)
dir(goodnessOfFitTestResult)
print('%s\n' % goodnessOfFitTestResult)
# 时间发生概率组成的矩阵
mat = Matrices.dense(3, 2, [1.0, 5.0, 3.0, 2.0, 4.0, 6.0])
# 计算事件的独立性
independenceTestResult = Statistics.chiSqTest(mat)
print('%s\n' % independenceTestResult)
>>> 
Chi squared test summary:
method: pearson
degrees of freedom = 4 
statistic = 0.12499999999999999 
pValue = 0.998126379239318 
No presumption against null hypothesis: observed follows the same distribution as expected..

Chi squared test summary:
method: pearson
degrees of freedom = 2 
statistic = 1.0370370370370372 
pValue = 0.595401971897774 
No presumption against null hypothesis: the occurrence of the outcomes is statistically independent..

obs = sc.parallelize([
    LabeledPoint(1.0, [1.0, 0.0, 3.0]),
    LabeledPoint(1.0, [1.0, 2.0, 0.0]),
    LabeledPoint(1.0, [-1.0, 0.0, -0.5])
], 3)
featrueTestResult = Statistics.chiSqTest(obs)
for i, result in enumerate(featrueTestResult):
    print('Column %d:\n %s' % (i + 1, result))  
Column 1:
 Chi squared test summary:
method: pearson
degrees of freedom = 0 
statistic = 0.0 
pValue = 1.0 
No presumption against null hypothesis: the occurrence of the outcomes is statistically independent..
Column 2:
 Chi squared test summary:
method: pearson
degrees of freedom = 0 
statistic = 0.0 
pValue = 1.0 
No presumption against null hypothesis: the occurrence of the outcomes is statistically independent..
Column 3:
 Chi squared test summary:
method: pearson
degrees of freedom = 0 
statistic = 0.0 
pValue = 1.0 
No presumption against null hypothesis: the occurrence of the outcomes is statistically independent..
~~~

还有一个概念叫自由度，自由度的定义：构成样本统计量的独立的样本观测值的数目或自由变动的样本观测值的数目。用df表示  

另一种检验方式： 柯斯米尔诺夫：基于DataFrame的假设检验和基于RDD的假设检验类似。需要引入pyspark.ml.stat模块下的ChiSquareTest类来完成卡方检验

~~~python
from pyspark.ml.linalg import Vectors
from pyspark.ml.stat import ChiSquareTest

data = [(0.0, Vectors.dense(0.5, 10.0)), (0.0, Vectors.dense(1.5, 20.0)),
        (1.0, Vectors.dense(1.5, 30.0)), (0.0, Vectors.dense(3.5, 30.0)),
        (0.0, Vectors.dense(3.5, 40.0)), (1.0, Vectors.dense(3.5, 40.0))]

df = spark.createDataFrame(data, ['label', 'features'])
r = ChiSquareTest.test(df, 'features', 'label').head()
r
>>> Row(pValues=DenseVector([0.6873, 0.6823]), degreesOfFreedom=[2, 3], statistics=DenseVector([0.75, 1.5]))
~~~

6. 随机数据在统计或者在算法的测试过程中是很有用的，在ml库中提供了产生随机数的方法，会产生随机的double类型的RDD，随机数据的分布可以指定为服务从哪种分布，例如：统一分布uniform，标准正态分布standard normal，或者泊松分布Possion。下面使用RandomRDDs上的normalRDD方法，出入sparkcontext，生成随机数的个数9999，及分区数10，返回rdd

~~~python
from pyspark.mllib.random import RandomRDDs

u = RandomRDDs.normalRDD(sc, 9999, 10)
v = u.map(lambda x : 1.0 + 2.0 * x)
u 
>>>  MapPartitionsRDD[22] at mapPartitions at PythonMLLibAPI.scala:1339
u.count()
>>>  9999
u.getNumPartitions()
>>>  10
~~~

7. 核密度估计  核密度估计是一种有用的计数，用于可视化经验概率分布，而不需要假设所观察到的样本的特定分布。它计算一个随机变量的概率密度函数的估计，在给定的点集上进行评估。在ml模块中使用KernelDensity来对RDD中的元素进行核密度估计。setBandwidth用于设置高斯核函数的标准偏差。  
使用estimate计算给定值的核密度估计  

~~~python
from pyspark.mllib.stat import KernelDensity

data = sc.parallelize(
    [1.0, 1.0, 1.0, 2.0, 3.0, 4.0, 5.0, 5.0, 6.0, 7.0, 8.0, 9.0, 9.0], 3)

kd = KernelDensity()
kd.setSample(data)
kd.setBandwidth(3.0)
densities = kd.estimate([-1.0, 2.0, 5.0])
densities
>>>  array([0.04145944, 0.07902017, 0.0896292 ])
kd
>>>  <pyspark.mllib.stat.KernelDensity.KernelDensity at 0x11b2da15940>
~~~



- 最直观的理解核密度估计就是下面的图。对于一个分布通过和函数估计出分布曲线

~~~python
%matplotlib inline
import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
import seaborn as sns

# 设置风格、尺度
sns.set_style('darkgrid')
sns.set_context('paper')
# 不发出警告
import warnings
warnings.filterwarnings('ignore')
# 设定随机数种子
rs = np.random.RandomState(10)
s = pd.Series(rs.randn(100) * 100)
sns.distplot(s,
             bins=10,
             hist=True,
             kde=True,
             norm_hist=False,
             rug=True,
             vertical=True,
             color='b',
             label='distplot',
             axlabel='x')
plt.legend()
>>> 
~~~
![下载](F:\下载\下载.png)
