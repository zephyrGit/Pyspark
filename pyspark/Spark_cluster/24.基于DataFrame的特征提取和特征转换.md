
1. 这部分主要涉及ETS【Extraction，Transformation，Selection】，完成对机器学的中数据的预处理


2. **TF-IDF**   

词频逆文档数，这个方法广泛的应用于文本挖掘，用于衡量一个词对于一个文本的重要程度。用“t”表示一个词，“d”表示文档，'D'表示多个文档组成的语料，词频可以表示为TF（t，d）单词t在文档中出现的频率，文档频率剋表示为DF(t，D)，意思时包含词t的文档数。但是，如果只用TF去衡量一个单词的重要程度，很容易偏向于强调出现频率多但是携带信息少的单词，如介词、语气助词等，而这些词实际携带的信息量是非常少的。如何去避免出现次数多而携带信息量少的词。解决办法是用文档数去限制这些在各个文档中经常出点的次的权重，做法是|D|文档数加1除以词t出现的文档频率DF(t，D)+1，再求对数。因此IDF定义如下：  
  ![image.png](attachment:image.png)
  可以看出当出现的文档数越多，分母越大，而整个对数的取值越小，这就给文档频率大
的词t做了降权操作。那整个TF-IDF的定义如下：   
TFIDF(t,d,D) = TF(t,d) · IDF(t,D)  
在pyspark中，实际是将上面的TF和IDF分开定义    


对于词频TF，可以使用HashingTF和CountVectorizer来产生，HashingTF是一个Transformer转器，它接收一组单词并转换成固定长度的特征向量，在文本处理中一组单词就是指词袋中的单词。原始的单词通过hash函数隐射到一个index，然后词频的计算则是根据隐射的index来计算。这种方式避免计算整个语料，但是会存在潜在的hash碰撞的冲突，不同的原始词经过hash算法后可以得到相同的index。CountVectorizer是把文本转换为词向量进行计数。    


对于IDF，IDF它是一个估计器，给它喂入数据之后会产生IDFModel模型，IDFModel中会获取到特征向量，IDF的作用是对语料中频繁出现的词进行降权。最后需要注意ml模块没有提供文本分词的工具，需要借助Stanford NLP Group或者scalanlp/chalk.  
接下来我们就使用TF-IDF来为文档创建向量表示。从pyspark.ml.feature总导入HashingTF,IDF和Tokenizer。使用createDataFrame方法创建语料库。  

~~~python
from pyspark.sql import SparkSession
from pyspark.ml.feature import HashingTF, IDF, Tokenizer

spark = SparkSession.builder.master('local').appName('df').getOrCreate()
sc = spark.sparkContext

sentenceData = spark.createDataFrame(
    [(0.0, 'Hi i heard about Spark'),
     (0.0, 'I wish java could use case classes'),
     (1.0, 'Logistic regression models are neat')], ['label', 'sentence'])
tokenizer = Tokenizer(inputCol='sentence', outputCol='words')
wordsData = tokenizer.transform(sentenceData)
wordsData
>>>  DataFrame[label: double, sentence: string, words: array<string>]
wordsData.show()
>>> 
+-----+--------------------+--------------------+
|label|            sentence|               words|
+-----+--------------------+--------------------+
|  0.0|Hi i heard about ...|[hi, i, heard, ab...|
|  0.0|I wish java could...|[i, wish, java, c...|
|  1.0|Logistic regressi...|[logistic, regres...|
+-----+--------------------+--------------------+
~~~
↑ 创建Tokenizer分词器，对DataFrame的sentence列采用默认的空格进行分词，分词保存为新的列words
~~~python
hashingTF = HashingTF(inputCol='words', outputCol='rawFeatures', numFeatures=20)
featurizedData = hashingTF.transform(wordsData)
featurizedData.show(1)
>>> 
+-----+--------------------+--------------------+--------------------+
|label|            sentence|               words|         rawFeatures|
+-----+--------------------+--------------------+--------------------+
|  0.0|Hi i heard about ...|[hi, i, heard, ab...|(20,[0,5,9,17],[1...|
+-----+--------------------+--------------------+--------------------+
only showing top 1 row  
featurizedData.collect()[0]
>>>  Row(label=0.0, sentence='Hi i heard about Spark', words=['hi', 'i', 'heard', 'about', 'spark'], rawFeatures=SparseVector(20, {0: 1.0, 5: 1.0, 9: 1.0, 17: 2.0}))                                                                 
~~~
↑ 对于输入的单词向量使用HashingTF来创建词频向量，输出到新列rawFeatures，在transform中会移除掉stopwords
~~~python
idf = IDF(inputCol='rawFeatures', outputCol='features')
idfModel = idf.fit(featurizedData)
rescaledData = idfModel.transform(featurizedData)
rescaledData.select('label', 'features').show(truncate=False)
>>> 
+-----+----------------------------------------------------------------------------------------------------------------------+
|label|features                                                                                                              |
+-----+----------------------------------------------------------------------------------------------------------------------+
|0.0  |(20,[0,5,9,17],[0.6931471805599453,0.6931471805599453,0.28768207245178085,1.3862943611198906])                        |
|0.0  |(20,[2,7,9,13,15],[0.6931471805599453,0.6931471805599453,0.8630462173553426,0.28768207245178085,0.28768207245178085]) |
|1.0  |(20,[4,6,13,15,18],[0.6931471805599453,0.6931471805599453,0.28768207245178085,0.28768207245178085,0.6931471805599453])|
+-----+----------------------------------------------------------------------------------------------------------------------+
~~~
↑ 最终看到经过TF-IDF转换之后得到的特征  
既然以得到了文本的量化信息，自然可以用这份信息去运行其它算法

3.**Word2Vec**   

Word2Vec是一个Estimator，可以借助其transform方法接收代表文档的序列单词而训练为一Word2VecMode模型，这个模型将出入的单词或文档转换为一个固定长度的向量，向量默认的长度是100，相比于One Hot Representation多达几万几十万的维度这算很小的维度了。对于Word2Vec中使用的多层次分类及Negetive Sampling模型对应的CBOW连续词袋算法和Skip-gram算法。  
     使用Word2Vec得到的向量及文档可以用于计算此或者文本相似度，是NLP中常用的文本量化工具  
     要使用Word2Vec，需要从pyspark.ml.feature中引入Word2Vec类  

~~~python

documentDF = spark.createDataFrame(
    [('Hi I read about Spark'.split(' '), ),
     ('I wish java could use case classes'.split(' '), ),
     ('Logistic regression models are neat'.split(' '), )], ['text'])

documentDF.show(truncate=False)
>>> 
+------------------------------------------+
|text                                      |
+------------------------------------------+
|[Hi, I, read, about, Spark]               |
|[I, wish, java, could, use, case, classes]|
|[Logistic, regression, models, are, neat] |
+------------------------------------------+
~~~
初始化Word2Vec，词向量大小通过关键字参数vectorSize指定为3
~~~python

word2Vec = Word2Vec(vectorSize=3,
                    minCount=0,
                    inputCol='text',
                    outputCol='result')
model = word2Vec.fit(documentDF)
type(model)
result = model.transform(documentDF)
result.show(truncate=False)
>>>
+------------------------------------------+------------------------------------------------------------------+
|text                                      |result                                                            |
+------------------------------------------+------------------------------------------------------------------+
|[Hi, I, read, about, Spark]               |[-0.06167888566851616,-0.009834748134016991,-0.023303847759962082]|
|[I, wish, java, could, use, case, classes]|[0.009561929386109114,-0.030813851260713166,0.03833093028515577]  |
|[Logistic, regression, models, are, neat] |[-0.006050730124115944,-0.028602862358093263,0.034613007307052614]|
+------------------------------------------+------------------------------------------------------------------+
for row in result.collect():
    text, vector = row
    print('Text: [%s] => \nVector: %s\n' % (','.join(text), str(vector)))
Text: [Hi,I,read,about,Spark] => 
Vector: [-0.0616788856685,-0.00983474813402,-0.02330384776]

Text: [I,wish,java,could,use,case,classes] => 
Vector: [0.00956192938611,-0.0308138512607,0.0383309302852]

Text: [Logistic,regression,models,are,neat] => 
Vector: [-0.00605073012412,-0.0286028623581,0.0346130073071]    
~~~

4.**CountVectorizer**  
   它的目的是转换一组文本到一个带有计数的向量中去，这个转换器将在词典上位文档映射一个稀疏向量，这个向量可以被传给例如PCA、LDA等算法进行降维处理
   
 id | texts  
 ———|————————————————  
  0 |Array('a', 'b', 'c')  
  1 |Array('a', 'b', 'b', 'c', 'a')  
对于上面的DataFrame，经过CountVectorizer处理后，结果如下，第一个参数表示单词的个数，第二个参数位一个列表，表示单词在词典中的索引，第三个参数表示对应单词出现的次数  
 id | texts                          | vector   
 ———|————————————————————————————————|——————————————————————————————————    
  0 |Array('a', 'b', 'c')            | (3, [0, 1, 2], [1.0, 1.0, 1.0])  
  1 |Array('a', 'b', 'b', 'c', 'a')  | (3, [0, 1, 2], [2.0, 2.0, 1.0])  

DataFrame中的每行带有ID和由字符组成的list组成，在实例化CountVectorizer的时候，可以通过关键字参数vocalSize指定字典的大小。minDF指定最少的文档数目  
~~~~python

df = spark.createDataFrame([(0, 'a'.split(' ')), (1, 'a b b c a '.split(' '))],
                           ['id', 'words'])
cv = CountVectorizer(inputCol='words', outputCol='features')
model = cv.fit(df)
result = model.transform(df)
result.show(truncate=False)

>>> 
+---+-----------------+-------------------------------+
|id |words            |features                       |
+---+-----------------+-------------------------------+
|0  |[a]              |(4,[0],[1.0])                  |
|1  |[a, b, b, c, a, ]|(4,[0,1,2,3],[2.0,2.0,1.0,1.0])|
+---+-----------------+-------------------------------+
~~~

5.**特征转换**  
**Tokenizer**：用于处理传入的文本，切分为独立的单词。除Tokenizer外还可以用RegexTokenizer它提供了基于正则表达式的匹配，默认的正则参数为‘\\s+’，使用空格切分文本为单词。  

~~~python

sentenceDataFrame = spark.createDataFrame(
    [(0, 'Hi I heard about Spark'), (1, 'I wish java could use case classes'),
     (2, 'Logistic,regresstion,models,are,neat')], ['id', 'sentence'])

tokenizer = Tokenizer(inputCol='sentence', outputCol='words')
regexTokenizer = RegexTokenizer(inputCol='sentence',
                                outputCol='words',
                                pattern='\\W')

countTokens = udf(lambda words: len(words), IntegerType())
tokenized = tokenizer.transform(sentenceDataFrame)
tokenized.select('sentence', 'words').withColumn(
    'tokens', countTokens(col('words'))).show(truncate=False)
regexTokenized = regexTokenizer.transform(sentenceDataFrame)
regexTokenized.select('sentence', 'words').withColumn(
    'tokens', countTokens(col('words'))).show(truncate=False)
>>> 
+------------------------------------+------------------------------------------+------+
|sentence                            |words                                     |tokens|
+------------------------------------+------------------------------------------+------+
|Hi I heard about Spark              |[hi, i, heard, about, spark]              |5     |
|I wish java could use case classes  |[i, wish, java, could, use, case, classes]|7     |
|Logistic,regresstion,models,are,neat|[logistic,regresstion,models,are,neat]    |1     |
+------------------------------------+------------------------------------------+------+

+------------------------------------+------------------------------------------+------+
|sentence                            |words                                     |tokens|
+------------------------------------+------------------------------------------+------+
|Hi I heard about Spark              |[hi, i, heard, about, spark]              |5     |
|I wish java could use case classes  |[i, wish, java, could, use, case, classes]|7     |
|Logistic,regresstion,models,are,neat|[logistic, regresstion, models, are, neat]|5     |
+------------------------------------+------------------------------------------+------+
~~~

**StopWordsRemover**  
在文本的处理中，有很多出现频率很高但是不携带任何信息的词，例如“de”、“也”等，这些语气词或介词在文本分析中是没有用处的，因此可以把这些词给去除掉。可以通过StopWordsRemover.loadDefaultsStopWords (language)指定去除什么语言对应的停用词，默认为enlish，且不支持中文

~~~python

from pyspark.ml.feature import StopWordsRemover

sentenceDate = spark.createDataFrame(
    [(0, ['I', 'saw', 'the', 'red', 'balloon']),
     (1, ['Mary', 'head', 'a', 'little', 'lamb'])], ['id', 'raw'])

remover = StopWordsRemover(inputCol='raw', outputCol='filtered')
remover.transform(sentenceDate).show(truncate=False)
>>> 
+---+-----------------------------+--------------------------+
|id |raw                          |filtered                  |
+---+-----------------------------+--------------------------+
|0  |[I, saw, the, red, balloon]  |[saw, red, balloon]       |
|1  |[Mary, head, a, little, lamb]|[Mary, head, little, lamb]|
+---+-----------------------------+--------------------------+
~~~

**n-gram模型**  
n-gram词袋模型用于产生n元的模型，n作为参数指定产生几元的词袋。当输入的文本序列小于n的时候，没有任何输出产生。指定n为2，产生2元词袋模型  

~~~python

from pyspark.ml.feature import NGram

wordDataFrame = spark.createDataFrame(
    [(0, ['Hi', 'I', 'heard', 'about', 'Spark']),
     (1, ['I', 'wish', 'java', 'could', 'use', 'case', 'classes']),
     (2, ['Logistic', 'regresstion', 'models', 'are', 'neat'])],
    ['id', 'words'])
ngram = NGram(n=2, inputCol='words', outputCol='ngrams')
ngramDataFrame = ngram.transform(wordDataFrame)
ngramDataFrame.select('ngrams').show(truncate=False)
>>> 
+------------------------------------------------------------------+
|ngrams                                                            |
+------------------------------------------------------------------+
|[Hi I, I heard, heard about, about Spark]                         |
|[I wish, wish java, java could, could use, use case, case classes]|
|[Logistic regresstion, regresstion models, models are, are neat]  |
+------------------------------------------------------------------+
~~~

**Binarizer**  
用于对数值类型的列及逆行二值化，可以指定一个阀值，大于阀值的返回1，小于等于阀值的返回0  

~~~python

continuousDataFrame = spark.createDataFrame([(0, 0.1), (1, 0.8), (2, 0.2)],
                                            ['id', 'feature'])
binarizer = Binarizer(threshold=0.5, inputCol='feature', outputCol='binarized_feature')
binarizedDataFrame = binarizer.transform(continuousDataFrame)
print('threshold = %f' % binarizer.getThreshold())
binarizedDataFrame.show()
>>> 
threshold = 0.500000
+---+-------+-----------------+
| id|feature|binarized_feature|
+---+-------+-----------------+
|  0|    0.1|              0.0|
|  1|    0.8|              1.0|
|  2|    0.2|              0.0|
+---+-------+-----------------+
~~~

**PCA**  
主成分分析，用正交变换将可能相关的向量的观测值转换成一组不相关变量的值统计的过程。使用PCA可以把特征值向量映射到更加低纬度的空间，可以通过关键字参数指定映射后特征空间的维度。数学原理大意为先求出各个变量的协方差矩阵，在求出协方差矩阵的特征值和特征向量，按特征值逆序排列，取出topn即为主特征。  
![image.png](attachment:image.png)
![image.png](attachment:image.png)

~~~python

from pyspark.ml.feature import PCA
from pyspark.ml.linalg import Vectors

data = [(Vectors.sparse(5, [(1, 1.0), (3, 7.0)]), ),
        (Vectors.dense([2.0, 0.0, 3.0, 4.0, 5.0]), ),
        (Vectors.dense([4.0, 0.0, 0.0, 6.0, 7.0]), )]

df = spark.createDataFrame(data, ['features'])
pca = PCA(k=3, inputCol='features', outputCol='pcaFeatures')
model = pca.fit(df)
result = model.transform(df).select('pcaFeatures')
result.show(truncate=False)
>>> 
+-----------------------------------------------------------+
|pcaFeatures                                                |
+-----------------------------------------------------------+
|[1.6485728230883807,-4.013282700516296,-5.524543751369388] |
|[-4.645104331781534,-1.1167972663619026,-5.524543751369387]|
|[-6.428880535676489,-5.337951427775355,-5.524543751369389] |
+-----------------------------------------------------------+
~~~
在MNIST图像处理中，可以使用PCA先降维，再对降维后的低纬度特征进行分类

**PolynomialExpansion**  
有时候再处理数据的时候，特征数比较少，可以借助PolynomialExpansion将低纬度的特征扩展到高纬度。这种草所可以借助PolynomialExpansion。下面将二维数据扩展到三维。借助多项式核函数，可以将低维度映射到高维度便于增大类间距对线性不可分的数据进行分类，如SVM分类算法中多项式核函数，多项式核函数的定义如下：  
![image.png](attachment:image.png)  

~~~python

from pyspark.ml.feature import PolynomialExpansion
from pyspark.ml.linalg import Vectors

df = spark.createDataFrame([(Vectors.dense([2.0, 1.0]), ),
                            (Vectors.dense([0.0, 0.0]), ),
                            (Vectors.dense([3.0, -1.0]), )], ['features'])

polyExpansion = PolynomialExpansion(degree=3,
                                    inputCol='features',
                                    outputCol='polyFeatures')
polyModel = polyExpansion.transform(df)
polyModel.show(truncate=False)
>>> 
+----------+------------------------------------------+
|features  |polyFeatures                              |
+----------+------------------------------------------+
|[2.0,1.0] |[2.0,4.0,8.0,1.0,2.0,4.0,1.0,2.0,1.0]     |
|[0.0,0.0] |[0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0]     |
|[3.0,-1.0]|[3.0,9.0,27.0,-1.0,-3.0,-9.0,1.0,3.0,-1.0]|
+----------+------------------------------------------+

polyExpansion = PolynomialExpansion(degree=2,
                                    inputCol='features',
                                    outputCol='polyFeatures')
polyModel = polyExpansion.transform(df)
polyModel.show(truncate=False)
>>> 
+----------+-----------------------+
|features  |polyFeatures           |
+----------+-----------------------+
|[2.0,1.0] |[2.0,4.0,1.0,2.0,1.0]  |
|[0.0,0.0] |[0.0,0.0,0.0,0.0,0.0]  |
|[3.0,-1.0]|[3.0,9.0,-1.0,-3.0,1.0]|
+----------+-----------------------+
~~~
上面分别为3项式和2项式  
$\color{#FF0000}{【A^x, B^y, x + y = n】}$ 

**Normalizer**  
Nomalizer是一个Transformer，它使用p-norm范数对数据集进行正则化，默认p的取值为2，借助Normalizer可以标准化输入，标准化的输入数据能够提高学习算法的表现性能。接下来演示范数p=1和p->无穷的时候使用Normalizer对数据进行标准化处理。将每个样本缩放到单位范数（每个样本的范数为1），如果后面要使用二次型（点积）或其它核函数方法计算两个样本之间的相似性，这个方法很有用。  
Normalization主要思想是对每个样本计算器p-范数，然后对该样本中每个元素除以该范数，这样处理的结果是每个处理后样本的p-范数（L1-norm、L2-norm）等于1。  
p-范数的计算公式：||x||p=(|x1|^p+|x2|^p+...+|xn|^p)^(1/p)  
该方法主要应用再文本分类和聚类中。例如：对于两个TF-IDF向量和L2-norm进行点积，就可以得到这两个向量的余弦相似性  

~~~python

from pyspark.ml.feature import Normalizer
from pyspark.ml.linalg import Vectors

dataFrame = spark.createDataFrame([(
    0,
    Vectors.dense([1.0, 0.5, -1.0]),
), (
    1,
    Vectors.dense([2.0, 1.0, 1.0]),
), (
    2,
    Vectors.dense([4.0, 10.0, 2.0]),
)], ['id', 'features'])
dataFrame.show()
>>> 
+---+--------------+
| id|      features|
+---+--------------+
|  0|[1.0,0.5,-1.0]|
|  1| [2.0,1.0,1.0]|
|  2|[4.0,10.0,2.0]|
+---+--------------+

normalizer = Normalizer(inputCol='features', outputCol='normFeatures', p=1.0)
l1NormData = normalizer.transform(dataFrame)
print('Normalized using L^1 norm')
l1NormData.show()
>>>
Normalized using L^1 norm
+---+--------------+------------------+
| id|      features|      normFeatures|
+---+--------------+------------------+
|  0|[1.0,0.5,-1.0]|    [0.4,0.2,-0.4]|
|  1| [2.0,1.0,1.0]|   [0.5,0.25,0.25]|
|  2|[4.0,10.0,2.0]|[0.25,0.625,0.125]|
+---+--------------+------------------+

normalizer = Normalizer(inputCol='features', outputCol='normFeatures')
l2NormData = normalizer.transform(dataFrame, {normalizer.p:float(2)})
print('Normalized using L^2 norm')
l2NormData.show(truncate=False)
>>> 
Normalized using L^2 norm
+---+--------------+-----------------------------------------------------------+
|id |features      |normFeatures                                               |
+---+--------------+-----------------------------------------------------------+
|0  |[1.0,0.5,-1.0]|[0.6666666666666666,0.3333333333333333,-0.6666666666666666]|
|1  |[2.0,1.0,1.0] |[0.8164965809277261,0.4082482904638631,0.4082482904638631] |
|2  |[4.0,10.0,2.0]|[0.3651483716701107,0.9128709291752769,0.18257418583505536]|
+---+--------------+-----------------------------------------------------------+
~~~
怎么计算？利用p-norm范数公式

~~~python

1**2 + 0.5**2 + (-1)**2
>>>  2.25
1.0 / 1.5
>>>  0.6666666666666666
-1.0 / 1.5
>>>  -0.6666666666666666

linfNormData = normalizer.transform(dataFrame, {normalizer.p:float('inf')})
print('Normalized using L^f norm')
linfNormData.show()
>>> 
Normalized using L^f norm
+---+--------------+--------------+
| id|      features|  normFeatures|
+---+--------------+--------------+
|  0|[1.0,0.5,-1.0]|[1.0,0.5,-1.0]|
|  1| [2.0,1.0,1.0]| [1.0,0.5,0.5]|
|  2|[4.0,10.0,2.0]| [0.4,1.0,0.2]|
+---+--------------+--------------+
~~~

**StandardScaler**  
标准化也叫归一化是对于每个特征进行的，经过归一化处理之后，每个特征服从均值为0，标准差为1的分布。使用StandardScaler对数据特征进行归一化。StandardScaler也叫Z-Score标准化  

~~~python

from pyspark.ml.feature import StandardScaler
dataFrame = spark.read.format("libsvm").load('D:/data/mllib/sample_libsvm_data.txt')
dataFrame.show(truncate=False)    
>>>  
--------------------------------------------+
|0.0  |(692,[127,128,129,130,131,154,155,156,157,158,159,181,182,183,184,185,186,187,188,189,207,208,209,210,211,212,213,214,215,216,217,235,236,237,238,239,240,241,242,243,244,245,262,263,264,265,266,267,268,269,270,271,272,273,289,290,291,292,293,294,295,296,297,300,301,302,316,317,318,319,320,321,328,329,330,343,344,345,346,347,348,349,356,357,358,371,372,373,374,384,385,386,399,400,401,412,413,414,426,427,428,429,440,441,442,454,455,456,457,466,467,468,469,470,482,483,484,493,494,495,496,497,510,511,512,520,521,522,523,538,539,540,547,548,549,550,566,567,568,569,570,571,572,573,574,575,576,577,578,594,595,596,597,598,599,600,601,602,603,604,622,623,624,625,626,627,628,629,630,651,652,653,654,655,656,657],[51.0,159.0,253.0,159.0,50.0,48.0,238.0,252.0,252.0,252.0,237.0,54.0,227.0,253.0,252.0,239.0,233.0,252.0,57.0,6.0,10.0,60.0,224.0,252.0,253.0,252.0,202.0,84.0,252.0,253.0,122.0,163.0,252.0,252.0,252.0,253.0,252.0,252.0,96.0,189.0,253.0,167.0,51.0,238.0,253.0,253.0,190.0,114.0,253.0,228.0,47.0,79.0,255.0,168.0,48.0,238.0,252.0,252.0,179.0,12.0,75.0,121.0,21.0,253.0,243.0,50.0,38.0,165.0,253.0,233.0,208.0,84.0,253.0,252.0,165.0,7.0,178.0,252.0,240.0,71.0,19.0,28.0,253.0,252.0,195.0,57.0,252.0,252.0,63.0,253.0,252.0,195.0,198.0,253.0,190.0,255.0,253.0,196.0,76.0,246.0,252.0,112.0,253.0,252.0,148.0,85.0,252.0,230.0,25.0,7.0,135.0,253.0,186.0,12.0,85.0,252.0,223.0,7.0,131.0,252.0,225.0,71.0,85.0,252.0,145.0,48.0,165.0,252.0,173.0,86.0,253.0,225.0,114.0,238.0,253.0,162.0,85.0,252.0,249.0,146.0,48.0,29.0,85.0,178.0,225.0,253.0,223.0,167.0,56.0,85.0,252.0,252.0,252.0,229.0,215.0,252.0,252.0,252.0,196.0,130.0,28.0,199.0,252.0,252.0,253.0,252.0,252.0,233.0,145.0,25.0,128.0,252.0,253.0,252.0,141.0,37.0])                                                                                             |
|1.0  |(692,[158,159,160,161,185,186,187,188,189,213,214,215,216,217,240,241,242,243,244,245,267,268,269,270,271,295,296,297,298,322,323,324,325,326,349,350,351,352,353,377,378,379,380,381,404,405,406,407,408,431,432,433,434,435,459,460,461,462,463,486,487,488,489,490,514,515,516,517,518,542,543,544,545,569,570,571,572,573,596,597,598,599,600,601,624,625,626,627,652,653,654,655,680,681,682,683],[124.0,253.0,255.0,63.0,96.0,244.0,251.0,253.0,62.0,127.0,251.0,251.0,253.0,62.0,68.0,236.0,251.0,211.0,31.0,8.0,60.0,228.0,251.0,251.0,94.0,155.0,253.0,253.0,189.0,20.0,253.0,251.0,235.0,66.0,32.0,205.0,253.0,251.0,126.0,104.0,251.0,253.0,184.0,15.0,80.0,240.0,251.0,193.0,23.0,32.0,253.0,253.0,253.0,159.0,151.0,251.0,251.0,251.0,39.0,48.0,221.0,251.0,251.0,172.0,234.0,251.0,251.0,196.0,12.0,253.0,251.0,251.0,89.0,159.0,255.0,253.0,253.0,31.0,48.0,228.0,253.0,247.0,140.0,8.0,64.0,251.0,253.0,220.0,64.0,251.0,253.0,220.0,24.0,193.0,253.0,220.0])                                                              

scaler = StandardScaler(inputCol='features',
                        outputCol='scaleFeatures',
                        withStd=True,
                        withMean=True)

scaleModel = scaler.fit(dataFrame)
scaledData = scaleModel.transform(dataFrame)
scaledData.show(truncate=False)
>>> 

|0.0  |(692,[127,128,129,130,131,154,155,156,157,158,159,181,182,183,184,185,186,187,188,189,207,208,209,210,211,212,213,214,215,216,217,235,236,237,238,239,240,241,242,243,244,245,262,263,264,265,266,267,268,269,270,271,272,273,289,290,291,292,293,294,295,296,297,300,301,302,316,317,318,319,320,321,328,329,330,343,344,345,346,347,348,349,356,357,358,371,372,373,374,384,385,386,399,400,401,412,413,414,426,427,428,429,440,441,442,454,455,456,457,466,467,468,469,470,482,483,484,493,494,495,496,497,510,511,512,520,521,522,523,538,539,540,547,548,549,550,566,567,568,569,570,571,572,573,574,575,576,577,578,594,595,596,597,598,599,600,601,602,603,604,622,623,624,625,626,627,628,629,630,651,652,653,654,655,656,657],[51.0,159.0,253.0,159.0,50.0,48.0,238.0,252.0,252.0,252.0,237.0,54.0,227.0,253.0,252.0,239.0,233.0,252.0,57.0,6.0,10.0,60.0,224.0,252.0,253.0,252.0,202.0,84.0,252.0,253.0,122.0,163.0,252.0,252.0,252.0,253.0,252.0,252.0,96.0,189.0,253.0,167.0,51.0,238.0,253.0,253.0,190.0,114.0,253.0,228.0,47.0,79.0,255.0,168.0,48.0,238.0,252.0,252.0,179.0,12.0,75.0,121.0,21.0,253.0,243.0,50.0,38.0,165.0,253.0,233.0,208.0,84.0,253.0,252.0,165.0,7.0,178.0,252.0,240.0,71.0,19.0,28.0,253.0,252.0,195.0,57.0,252.0,252.0,63.0,253.0,252.0,195.0,198.0,253.0,190.0,255.0,253.0,196.0,76.0,246.0,252.0,112.0,253.0,252.0,148.0,85.0,252.0,230.0,25.0,7.0,135.0,253.0,186.0,12.0,85.0,252.0,223.0,7.0,131.0,252.0,225.0,71.0,85.0,252.0,145.0,48.0,165.0,252.0,173.0,86.0,253.0,225.0,114.0,238.0,253.0,162.0,85.0,252.0,249.0,146.0,48.0,29.0,85.0,178.0,225.0,253.0,223.0,167.0,56.0,85.0,252.0,252.0,252.0,229.0,215.0,252.0,252.0,252.0,196.0,130.0,28.0,199.0,252.0,252.0,253.0,252.0,252.0,233.0,145.0,25.0,128.0,252.0,253.0,252.0,141.0,37.0]) 

[0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,-0.1,-0.1,-0.13571954056485272,-0.1287247391513735,-0.1584444051943696,-0.1688554409010401,-0.19337866817164565,-0.10676205778492123,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,-0.09999999999999999,-0.1,-0.13384765349564315,-0.13604725959041086,-0.2777313470985699,-0.44478550837283903,-0.5366590481637703,-0.624230478450763,-0.14131634759821954,0.897511160838758,1.68340989431959,1.078579629528717,0.277842102313824,-0.2783504214480725,-0.14240908055020368,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,-0.09999999999999999,-0.1,-0.12377112679326373,-0.17405308098804234,-0.2874755548740231,-0.43600087217125555,-0.6435872477407749,-0.804021137400833,-0.5562514763094688,0.9700867517442657,1.0756001116083052,1.063262677016237,1.2936374421961787,1.5017327862351777,-0.4409251264
~~~

常见的标准化方法有Min-Max标准化，Z-Score标准化  
Min-MAX: 也叫离差标准化，是对原始数据的线性变换，使结果落到[0,1]区间，转换函数如下：   
  ![image.png](attachment:image.png)
  直接用每个元素除以总元素的和这种方式也可以用于归一化数据。  
  ![image.png](attachment:image.png)
对于Min-Max标准化，pyspark提供了MinMaxScaler类用来做这类处理  

~~~python

from pyspark.ml.feature import MinMaxScaler
from pyspark.ml.linalg import Vectors

dataFrame = spark.createDataFrame([(
    0,
    Vectors.dense([1.0, 0.1, -1.0]),
), (
    1,
    Vectors.dense([2.0, 1.1, 1.0]),
), (
    2,
    Vectors.dense([3.0, 10.1, 3.0]),
)], ['id', 'features'])

scaler = MinMaxScaler(inputCol='features', outputCol='scaledFeatures')
scalerModel = scaler.fit(dataFrame)

scaledData = scalerModel.transform(dataFrame)
print('Features scaled to range: [%f, %f]' %
      (scaler.getMin(), scaler.getMax()))
scaledData.select('features', 'scaledFeatures').show()
>>> 
Features scaled to range: [0.000000, 1.000000]
+--------------+--------------+
|      features|scaledFeatures|
+--------------+--------------+
|[1.0,0.1,-1.0]| [0.0,0.0,0.0]|
| [2.0,1.1,1.0]| [0.5,0.1,0.5]|
|[3.0,10.1,3.0]| [1.0,1.0,1.0]|
+--------------+--------------+
~~~

还有一种归一化方法叫MaxAbsScaler，它会把数据归一到[-1,1]之间，它除以的是数据集总绝对值的最大的值。这种方法不会移动或集中数据，因此不会破坏数据的稀疏性  

~~~~python

from pyspark.ml.feature import MaxAbsScaler
dataFrame = spark.createDataFrame([(
    0,
    Vectors.dense([1.0, 0.1, -8.0]),
), (
    1,
    Vectors.dense([2.0, 1.0, -4.0]),
), (
    2,
    Vectors.dense([4.0, 10.0, 8.0]),
)], ['id', 'features'])

scaler = MaxAbsScaler(inputCol='features', outputCol='scaledFeatures')
scalsedModel = scaler.fit(dataFrame)
scaledData = scalerModel.transform(dataFrame)
scaledData.select('features', 'scaledFeatures').show(truncate=False)
>>>  
+--------------+----------------+
|features      |scaledFeatures  |
+--------------+----------------+
|[1.0,0.1,-8.0]|[0.0,0.0,-1.75] |
|[2.0,1.0,-4.0]|[0.5,0.09,-0.75]|
|[4.0,10.0,8.0]|[1.5,0.99,2.25] |
+--------------+----------------+
~~~

**Bucketizer**  
分桶器，对于连续的数据进行分桶。分桶的区间由用户指定。对于分桶的区间不确定的时候，建议指定桶的下界为Double.NegativeInfinity(负无穷)，上界为Double.PositiveInfinity(正无穷)。若超过了桶的定义的界限会报错。每个桶对应的区间为[lowwer，upper)前闭后开  
    
~~~python

rom pyspark.ml.feature import Bucketizer

splits = [-float('inf'), -0.5, 0.0, 0.5, float('inf')]
data = [(-999.9, ), (-0.5, ), (-0.3, ), (0.0, ), (0.2, ), (999.9, )]
dataFrame = spark.createDataFrame(data, ['features'])
buicketizer = Bucketizer(splits=splits,
                         inputCol='features',
                         outputCol='bucketedFeatures')
buickedData = buicketizer.transform(dataFrame)
print('Bucketizer output with %d buckets' % (len(buicketizer.getSplits()) - 1))
buickedData.show(truncate=False)
>>> 
Bucketizer output with 4 buckets
+--------+----------------+
|features|bucketedFeatures|
+--------+----------------+
|-999.9  |0.0             |
|-0.5    |1.0             |
|-0.3    |1.0             |
|0.0     |2.0             |
|0.2     |2.0             |
|999.9   |3.0             |
+--------+----------------+
~~~

**ElementwiseProduct**  
对输入向量的每个元素乘以一个权重向量的每个元素，对输入向量每个元素逐个进行缩放  

~~~python

from pyspark.ml.feature import ElementwiseProduct
from pyspark.ml.linalg import Vectors

data = [(Vectors.dense([1.0, 2.0, 3.0]), ), (Vectors.dense([4.0, 5.0, 6.0]), )]
df = spark.createDataFrame(data, ['vector'])

transformer = ElementwiseProduct(scalingVec=Vectors.dense([0.0, 1.0, 2.0]),
                                 inputCol='vector',
                                 outputCol='transformedVector')
transformer.transform(df).show()
>>>
+-------------+-----------------+
|       vector|transformedVector|
+-------------+-----------------+
|[1.0,2.0,3.0]|    [0.0,2.0,6.0]|
|[4.0,5.0,6.0]|   [0.0,5.0,12.0]|
+-------------+-----------------+
~~~

**SQLTransformer**  
实现了用SQL对数据的转换操作，现只支持“SELECT ... FROM __THIS__”这种SQL语法，‘__THIS__’代表的是输入数据对应的DataSet，select语句中可以选择字段，常量或者表达式，可以支持所有select语法，甚至可以使用PYSPARK SQL内置的函数或者用户自定义的方法udf  

~~~python

from pyspark.ml.feature import SQLTransformer
from pyspark.sql.functions import *

df = spark.createDataFrame([(0, 1.0, 3.0), (2, 2.0, 5.0)], ['id', 'v1', 'v2'])
sqlTran = SQLTransformer(
    statement=
    "SELECT*, (v1+v2) AS v3,(v1*v2) AS v4, sqrt(v1) as Sqrt FROM __THIS__")
sqlTran.transform(df).show()
>>> 
+---+---+---+---+----+------------------+
| id| v1| v2| v3|  v4|              Sqrt|
+---+---+---+---+----+------------------+
|  0|1.0|3.0|4.0| 3.0|               1.0|
|  2|2.0|5.0|7.0|10.0|1.4142135623730951|
+---+---+---+---+----+------------------+
~~~

**VectorAssembler**  
它是一个Transformer，作用是把给定列中的list数据组合到一个单独的list中  

~~~python

from pyspark.ml.linalg import Vectors
from pyspark.ml.feature import VectorAssembler

dataset = spark.createDataFrame(
    [(0, 18, 1.0, Vectors.dense([0.0, 10.0, 0.5]), 1.0)],
    ['id', 'hour', 'mobile', 'userFeatures', 'clicked'])
assembler = VectorAssembler(inputCols=['hour', 'mobile', 'userFeatures'],
                            outputCol='features')
output = assembler.transform(dataset)
output.select('features', 'clicked').show(truncate=False)
>>> 
+-----------------------+-------+
|features               |clicked|
+-----------------------+-------+
|[18.0,1.0,0.0,10.0,0.5]|1.0    |
+-----------------------+-------+
~~~
这个转换器在机器学习中可以把不同特征转换器转换得到的特征整合到一起

**VectorSizeHint**  
用于设定指定列元素的个数，对于不符合特征个数的列数据直接过滤掉  

~~~python

from pyspark.ml.linalg import Vectors
from pyspark.ml.feature import VectorAssembler, VectorSizeHint

dataset = spark.createDataFrame(
    [(0, 18, 1.0, Vectors.dense([0.0, 10.0, 0.5]), 1.0),
     (0, 18, 1.0, Vectors.dense([0.0, 10.0]), 0.0)],
    ['id', 'hour', 'mobile', 'userFeatures', 'clicked'])
sizeHint = VectorSizeHint(inputCol='userFeatures',
                          handleInvalid='skip',
                          size=3)
datasetWithSize = sizeHint.transform(dataset)
datasetWithSize.show(truncate=False)
>>> 
+---+----+------+--------------+-------+
|id |hour|mobile|userFeatures  |clicked|
+---+----+------+--------------+-------+
|0  |18  |1.0   |[0.0,10.0,0.5]|1.0    |
+---+----+------+--------------+-------+
~~~
对于不符合size大小为3行数据，直接使用’skip‘策略给过滤掉，这里可能会问，为什么要过滤掉元素个数不满足的情况，原因是可能使用前面的VectorAssembler将多列数据整合到一个大的列表中，如果某些列的个数不相同，那组合出来的列维度也不相同，会给算法带来困扰
~~~python

VectorAssembler(inputCols=['hour', 'mobile', 'userFeatures'],
                outputCol='features')
output = assembler.transform(datasetWithSize)
print(
    'Assembled columns "hour","mobile","userFeatures" to vector column "features"'
)
output.select('features', 'clicked').show(truncate=False)
>>>
Assembled columns "hour","mobile","userFeatures" to vector column "features"
+-----------------------+-------+
|features               |clicked|
+-----------------------+-------+
|[18.0,1.0,0.0,10.0,0.5]|1.0    |
+-----------------------+-------+
~~~

**Imputer**   
用于计算数据集中的缺失值，然后用均值或中位数进行填充，也可以接受用户指定的值进行填充。默认的填充策略为mean，可以通过strategy关键字参数指定为median。对于缺失值，可以通过missingValue关键字参数值指定，例如指定-1为缺失值  

~~~python

from pyspark.ml.feature import Imputer

df = spark.createDataFrame([(1.0, float('nan')), (2.0, float('nan')),
                            (float('nan'), 3.0), (4.0, 4.0), (5.0, 5.0)],
                           ['a', 'b'])
df.show()
>>> 
+---+---+
|  a|  b|
+---+---+
|1.0|NaN|
|2.0|NaN|
|NaN|3.0|
|4.0|4.0|
|5.0|5.0|
+---+---+

imputer = Imputer(inputCols=['a','b'],outputCols=['out_a','out_b'])
model = imputer.fit(df)
model.transform(df).show()
>>> 
+---+---+-----+-----+
|  a|  b|out_a|out_b|
+---+---+-----+-----+
|1.0|NaN|  1.0|  4.0|
|2.0|NaN|  2.0|  4.0|
|NaN|3.0|  3.0|  3.0|
|4.0|4.0|  4.0|  4.0|
|5.0|5.0|  5.0|  5.0|
+---+---+-----+-----+

imputer = Imputer(inputCols=['a','b'],outputCols=['out_a','out_b'], strategy='median')
model = imputer.fit(df)
model.transform(df).show()
>>> 
+---+---+-----+-----+
|  a|  b|out_a|out_b|
+---+---+-----+-----+
|1.0|NaN|  1.0|  4.0|
|2.0|NaN|  2.0|  4.0|
|NaN|3.0|  2.0|  3.0|
|4.0|4.0|  4.0|  4.0|
|5.0|5.0|  5.0|  5.0|
+---+---+-----+-----+

imputer = Imputer(inputCols=['a', 'b'],
                  outputCols=['out_a', 'out_b'],
                  strategy='median',
                  missingValue=4.0)
model = imputer.fit(df)
model.transform(df).show()
>>> 
+---+---+-----+-----+
|  a|  b|out_a|out_b|
+---+---+-----+-----+
|1.0|NaN|  1.0|  NaN|
|2.0|NaN|  2.0|  NaN|
|NaN|3.0|  NaN|  3.0|
|4.0|4.0|  2.0|  3.0|
|5.0|5.0|  5.0|  5.0|
+---+---+-----+-----+
~~~

**StringIndexer**  
采用连续的从0开始到字符类别个数前开后闭的整数，为每个字符编码，编码使用字符的频率，频率最高的编码为0，频率最低的编码为字符的类别数-1  

~~~python

from pyspark.ml.feature import StringIndexer

df = spark.createDataFrame([(0, 'a'), (1, 'b'), (2, 'c'), (3, 'a'), (4, 'a'),
                            (5, 'c')], ['id', 'category'])
indexer = StringIndexer(inputCol='category', outputCol='categoryIndex')
indexed = indexer.fit(df).transform(df)
indexed.show()
>>> 
+---+--------+-------------+
| id|category|categoryIndex|
+---+--------+-------------+
|  0|       a|          0.0|
|  1|       b|          2.0|
|  2|       c|          1.0|
|  3|       a|          0.0|
|  4|       a|          0.0|
|  5|       c|          1.0|
+---+--------+-------------+
~~~

**VectorIndexer**  
主要作用：提高决策树或随机森林等ML方法的分类效果  
VectorIndexer是对数据集特征向量中的类别（离散值）特征（index categorical features categorical features）进行编号。它能够自动判断哪些特征是离散值型的特征，并对他们进行编号，具体做法是通过设置一个maxCageories，特征向量中某一个特征不重复取值个数小于maxCategories，则被重新编号0~K（K <= maxCategories-1）。某一个特征不重复取值个数大于maxCateGories，则该特征视为连续值，不会重新编号（不会发生任何改变）  

~~~python

from pyspark.ml.feature import VectorIndexer

data = spark.read.format('libsvm').load('D:/data/mllib/sample_libsvm_data.txt')
indexer = VectorIndexer(inputCol='features',outputCol='indexed', maxCategories=10)
indexerModel = indexer.fit(data)
categoricalFeatures = indexerModel.categoryMaps
print('Chose %d categorical features: %s' %
      (len(categoricalFeatures), ','.join(
          str(k) for k in categoricalFeatures.keys())))

# Create new column 'indexed' with categorical values transfromed to indices 
indexedData = indexerModel.transform(data)
indexedData.show()
>>> 
Chose 351 categorical features: 645,69,365,138,101,479,333,249,0,555,666,88,170,115,276,308,5,449,120,247,614,677,202,10,56,533,142,500,340,670,174,42,417,24,37,25,257,389,52,14,504,110,587,619,196,559,638,20,421,46,93,284,228,448,57,78,29,475,164,591,646,253,106,121,84,480,147,280,61,221,396,89,133,116,1,507,312,74,307,452,6,248,60,117,678,529,85,201,220,366,534,102,334,28,38,561,392,70,424,192,21,137,165,33,92,229,252,197,361,65,97,665,583,285,224,650,615,9,53,169,593,141,610,420,109,256,225,339,77,193,669,476,642,637,590,679,96,393,647,173,13,41,503,134,73,105,2,508,311,558,674,530,586,618,166,32,34,148,45,161,279,64,689,17,149,584,562,176,423,191,22,44,59,118,281,27,641,71,391,12,445,54,313,611,144,49,335,86,672,172,113,681,219,419,81,230,362,451,76,7,39,649,98,616,477,367,535,103,140,621,91,66,251,668,198,108,278,223,394,306,135,563,226,3,505,80,167,35,473,675,589,162,531,680,255,648,112,617,194,145,48,557,690,63,640,18,282,95,310,50,67,199,673,16,585,502,338,643,31,336,613,11,72,175,446,612,143,43,250,231,450,99,363,556,87,203,671,688,104,368,588,40,304,26,258,390,55,114,171,139,418,23,8,75,119,58,667,478,536,82,620,447,36,168,146,30,51,190,19,422,564,305,107,4,136,506,79,195,474,664,532,94,283,395,332,528,644,47,15,163,200,68,62,277,691,501,90,111,254,227,337,122,83,309,560,639,676,222,592,364,100
+-----+--------------------+--------------------+
|label|            features|             indexed|
+-----+--------------------+--------------------+
|  0.0|(692,[127,128,129...|(692,[127,128,129...|
|  1.0|(692,[158,159,160...|(692,[158,159,160...|
|  1.0|(692,[124,125,126...|(692,[124,125,126...|
|  1.0|(692,[152,153,154...|(692,[152,153,154...|
|  1.0|(692,[151,152,153...|(692,[151,152,153...|
|  0.0|(692,[129,130,131...|(692,[129,130,131...|
|  1.0|(692,[158,159,160...|(692,[158,159,160...|
|  1.0|(692,[99,100,101,...|(692,[99,100,101,...|
|  0.0|(692,[154,155,156...|(692,[154,155,156...|
|  0.0|(692,[127,128,129...|(692,[127,128,129...|
|  1.0|(692,[154,155,156...|(692,[154,155,156...|
|  0.0|(692,[153,154,155...|(692,[153,154,155...|
|  0.0|(692,[151,152,153...|(692,[151,152,153...|
|  1.0|(692,[129,130,131...|(692,[129,130,131...|
|  0.0|(692,[154,155,156...|(692,[154,155,156...|
|  1.0|(692,[150,151,152...|(692,[150,151,152...|
|  0.0|(692,[124,125,126...|(692,[124,125,126...|
|  0.0|(692,[152,153,154...|(692,[152,153,154...|
|  1.0|(692,[97,98,99,12...|(692,[97,98,99,12...|
|  1.0|(692,[124,125,126...|(692,[124,125,126...|
+-----+--------------------+--------------------+
only showing top 20 rows

~~~
