
**正则化**  
    例如要对输入的一张图片去做分类，判断是猫还是狗，那对于RGB图像32*32*3的大小的矩阵可以把它reshape成一个3072大小的向量，对于向量中的每个像元素看成线性方程X，对于每个X会有一个系数w，在训练的过程中不断调整w使其能正确的预测图片的分类  
![2019730 9240.png](0)
模型训练好之后，可以通过Y = 2X+b 这个模型获取每个类别的分数。假设最终的分类类别由10个  
![2019730 92419.png](1)
对于一个更具体的实例如下：  
![2019730 92513.png](2)
为了方便说明，假设输入的图片只有四个像素，每个像素对应的值形成一个X向量，用一个W矩阵和向量相乘再加上偏置项即可得到每个类别的分数  
0.2 * 56 + (-0.5) * 231 + 0.1 * 24 + 2.0 * 2 +1.1   
 -96.8  
 经过计算可以看到预测为猫的分数是最低的。而预测为狗的分数是最高的，因此预测错误，这就产生了误差，再机器学习里面叫损失，我们要根据这个损失去调整权重矩阵W，以使得分类朝着猫的方向发展  

对于这类线性的模型，我们抽象出统一的线性方程f(Xi，W，b) = WXi + b  
对于前面出现的预测出现的损失，我们需要找到一种衡量损失的方法。例如对于猫图片的分类  
![image.png](attachment:image.png)
第一列表示猫图片预测为猫的分数为3.2，预测为汽车的概率为5.1，预测为蛙的概率为-1.7，很明显分类是错误的；第二列本身为汽车图片，预测为猫的分数是1.3，预测为汽车的分数为4.9，预测为蛙的分数为2.0，分类正确；第三种蛙的图片预测为猫的分数为2.2，预测为汽车的分数为2.5，预测为蛙的分数为-3.1，很明显分类错误  
既然分类错误，我们需要找到一个衡量错误的方法，第一种衡量损失的方法为Hinge-loss折页损失，二维表示形式为一条折线，通用表达式如下：  
$$L_i = \Sigma_{j\ne yi}max(0,s_j - s_{y_i} + 1) $$
如果被正确分类的损失为0，否则损失为Sj-Syi+1。Sj表示预测为错误类别的得分，Syi表示预测为正确类别的得分 
![image.png](attachment:image.png)
由此定义，计算上面的分类的损失：  
![image.png](attachment:image.png)
可得到蛙预测的损失最大，而汽车预测正确，损失为0，猫预测成了汽车损失值为2.9整体损失为2.9+0+10.9，整体平均损失为13.8/3 = 4.6，而我们的目标是使用这个损失去更新权重参数，使得损失值最小。  
Hinge-loss损失函数完整定义如下：  
$$f(x,W)=W_x$$
$$L = \frac{1}{N} \Sigma^N_{i=1} \Sigma_{j \ne yi} max(0, f(x_i;W)_j - f(x_i;W)_{yi}+1 $$
虽然可以通过损失值调整权重参数，但不同的权重参数对结果会有什么不同的影响：  
x = [1,1,1,1]  
w1 = [1,0,0,0]  
w2 = [0.25,0.25,0.25,0.25]  
分别有输入向量x，和权重参数w1和w2  
x'W1 =  1*1 + 1* 0 + 1*0 +1*0 = 1  
x'W2 = 0.25 * 1 + 0.25 * 1+ 0.25 * 1+ 0.25 *1 =1  
对比两个结果，都为1，但是哪个权重参数更好呢，如果把x想象成输入的图片，w1只关注了图片上的局部，而w2关注了整张图片，w1不具备泛化能力容易出现过拟合，而w2泛化能力更强且不容易过拟合到数据的局部特征。因此说权重参数也是对结果产生影响的，怎样避免或者减少这个影响？这里引出了**正则化**这个概念。正则化实际上是将权重带来的损失添加到损失函数中，有两种常见的方法就是L1和L2正则化  
- L1正则化是指权值向量w中各个元素的**绝对值之和**，通常表示为||w||1  
- L2正则化是指权值向量w中各个元素的平方和然后求平方根（可以看到Ridge回归的L2正则化项有平方符号），通常表示为||w||2  
$\color{red}{- L1正则化可以产生稀疏权值矩阵，即产生一个稀疏模型，可以用于特征选择}$
$\color{red}{- L2正则化可以防止模型过拟合（overfitting）；一定程度上，L1也可防止过拟合}$  
现在加入权重参数的正则化项后，整个损失函数变成如下样子：  
$$L=\frac{1}{N} \Sigma^N_{i=1} \Sigma_{j\ne yi} max(0,f(x_i;W)_j - f(X_i;W)_{yi} + 1) + \lambda R(W)$$
其中**λ**为惩罚系数，用于调节分类损失和权重参数损失  
以后遇到正则化惩罚项，其实都是指的对权重参数的惩罚。不管是L1，还是L2，只要知道他们对模型的影响就可以了 
  
损失函数，损失函数除了Hinge-loss之外，还有一种常见的损失函数叫交叉熵损失。上面对猫的分类中，分类都是以分数来衡量的。  
![image.png](attachment:image.png)
那对于这种分数有什么不好的地方呢？直观上是没有归一和统一化，分数有大有小，还有负值。那能不能将这种分数直接以某种方式变成百分数比例呢？这就是softmax要做的，先来看下Softmax函数的定义：  
$$f_j(z) = \frac{e^{zj}}{\Sigma_{k^{e^{zk}}}}$$ 
被称作**softmax 函数**  
其输入值为一个向量，向量中的每个值为任意的实数；输出值为一个向量且每个元素位于0~1之间，且元素之和为1。完美！这不就将得分变成了概率分布了吗！  
公式中为得到e的得分次幂，使用e为底的幂运算进行了缩放，分母为所有预测类别得分经过e为底的幂运算放缩后的和，可以把它看成归一化因子。得到了每个分类的概率，怎样用概率定义损失呢？概率越小，损失越大，概率越大，损失越小。基于这种关系选择-log不就可以完成了吗？因此最终每个类别的损失定义为：  
$$L_i = -log(\frac {e^{f_yi}}{\Sigma_{j^{e^{fj}}}})$$
这个损失也被称为交叉熵损失  
![image.png](attachment:image.png)
接下来使用交叉熵来计算一下分类的损失  
![image.png](attachment:image.png)
可以看出概率值小的，最终损失值就越大！  
接下对比折页损失和交叉熵损失：  
![image.png](attachment:image.png)
得到了总体的损失之后，可以使用相应的策略来优化相关的参数了，例如在神经网络中经过PB算法来更新W参数  
![image.png](attachment:image.png)
