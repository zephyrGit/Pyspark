
1. RDD是spark里面最重要的基础抽象，代表的是弹性的分布式的数据集，RDD有很多的实现类，在各个RDD之上提供了transformation喝action两大类算子。transformation算子具有惰性，他们并不会触发作业的提交，一个个的transform算子操作只是定义出了计算所依赖的DAG有向无环图，他只是一个计算的逻辑，而真正会触发作业提交的算子属于action类别的算子。

### action类的算子
2. spark中如何实现分布式并行的计算一个数据集中所有元素的和？这属于一个聚合操作，spark先把数据分区，每个分区由任务调度器分发到不同的节点分别计算和，计算的结果返回driver节点，在driver节点将所有节点的返回值相加，就得到了整个数据集的和了。spark里实现这样一个算子aggregate(zeroValue,seqOp,combOp)  
zeroValue是每个节点相加的初始值，seqOp为每个节点上调用的运算函数，combOp为节点返回结果调用的运算函数  


```python
# 创建aggregate.py文件
from pyspark import SparkContext, SparkConf
import numpy as np

conf = SparkConf()
conf.setMaster("local").setAppName("action")
# conf.set("master", "spark://hadoop-maste:7077")
sc = SparkContext(conf=conf)

rdd = sc.parallelize(np.arange(11),3)
rdd.collect()
```




    [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10]




```python
rdd.aggregate(0, lambda x,y:x+y, lambda x,y:x+y)
```




    55




```python
rdd.aggregate(8, lambda x,y:x+y, lambda x,y:x+y)
```




    87




```python
rdd.aggregate(3, lambda x,y:x+y, lambda x,y:x+y)
```




    67



~~~shell
>>> cat ~/.ssh/id_rsa.pub
>>> docker exec -it hadoop-maste /bash
>>> vim ~/.ssh/authorized_keys
>>> pyspark --master spark://hadoop-matse:7077
>>> spark-submit aggregate.py 
~~~

从结果上看到每个分区调用seqOp函数都要加上zeroValue，最后运行combOp也要加上zeroValue。3个分区加上最后的combOp所以总共加了四次zeroValue  

3. aggregateByKey(zeroValue,seqFunc,combFunc,numPartitons=None,partitionFunc=<function portable_hash>)这个方法用来对先统的key值进行聚合操作，同样的是指定zeroValue，seqFunc，num_Partitons为分区数，partitionFunc为作用在分区上的函数。  
这个方法通aggregate方法名字相似，但是它趋势一个transformation方法，不会触发作业的提交！  


```python
datas = [("a",22),("b",33),("c",44),("b",55),('a',66)]
rdd1 = sc.parallelize(datas,3)
rdd1.collect()
```




    [('a', 22), ('b', 33), ('c', 44), ('b', 55), ('a', 66)]




```python
rdd1.aggregateByKey(1, lambda x,y:x+y,lambda x,y:x+y,1).collect()
```

4. collect方法  
该方法会触发作业的提交，返回一个结果的列表，注意若结果集较大，使用collect方法可能会使driver程序的崩溃，因为collect方法会返回所有节点的结果数据到driver节点，造成OMM或其它异常。


```python
rdd.collect()
```




    [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10]



5. collectAsMap()这个方法仍然是action方法，会触发作业的执行，顾名思义该方法返回的结果是一个字典结构  


```python
dd = rdd1.collectAsMap()
```


```python
type(dd)
```




    dict




```python
dd
```




    {'a': 66, 'b': 55, 'c': 44}



6. count()方法，统计RDD中元素的个数  


```python
rdd1.count()
```




    5



7. countApprx(timeout, confidence=0.95)  
带有超市限制的count统计函数，时间一到，即便所有的任务还没有完成，该方法也会返回已经完成的任务统计的结果  


```python
rdd2 = sc.parallelize(range(100000),100)
```


```python
rdd2.countApprox(1000, 1.0)
```




    100000




```python
rdd2.count()
```




    100000




```python
rdd2.countApprox(2000, 1.0)
```


```python
rdd.countApprox(5000, 1.0)
```

对于不同的时间，返回结果是不一样的

8. countApproxDistinct(relativeSD=0.05)  
返回大概的RDD数据集中没有重复的数据条数  


```python
rdd3 = sc.parallelize(range(1000),10)
rdd4 = sc.parallelize(np.arange(1000),10)
rdd5 = sc.union([rdd3, rdd4])
rdd5.countApproxDistinct()

rdd5.countApproxDistinct(relativeSD=0.01/0.001)

```

9. countByKey()  
统计相同key的个数，以字典形式返回  


```python
datas = [("a",11),("b",22),("b",88),("c",55)]
rdd = sc.parallelize(datas,3)
rdd.countByKey()
```




    defaultdict(int, {'a': 1, 'b': 2, 'c': 1})



10. countByValue()  
统计值出现的次数，以值为键，次数为值返回字典


```python
datas = ['a','b','c','d','a','a','b','c']
rdd = sc.parallelize(datas,3)
rdd.countByValue()
```




    defaultdict(int, {'a': 3, 'b': 2, 'c': 2, 'd': 1})



11. first()返回RDD中第一条记录  


```python
rdd = sc.parallelize(range(10))
rdd.first()
```




    0




```python
# 注意emptyRDD调用是会报错的！
rdd = sc.emptyRDD()
rdd.first()
```


    ---------------------------------------------------------------------

    ValueError                          Traceback (most recent call last)

    <ipython-input-20-b731d1fffc0f> in <module>()
          1 rdd = sc.emptyRDD()
    ----> 2 rdd.first()
    

    D:\Anaconda\lib\site-packages\pyspark\rdd.py in first(self)
       1377         if rs:
       1378             return rs[0]
    -> 1379         raise ValueError("RDD is empty")
       1380 
       1381     def isEmpty(self):
    

    ValueError: RDD is empty


12. fold(zeroValue, op) 该方法使用给定的zeroValue和op方法，先聚合每一个partitions中的记录，然后全局聚合


```python
rdd = sc.parallelize(range(11),3)
rdd.fold(0, lambda x,y:x+y)
```




    55




```python
rdd.fold(1, lambda x,y:x+y)
```




    59



该方法类似于aggragete方法，aggtagete方法要求提供两个op，而fold方法只需要提供一个op即可！

13. foreachf(f)在RDD中的每个元素上应用f方法


```python
sc.parallelize(range(10),3).foreach(lambda x:x**2)
```

该方法确实是action方法，但是用处不死特别大

- 14 foreachPartition(f)在每个分区上应用一个方法f，这个方法就很有用了，例如可以在该方法中建立与关系行数据库的连接，将数据保存到mysql或其它数据库中

~~~python
>>> def f(iter):
>>>    for i in iter
>>>        save to mysql xxxx
        
>>> sc.parallelize(range(10),3).foreachPartition(f)
~~~

- 15.getNumPartitions(),这个方法不是action方法，但是可以得到RDD上的分区数，立即执行有返回值！
~~~python
>>> rdd = sc.parallelize(range(100),3)
>>> rdd.getNumPartitions()
>>> 3
~~~

- 16.getStorageLevel()得到当前RDD的缓存级别
~~~python
>>> rdd.getStorageLevel()
StorageLevel(False, False,False,1)
>>> print(rdd.getStorageLevel())
~~~

- 17.id()返回该RDD的唯一值
~~~python
>>> rdd.id()
306
~~~

- 18. histogram(buckets)通过设置的bin区间，返回对应的直方图数据，结果为一个二元tuple，tuple第一个元素为区间，第二个元素为区间上元素的个数
~~~python
>>> rdd = sc.parallelize(range(20))
>>> rdd.collect()
>>> rdd.histogram([0, 6, 20])
>>> ([0, 6, 20], [6, 14])
~~~

- 19.isCheckpointed()返回该RDD是否checkpointed了
~~~python
>>> rdd.isCheckpointed()
>>> False
~~~

- 20.isEmpty()返回该RDD是否为empty的RDD，即RDD中没有元素
~~~python
>>> rdd1 = sc.emptyRDD()
>>> rdd1.isEmpty()
>>> True
~~~

- 21.lokkup(key)由key值在RDD中查找数据
~~~python
>>> rdd = sc.parallelize([('a', 1),('b',2),('c',3),('a',11)],3)
>>> rdd.lookup('a')
>>> rdd.lookup('b')
~~~

- 22.mean()返回RDD中元素的均值
~~~python
>>> rdd = sc.parallelize(range(20), 3)
>>> rdd.mean()
>>> 9.5
~~~

- 23.min(key=None)/max()返回RDD中元素的最小/最大值
~~~python
>>> rdd.min()
>>> rdd.max()
~~~

- 24.name()返回RDD的名称
~~~python
>>> rdd.setName("myrdd")
>>> rdd.name()
>>> 'myrdd'
~~~

- 25.reduce(f)使用指定的方法对RDD中的数据进行聚合操作
~~~python
>>> rdd = sc.parallelize(range(20),100)
>>> rdd.reduce(lambda x,y: x+y)
>>> 190
>>> sum(rdd.collect())
>>> 190
~~~
可以从operator模块导入add二元操作函数，直接用于求和，更加简洁
~~~python
>>> from operator import add
>>> rdd.reduce(add)
>>> 190
~~~

- 26.reduceByKeyLocally()通过key进行聚合计算，返回一个字典结构
~~~python
>>> rdd = sc.parallelize([('a',10),('b',19),('a',88)],3)
>>> rdd.reduceByKeyLocally(add)
>>> {'a':98, 'b':19}
~~~

- 27.sampleStdev()计算RDD中元素的标准差，注意样本的标准差除的是N-1而不是样本的个数N
~~~python
>>> rdd = sc.parallelize(np.arange(1, 10, 2),3)
>>> rdd.sampleStdev()
>>> 3.162277660
~~~
验证除的是N-1 不是N
~~~python
>>> rdd.collect()
>>> [1,3,5,7,9]
>>> rdd.mean()
>>> 5
>>> (1-5)**2+(3-5)**2+(5-5)**2+(7-5)**2+(9-5)**2 
>>> 40. / 4.
>>> 10.0
>>> import math
>>> math.sqrt(10.)
>>> 3.16227766
~~~

- 28.sampleVariance()RDD中元素的方差，同sampleStdev()方法类似，除的是N-1
~~~python
>>> rdd = sc.parallelize(np.arange(1,10,2),3)
>>> rdd.sampleVariance()
>>> 10.0
~~~

- 29.saveAsPickleFile()使用pyspark.serializers.PickleSerializer序列化器将RDD数据持久化，使用sparkcontext的pickleFile读取
~~~python
>>> sc.parallelize(range(100),3).saveAsPickleFile('datas/pickles/ccc',5)
>>> sorted(sc.pickleFile('datas/pickles/ccc', 3).collect())
~~~

- 30.saveAsTextfile并指定压缩格式
~~~python
>>> sc.pickleFile('datas/pickles/ccc',3).saveAsTextFile('datas/compress/test','org.apache.hadoop.io.compress.GzipCodec')
>>> hdfs dfs -ls /datas/compress
>>> hdfs dfs -ls /datas/compress/test
~~~

- 31.saveAsSequenceFile(path,compressionCodecClass=None)将RDD保存为序列文件，注意：只能保存key，value结构的RDD数据！可以指定compressionCodecClass为“org.apache.hadoop.io.compress.GzipCodec”
~~~python
>>> rdd = sc.parallelize(range(100),3).map(lambda x: (x,str(x)+"——hello"))
>>> rdd.saveAsSequenceFile('/datas/seq/sequencefile')
>>> rdd.saveAsSequenceFile('./datas/seq/sequencefile1', compressionCodecClass='org.apache.hadoop.io.compress.GzipCodec')
~~~
保存后使用sparkcontext的sequenceFile进行读取
~~~python
>>> sc.sequenceFile('./datas/seq/sequencefile').collect()
~~~

- 32.stats()返回一个StatCounter对象，它包含RDD元素的均值，方差等信息
~~~python
>>> rdd = sc.parallelize(range(100),3)
>>> rdd.stats()
>>> (count: 100, mean: 49.5, stdev: 28.86607004772212, max: 99.0, min: 0.0)
~~~

- 33.stdev()计算RDD元素的标准差，注意该方法除的是N，而sampleStdev除的是N-1
~~~python
>>> rdd.stdev()
>>> 28.86607004772212
>>> 29.011491975882016
~~~

- 34.sum()求RDD中所有元素的和
~~~python
>>> rdd.sum()
>>> 4950
~~~

- 35.sumApprox(timeout,confidence=0.95)带有超时限制的求和估计方法
~~~python
>>> rdd.sumApprox(1)
>>> 4950
~~~

- 36.take()获取RDD前num条数据
~~~python
>>> rdd.take(10)
>>> [0, 1, 2, 3, 4, 5, 6, 7, 8, 9]
~~~

- 37.takeOrdered(num, key=None)按照某周顺序取出num条数据，排序方法可以由key关键字指定
~~~python
>>> rdd = sc.parallelize([2,1,3,6,9,1,3,5,6,7,2,3],3)
>>> rdd.takeOrdered(5)
>>> [1, 1, 2, 2, 3]
>>> rdd.takeOrdered(5, key=lambda x: -x)
>>> [9, 7, 6, 6, 5]
~~~

- 38.takeSample(withReplacement, num, seed=None)从RDD数据集中采样，withReplacement指定是否有放回，num指定采样个数，seed指定采样的种子，便于结果复现！
~~~python
>>> rdd.takeSample(True,10,1)
>>> [3, 6, 7, 1, 2, 7, 1, 9, 6, 2]
>>> rdd.takeSample(True,10,2)
>>> [7, 5, 6, 1, 3, 2, 2, 9, 5, 3]
~~~

- 39.toDebugString()返回RDD的调试模式的字符描述
~~~python
>>> toDebugString
>>> b'(3) ParallelCollectionRDD[68] at parallelize at PythonRDD.scala:194 []'
~~~

- 40.toLocalIterator()将RDD中的元素返回为本地可迭代数据集，因此要保证贝蒂内存足够大，能满足存放数据的要求
~~~python
>>> local = rdd.toLocalIterator()
>>> type(local)
>>> itertools.chain
>>> for i in local:
    print(i)
~~~

- 41.top(num, key=None)返回RDD中从大到校排序的前num个，默认降序排序，可以通过关键参数key指定排序方式，数据量太大的时候，需要注意内是否满足!
~~~python
>>> rdd.top(5)
>>> [9, 7, 6, 6, 5]
>>> rdd.top(5, key=lambda x: -x)
>>> [1, 1, 2, 2, 3]
~~~

- 42.treeAggregate(zeroValue, seqOp, combOp, depth=2)该方法实现了多层次的树算法来对数据进行聚合，使用类似aggregate()方法
~~~python
>>> from operator import add
>>> rdd.treeAggregate(0, add, add, 4)
>>> 48
>>> rdd.treeAggregate(0, add, add, 5)
>>> 48
>>> rdd.treeAggregate(0, add, add, 50)
>>> 48
>>> rdd.treeAggregate(0, add, add, 2)
>>> rdd.treeAggregate(1, add, add, 5)
>>> 51
~~~

- 43.variance(t, depth=2)实现了多层次的树模型算法来提上速度，使用同reduce方法类似
~~~python
>>> rdd.treeReduce(add, 3)
>>> 48
>>> rdd.treeReduce(add, 4)
>>> 48
~~~

- 44.variance()返回方差，注意和sampleVariance方法的区别，sampleVariance这个方法除的是N-1，而Variance除的是N
~~~python
>>> rdd.variance()
>>> 6.0
>>> rdd.sampleVariance()
>>> 6.545454545454546
~~~
