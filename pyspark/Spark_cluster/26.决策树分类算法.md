
### 决策树-熵  
P(X,Y) = P(X) * P(Y) X和Y两个事件相互独立 Log(XY)=Log(X)+Log(Y)  
P(X),H(Y) 当成他们发生的不确定性  
P（几率越大）-> H（x）值越小 如：今天正常上班  
P（几率越小）-> H（x）值越大 如：今天没翻车  
$$熵 = - \Sigma^n_{i=1}P_iln(p_i)$$
$$Gini系数=Gini(p)=\Sigma_{k=1}^{K}p_k(1-p_k)=1-\Sigma_{k=1}^{K}p_k^2$$
熵是无序性（或不确定性）的度量指标。假如事件A的全概率划分是（A1,A2,...,An），每部分发生的概率是（p1,p2,...,pn），那信息熵定义为：  
$$entropy(p1,p2,...pn)=-p_1log_2p_1-p_2log_2p_2-...-p_nlog_2p_n$$
构造树的基本想法是随着树深度的增加，节点的熵迅速地降低。熵降低地速度越快越好，这样我们有望得到一颗高度最矮地决策树  
接下来使用libSVM格式地数据集，采用决策树来做分类，使用StringIndexer给String类型地列按照类别和频率逆序给定索引值。索引从0到String类型地种类数-1。使用VectorIndexer给特征自动编号，这个Transformation会由maxCategores去自动判断哪些特征是连续型地，哪些特征是类别型地。对于少于等于maxCategores的认为是离散的，用从0到maxCategores-1的类别进行标记，对于大于maxCategores的则认为是连续性变量，不进行任何转换。推荐大于四个不重复的特征值的就视为连续型变量  

~~~python

from pyspark.ml import Pipeline
from pyspark.ml.classification import DecisionTreeClassifier
from pyspark.ml.feature import StringIndexer, VectorIndexer
from pyspark.ml.evaluation import MulticlassClassificationEvaluator

data = spark.read.format('libsvm').load('D:/data/mllib/sample_libsvm_data.txt')
labelIndexer = StringIndexer(inputCol='label', outputCol='indexedLabel').fit(data)
featureIndexer = VectorIndexer(inputCol='features',
                               outputCol='indexedFeatures',
                               maxCategories=4).fit(data)
(trainingDate, testData) = data.randomSplit([0.7,0.3])
dt = DecisionTreeClassifier(labelCol='indexedLabel', featuresCol='indexedFeatures') 
pipeline = Pipeline(stages=[labelIndexer, featureIndexer, dt])
model = pipeline.fit(trainingDate)
predictions = model.transform(testData)
predictions.select('prediction', 'indexedLabel', 'features').show(5)
>>> 
+----------+------------+--------------------+
|prediction|indexedLabel|            features|
+----------+------------+--------------------+
|       1.0|         1.0|(692,[122,123,148...|
|       1.0|         1.0|(692,[123,124,125...|
|       1.0|         1.0|(692,[123,124,125...|
|       1.0|         1.0|(692,[124,125,126...|
|       1.0|         1.0|(692,[124,125,126...|
+----------+------------+--------------------+
only showing top 5 rows

evaluator = MulticlassClassificationEvaluator(labelCol='indexedLabel', predictionCol='prediction', metricName='accuracy')
accuracy = evaluator.evaluate(predictions)
print('Test Error = %g'%(1.0 - accuracy))
treeModel = model.stages[2]
print(treeModel)
>>> 
Test Error = 0.0526316
DecisionTreeClassificationModel (uid=DecisionTreeClassifier_486fad9f03cd98c146ac) of depth 2 with 5 nodes
~~~
从上面的测试结果可以看到，分类的精度达到了1-0.0526316=0.9473684‬  

**Random forest classifier**   
随机森林属于集成模型，可以通过生成随机的N课数，并从N棵树中选取性能最好的树作为最终模型。使用随机森林上的libsvm数据集进行分类。使用RadomForestClassifier创建随机森林，指定分类的标签为indexedLabel，特征向量为indexedFeatures，用numTree关键字参数指定生成10棵树组成的森林。使用MulticlassClassificationEvaluator评估器对算法的性能进行评估  

~~~python

from pyspark.sql import SparkSession
from pyspark.ml.feature import StringIndexer, IndexToString, VectorIndexer
from pyspark.ml.classification import RandomForestClassifier
from pyspark.ml.evaluation import MulticlassClassificationEvaluator
from pyspark.ml import Pipeline

spark = SparkSession.builder.master('local').appName(
    'RandomForestClassifier').getOrCreate()

data = spark.read.format('libsvm').load('/data/mllib/sample_libsvm_data.txt')

labelIndexer = StringIndexer(inputCol='label',
                             outputCol='indexedLabel').fit(data)
featureIndexer = VectorIndexer(inputCol='features',
                               outputCol='indexedFeatures',
                               maxCategories=4).fit(data)

(trainingData, testData) = data.randomSplit([0.7, 0.3])
rf = RandomForestClassifier(labelCol='indexedLabel',
                            featuresCol='indexedFeatures',
                            numTrees=10)

labelConverter = IndexToString(inputCol='prediction',
                               outputCol='predictedLabel',
                               labels=labelIndexer.labels)

pipeline = Pipeline(stages=[labelIndexer, featureIndexer, rf, labelConverter])
model = pipeline.fit(trainingData)

predictions = model.transform(testData)
predictions.select('predictedLabel', 'label', 'features').show(5)
>>> 
+--------------+-----+--------------------+
|predictedLabel|label|            features|
+--------------+-----+--------------------+
|           0.0|  0.0|(692,[95,96,97,12...|
|           0.0|  0.0|(692,[98,99,100,1...|
|           0.0|  0.0|(692,[121,122,123...|
|           0.0|  0.0|(692,[122,123,124...|
|           0.0|  0.0|(692,[123,124,125...|
+--------------+-----+--------------------+
only showing top 5 rows

evalutor =  MulticlassClassificationEvaluator(labelCol='indexedLabel',
                                             predictionCol='prediction', 
                                             metricName='accuracy')
accuracy = evalutor.evaluate(predictions)
print('Test Error = %g '%(1.0 - accuracy))
fModel = model.stages[2]
print(fModel)
>>> 
Test Error = 0.0322581 
RandomForestClassificationModel (uid=RandomForestClassifier_4af087a7715236e9a119) with 10 trees
~~~
可以看到使用随机森林，预测的精度能够达到 1 - 0.0322581 = 0.9677419‬ 比单棵树的性能要好很多  

**Grandient-boosted trees**  
在集成算法中还有一个比较著名的算法是Grandient-boosted trees，梯度提升树，它和随机森林的区别是：gbdt每棵树的生成都要达到性能的提升，而随机森林每棵树都是独立创建的。因此随机森林在某种程度上来讲是在盲目的搜索，而GBDT是在沿着正确的方向上去做提升，这也是它名字的由来梯度提升树。接下来使用梯度提升树来分类上面的数据  

~~~python

from pyspark.ml import Pipeline
from pyspark.ml.classification import GBTClassifier
from pyspark.ml.feature import StringIndexer, VectorIndexer
from pyspark.ml.evaluation import MulticlassClassificationEvaluator

data = spark.read.format('libsvm').load('D:/data/mllib/sample_libsvm_data.txt')
labelIndexer = StringIndexer(inputCol='label',
                             outputCol='indexedLabel').fit(data)
featureIndexer = VectorIndexer(inputCol='features',
                               outputCol='indexedFeatures',
                               maxCategories=4).fit(data)
(trainingData, testData) = data.randomSplit([0.7, 0.3])
gbt = GBTClassifier(labelCol='indexedLabel', featuresCol='indexedFeatures', maxIter=10)
pipeline = Pipeline(stages=[labelIndexer, featureIndexer, gbt])
model = pipeline.fit(trainingData)
predictions = model.transform(testData)
predictions.select('prediction', 'indexedLabel', 'features').show(5)
>>> 
+----------+------------+--------------------+
|prediction|indexedLabel|            features|
+----------+------------+--------------------+
|       1.0|         1.0|(692,[100,101,102...|
|       1.0|         1.0|(692,[121,122,123...|
|       1.0|         1.0|(692,[122,123,124...|
|       1.0|         1.0|(692,[123,124,125...|
|       1.0|         1.0|(692,[123,124,125...|
+----------+------------+--------------------+
only showing top 5 rows

evalutor = MulticlassClassificationEvaluator(labelCol='indexedLabel',
                                             predictionCol='prediction',
                                             metricName='accuracy')

accuracy = evalutor.evaluate(predictions)
print('Test Error = %g ' % (1.0 - accuracy))
gbtModel = model.stages[2]
print(gbtModel)
>>> 
Test Error = 0.111111 
GBTClassificationModel (uid=GBTClassifier_4725ac9193a71790be2e) with 10 trees
~~~
可以看到虽然性能虽然没有随机森林好，但是其性能也是不错的  
