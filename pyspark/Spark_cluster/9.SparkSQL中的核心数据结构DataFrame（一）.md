
### SparkSQL中的核心数据结构DataFrame

1. DataFrame是按照列名来组织数据的分布式数据集，是SparkSQL最重要的抽象，由于基于DataFrame的算法在性能和优化的余地上【Tunstun和Catalyst】有更大的空间，因此现在Spark里基于DataFrame的机器学习库ml及Structured Streaming都采用这种数据结构。而且spark基于RDD的机器学习库mllib不会再更新，最新的算法都采用基于DataFrame来实现


2. pyspark.sql.DataFrame(df, sql_ctx)一个DataFrame对象，在SparkSQL里面等价于一张关系型数据库，并且可以通过sparkSession上的很多个接口读取外部数据生成DataFrame对象


3.以read上的parquet方法，读取parquet格式文件创建DataFrame  

~~~python
from pyspark.sql import SparkSession


spark = SparkSession.builder.master('local')\
                    .appName('test')\
                    .config("spark.xxxx.conf", "some-value")\
                    .getOrCreate()
df = spark.read.parquet('./datas/users.parquet')
df.show()
>>> 
+------+--------------+----------------+
|  name|favorite_color|favorite_numbers|
+------+--------------+----------------+
|Alyssa|          null|  [3, 9, 15, 20]|
|   Ben|           red|              []|
+------+--------------+----------------+
~~~
既然DataFrame可以看成关系型数据库中的表，那自然可以通过字段的名字来获取表内容。可以直接使用df.name或df.favorite_color


4. 聚合函数agg(*exprs),传入要聚合的字段及聚合的方式，以字典的方式及逆行组合

~~~python
df = spark.read.csv('./datas/customers.csv', header=True)
df.printSchema()
df.show()
>>> 
root
 |-- CustomerID: string (nullable = true)
 |-- Genre: string (nullable = true)
 |-- Age: string (nullable = true)
 |-- Annual Income (k$): string (nullable = true)
 |-- Spending Score (1-100): string (nullable = true)

+----------+------+---+------------------+----------------------+
|CustomerID| Genre|Age|Annual Income (k$)|Spending Score (1-100)|
+----------+------+---+------------------+----------------------+
|      0001|  Male| 19|                15|                    39|
|      0002|  Male| 21|                15|                    81|
|      0003|Female| 20|                16|                     6|
|      0004|Female| 23|                16|                    77|
|      0005|Female| 31|                17|                    40|
|      0006|Female| 22|                17|                    76|
|      0007|Female| 35|                18|                     6|
|      0008|Female| 23|                18|                    94|
|      0009|  Male| 64|                19|                     3|
|      0010|Female| 30|                19|                    72|
|      0011|  Male| 67|                19|                    14|
|      0012|Female| 35|                19|                    99|
|      0013|Female| 58|                20|                    15|
|      0014|Female| 24|                20|                    77|
|      0015|  Male| 37|                20|                    13|
|      0016|  Male| 22|                20|                    79|
|      0017|Female| 35|                21|                    35|
|      0018|  Male| 20|                21|                    66|
|      0019|  Male| 52|                23|                    29|
|      0020|Female| 35|                23|                    98|
+----------+------+---+------------------+----------------------+
only showing top 20 rows
~~~
通过read上的csv方法进行读取，返回RDD，由关键字参数header指定保留文件的头信息，并使用头信息作为DataFrame的colums。调用printSchema方法打印出DataFrame的元素信息

这个DataFrame中第一列为编号，第二列为性别，第三列为年龄，第四列为实际收入，第五列为消费积分。可以使用agg聚合方法来找出Age的最大或最小值，实际收入的均值，消费积分的均值等信息。例如计算年龄最大值、收入均值，消费积分均值，对多个字段求得聚合放到一个字典中，键位列名称，值位用来计算的函数名称，当然这里的函数可以是自定义的聚合函数udaf

~~~python
df.agg({
    'Age': 'max',
    'Annual Income (k$)': 'mean',
    'Spending Score (1-100)': 'mean'
}).show()
>>> 
+---------------------------+-----------------------+--------+
|avg(Spending Score (1-100))|avg(Annual Income (k$))|max(Age)|
+---------------------------+-----------------------+--------+
|                       50.2|                  60.56|      70|
+---------------------------+-----------------------+--------+
~~~

5.alias(alias)为DataFrame定义一个别名，在函数中可以利用这个别名来做相关运算，例如自关联

~~~python
from pyspark.sql.functions import *

df = spark.read.csv('./datas/customers.csv', header=True)
df.printSchema()
df1 = df.alias('cus1')
type(df1)
>>> 
root
 |-- CustomerID: string (nullable = true)
 |-- Genre: string (nullable = true)
 |-- Age: string (nullable = true)
 |-- Annual Income (k$): string (nullable = true)
 |-- Spending Score (1-100): string (nullable = true)

pyspark.sql.dataframe.DataFrame

df2 = df.alias('cus2')
df3 = df1.join(df2, col('cus1.CustomerID') == col('cus2.CustomerID'), 'inner')
df3.count()
df.show(10)
>>> 
+----------+------+---+------------------+----------------------+
|CustomerID| Genre|Age|Annual Income (k$)|Spending Score (1-100)|
+----------+------+---+------------------+----------------------+
|      0001|  Male| 19|                15|                    39|
|      0002|  Male| 21|                15|                    81|
|      0003|Female| 20|                16|                     6|
|      0004|Female| 23|                16|                    77|
|      0005|Female| 31|                17|                    40|
|      0006|Female| 22|                17|                    76|
|      0007|Female| 35|                18|                     6|
|      0008|Female| 23|                18|                    94|
|      0009|  Male| 64|                19|                     3|
|      0010|Female| 30|                19|                    72|
+----------+------+---+------------------+----------------------+
only showing top 10 rows
~~~

6.cache()，将DataFrame缓存到StorageLevel对应的缓存级别中，默认 **MEMORY_AND_DISK**

~~~python
df = spark.read.csv('./datas/customers.csv',  header=True)
a = df.cache()
a.show()
>>>
+----------+------+---+------------------+----------------------+
|CustomerID| Genre|Age|Annual Income (k$)|Spending Score (1-100)|
+----------+------+---+------------------+----------------------+
|      0001|  Male| 19|                15|                    39|
|      0002|  Male| 21|                15|                    81|
|      0003|Female| 20|                16|                     6|
|      0004|Female| 23|                16|                    77|
|      0005|Female| 31|                17|                    40|
|      0006|Female| 22|                17|                    76|
|      0007|Female| 35|                18|                     6|
|      0008|Female| 23|                18|                    94|
|      0009|  Male| 64|                19|                     3|
|      0010|Female| 30|                19|                    72|
|      0011|  Male| 67|                19|                    14|
|      0012|Female| 35|                19|                    99|
|      0013|Female| 58|                20|                    15|
|      0014|Female| 24|                20|                    77|
|      0015|  Male| 37|                20|                    13|
|      0016|  Male| 22|                20|                    79|
|      0017|Female| 35|                21|                    35|
|      0018|  Male| 20|                21|                    66|
|      0019|  Male| 52|                23|                    29|
|      0020|Female| 35|                23|                    98|
+----------+------+---+------------------+----------------------+
only showing top 20 rows
~~~

将频繁需要查询的数据缓存起来，这样下一次要查询的时候就可以直接从内存中读取数据，从而提升数据的读取速度提升计算的效率


7. checkpoint(eager=True) 对DataFrame设置断点，这个方法是Spark2.1引入的方法，这个方法的调用会斩断在这个DataFrame上的逻辑执行计划，将前后的依赖关系持续到checkpoint文件中去。这个方法对于需要大量需要迭代的算法非常有用，因为算法在迭代的过程中逻辑计划的数据量会呈现指数级别的上升。要使用这个方法需要使用sparkcontext上面的setCheckpointDir设置检查点数据在HDFS中的存储了目录。有一个关键字参数eager，默认为True表示是否立即设置断点


~~~python
sc = spark.sparkContext
sc.setCheckpointDir('./datas/checkpoint')
a.checkpoint()
a.show()
>>> 
+----------+------+---+------------------+----------------------+
|CustomerID| Genre|Age|Annual Income (k$)|Spending Score (1-100)|
+----------+------+---+------------------+----------------------+
|      0001|  Male| 19|                15|                    39|
|      0002|  Male| 21|                15|                    81|
|      0003|Female| 20|                16|                     6|
|      0004|Female| 23|                16|                    77|
|      0005|Female| 31|                17|                    40|
|      0006|Female| 22|                17|                    76|
|      0007|Female| 35|                18|                     6|
|      0008|Female| 23|                18|                    94|
|      0009|  Male| 64|                19|                     3|
|      0010|Female| 30|                19|                    72|
|      0011|  Male| 67|                19|                    14|
|      0012|Female| 35|                19|                    99|
|      0013|Female| 58|                20|                    15|
|      0014|Female| 24|                20|                    77|
|      0015|  Male| 37|                20|                    13|
|      0016|  Male| 22|                20|                    79|
|      0017|Female| 35|                21|                    35|
|      0018|  Male| 20|                21|                    66|
|      0019|  Male| 52|                23|                    29|
|      0020|Female| 35|                23|                    98|
+----------+------+---+------------------+----------------------+
only showing top 20 rows
~~~
运行完成之后，在hdfs的/datas/checkpoint目录下会看到checkpoint的数据

~~~shell
 docker exec -it hadoop-maste /bin/bash
 hdfs dfs -ls /datas/
 hdfs dfs -ls /datas/checkpoint
~~~

8. coalesce(numPartitions)重分区算法，传入参数是DataFrame的分区数量。DataFrame上的这个方法和RDD上面的coalesce重分区方法是类似的

~~~python
df = spark.read.csv('./datas/customers.csv', header=True)
df.rdd.getNumPartitions()
>>> 1
spark.read.csv('./datas/customers.csv', header=True).coalesce(3).rdd.getNumPartitions()
>>> 1 
~~~
注意通过read方法读取文件，创建的DataFrame默认的分区数为问价的个数，即一个文件对应一个分区，在分区数少于coalesce指定的分区数的时候，调用coalesce是不起作用的

~~~python
df = spark.range(0, 20, 2, 3)
df.rdd.getNumPartitions()
>>> 3
df.coalesce(2).rdd.getNumPartitions()
>>> 2
~~~

repartition(numPartitions, *cols)这个方法和coalesce(numPartitions)方法一样，都是对DataFrame进行重新的分区，但是repartition这个方法会使用hash算法，在整个集群中进行shuffle，效率较低。repartition方法不仅可以指定分区数，还可以指定按照那些列来分区

~~~python
df = spark.read.csv('./datas/customers.csv', header=True)
df.rdd.getNumPartitions()
>>> 1
df2 = df.repartition(3)
df2.rdd.getNumPartitions()
>>> 3
~~~
指定按照Genre列进行重新分区

~~~python
df2.columns
df3 = df2.repartition(4, 'Genre')
df3.show(10)
>>> 
+----------+------+---+------------------+----------------------+
|CustomerID| Genre|Age|Annual Income (k$)|Spending Score (1-100)|
+----------+------+---+------------------+----------------------+
|      0003|Female| 20|                16|                     6|
|      0004|Female| 23|                16|                    77|
|      0005|Female| 31|                17|                    40|
|      0006|Female| 22|                17|                    76|
|      0007|Female| 35|                18|                     6|
|      0008|Female| 23|                18|                    94|
|      0010|Female| 30|                19|                    72|
|      0012|Female| 35|                19|                    99|
|      0013|Female| 58|                20|                    15|
|      0014|Female| 24|                20|                    77|
+----------+------+---+------------------+----------------------+
only showing top 10 rows
~~~

9. colRegex(colName)用正则表达式的方式返回我们想要的列


~~~python
df = spark.createDataFrame([("a", 1), ("b", 2), ("c",  3)], ["Col1", "Col2"])
df.select(df.colRegex("`(Col1)?+.+`")).show()
>>>
+----+
|Col2|
+----+
|   1|
|   2|
|   3|
+----+
~~~
上面的(Col)是一个整体，“？”表示前面的小括号里面的内容出现或者不出现，“.”表示任意字符，“+”表示出现前面的任意字符出现一次以上，既有一个以上的任意字符。


10. collect() 返回DataFrame中的所有数据，注意数据量大了容易造成Driver节点内存溢出

~~~python
df = spark.createDataFrame([('a', 1), ('b', 2), ('c', 3)], ['col', 'a'])
df.collect()
>>> [Row(col='a', a=1), Row(col='b', a=2), Row(col='c', a=3)]
~~~
