
31. groupby(*cols)这个方法是groupBy方法的别名

~~~python
from pyspark.sql import SparkSession
import numpy as np

spark = SparkSession.builder.master('local').appName('test').config(
    'spark.test.config', 'value').getOrCreate()
sc = spark.sparkContext

df = spark.read.csv('./datas/customers.csv', header=True)
df.columns
>>> ['CustomerID', 'Genre', 'Age', 'Annual Income (k$)', 'Spending Score (1-100)']
df.groupby('Genre').agg({'Age':'mean'}).show()
>>> 
+------+------------------+
| Genre|          avg(Age)|
+------+------------------+
|Female|38.098214285714285|
|  Male| 39.80681818181818|
+------+------------------+
~~~

32. head(n=None)返回DataFrame前n行数据，默认是返回1行，可以通过n关键字参数指定

~~~python
df.head(5)
>>> 
[Row(CustomerID='0001', Genre='Male', Age='19', Annual Income (k$)='15', Spending Score (1-100)='39'),
 Row(CustomerID='0002', Genre='Male', Age='21', Annual Income (k$)='15', Spending Score (1-100)='81'),
 Row(CustomerID='0003', Genre='Female', Age='20', Annual Income (k$)='16', Spending Score (1-100)='6'),
 Row(CustomerID='0004', Genre='Female', Age='23', Annual Income (k$)='16', Spending Score (1-100)='77'),
 Row(CustomerID='0005', Genre='Female', Age='31', Annual Income (k$)='17', Spending Score (1-100)='40')]
~~~

33. hint(name, *parameters)，hint方法用于两个DataFrame做join操作的时候，指定join的方式，一般为broadcast的方式。hint是暗示的意思，可以看出给程序一个暗示，按照那种方式join

~~~python
df1 = spark.createDataFrame([('tom', 23), ('ming', 24)],
                            schema=['name', 'age'])
df2 = spark.createDataFrame([('tom', 123), ('ming', 90)],
                            schema=['name', 'weight'])
df3 = df1.join(df2.hint('broadcast'), 'name').show()
>>> 
+----+---+------+
|name|age|weight|
+----+---+------+
| tom| 23|   123|
|ming| 24|    90|
+----+---+------+
~~~

34. intersect(other)返回两个DataFrame的交集是集合中的概念

~~~python
df1 = spark.createDataFrame([('tom', 23), ('ming', 24)],
                            schema=['name', 'age'])
df2 = spark.createDataFrame([('tom', 23), ('ming', 90)],
                            schema=['name', 'age'])
df3 = df1.intersect(df2).show()
>>>
+----+---+
|name|age|
+----+---+
| tom| 23|
+----+---+
~~~

35. join(other, on=None, how=None)用来对两个DataFrame做连接关联操作，other是另外一个DataFrame，on指定哪个字段做关联，how指定怎么关联，有 **inner、cross、outer、full、full_outer、left、left_outer、right、right_outer、left_semi、and left_anti**选项，默认是inner

~~~python
df1 = spark.createDataFrame([('tom',23),('ming',24)],schema=['name','age'])
df2 = spark.createDataFrame([('tom',90),('ming',90)],schema=['name','weight'])
df1.join(df2, on='name', how='left_outer').show()
>>> 
+----+---+------+
|name|age|weight|
+----+---+------+
|ming| 24|    90|
| tom| 23|    90|
+----+---+------+

df1.join(df2, on='name', how='right_outer').show()
>>>
+----+---+------+
|name|age|weight|
+----+---+------+
|ming| 24|    90|
| tom| 23|    90|
+----+---+------+
~~~

36. limit(num)限制返回的数据的条数，防止返回到driver节点的数据过大造成OOM

~~~python
df1 = spark.createDataFrame([('tom',23),('ming',24)],schema=['name','age'])
df1.limit(1).collect()
>>> [Row(name='tom', age=23)]
df1.limit(2).collect()
>>> [Row(name='tom', age=23), Row(name='ming', age=24)]
~~~

37. na 返回DataFrameNaFunctions对象，用来处理na数据。查看DataFrameNaFunctions对象内部

~~~python
df = spark.createDataFrame([('tom', 23), ('ming', 24)], schema=['name', 'age'])
dir(df.na)
>>> 
['__class__','__delattr__','__dict__','__dir__','__doc__','__eq__','__format__','__ge__','__getattribute__','__gt__','__hash__','__init__','__init_subclass__','__le__','__lt__','__module__','__ne__','__new__','__reduce__','__reduce_ex__','__repr__','__setattr__','__sizeof__','__str__','__subclasshook__','__weakref__','df','drop','fill','replace']
~~~

38. orderBy(*cols, \*** kwargs) 返回按照指定列排好序的新的DataFrame

~~~python
df = spark.read.csv('./datas/customers.csv', header=True)
df.orderBy('Age').show(3)
>>> 
+----------+-----+---+------------------+----------------------+
|CustomerID|Genre|Age|Annual Income (k$)|Spending Score (1-100)|
+----------+-----+---+------------------+----------------------+
|      0034| Male| 18|                33|                    92|
|      0066| Male| 18|                48|                    59|
|      0092| Male| 18|                59|                    41|
+----------+-----+---+------------------+----------------------+
only showing top 3 rows

df.orderBy('Age',ascending=False).show(3)
>>>
+----------+-----+---+------------------+----------------------+
|CustomerID|Genre|Age|Annual Income (k$)|Spending Score (1-100)|
+----------+-----+---+------------------+----------------------+
|      0061| Male| 70|                46|                    56|
|      0071| Male| 70|                49|                    55|
|      0058| Male| 69|                44|                    46|
+----------+-----+---+------------------+----------------------+
only showing top 3 rows
~~~
使用ascending关键字参数指定升降序排列。除了这种方式，还可以通过pyspark.sql.functions中定义好的desc和asc方法来排序

~~~python
from pyspark.sql.functions import *
df.orderBy(desc('Age')).show(3)
>>>
+----------+-----+---+------------------+----------------------+
|CustomerID|Genre|Age|Annual Income (k$)|Spending Score (1-100)|
+----------+-----+---+------------------+----------------------+
|      0061| Male| 70|                46|                    56|
|      0071| Male| 70|                49|                    55|
|      0058| Male| 69|                44|                    46|
+----------+-----+---+------------------+----------------------+
only showing top 3 rows

df.orderBy(df.Age.desc()).show(3)
>>> 
+----------+-----+---+------------------+----------------------+
|CustomerID|Genre|Age|Annual Income (k$)|Spending Score (1-100)|
+----------+-----+---+------------------+----------------------+
|      0061| Male| 70|                46|                    56|
|      0071| Male| 70|                49|                    55|
|      0058| Male| 69|                44|                    46|
+----------+-----+---+------------------+----------------------+
only showing top 3 rows
~~~
orderBy方法和sort方法类似

~~~python
df.sort(desc('Age')).show(3)
>>>
+----------+-----+---+------------------+----------------------+
|CustomerID|Genre|Age|Annual Income (k$)|Spending Score (1-100)|
+----------+-----+---+------------------+----------------------+
|      0061| Male| 70|                46|                    56|
|      0071| Male| 70|                49|                    55|
|      0058| Male| 69|                44|                    46|
+----------+-----+---+------------------+----------------------+
only showing top 3 rows

df.sort(df.Age.desc()).show(3)
>>>
+----------+-----+---+------------------+----------------------+
|CustomerID|Genre|Age|Annual Income (k$)|Spending Score (1-100)|
+----------+-----+---+------------------+----------------------+
|      0061| Male| 70|                46|                    56|
|      0071| Male| 70|                49|                    55|
|      0058| Male| 69|                44|                    46|
+----------+-----+---+------------------+----------------------+
only showing top 3 rows
~~~

39. persist(storageLevel=StorageLevel(True, True, False, False,1))用来指定DataFrame的缓存级别，默认为内存和磁盘

~~~python
from pyspark import StorageLevel
df = spark.read.csv('./datas/customers.csv', header=True)
df.persist(storageLevel=StorageLevel.MEMORY_AND_DISK_2)
>>> DataFrame[CustomerID: string, Genre: string, Age: string, Annual Income (k$): string, Spending Score (1-100): string]
~~~
StorageLevel还有其它的取值：可以使用dir(StorageLevel)查看

~~~python
dir(StorageLevel)
>>> ['DISK_ONLY','DISK_ONLY_2','MEMORY_AND_DISK','MEMORY_AND_DISK_2','MEMORY_AND_DISK_SER','MEMORY_AND_DISK_SER_2','MEMORY_ONLY','MEMORY_ONLY_2','MEMORY_ONLY_SER','MEMORY_ONLY_SER_2','OFF_HEAP','__class__','__delattr__','__dict__','__dir__','__doc__','__eq__','__format__','__ge__','__getattribute__','__gt__','__hash__','__init__','__init_subclass__','__le__','__lt__','__module__','__ne__','__new__','__reduce__','__reduce_ex__','__repr__','__setattr__','__sizeof__','__str__','__subclasshook__','__weakref__']
~~~

40. printSchema() 以树形结构的形式打印出DataFrame的shema信息

~~~python
df = spark.read.csv('./datas/customers.csv', header=True)
df.printSchema()
>>> 
root
 |-- CustomerID: string (nullable = true)
 |-- Genre: string (nullable = true)
 |-- Age: string (nullable = true)
 |-- Annual Income (k$): string (nullable = true)
 |-- Spending Score (1-100): string (nullable = true)
~~~

41. randomSplit(weights, seed=None) 按照给定的权重将DataFrame分为几个DataFrame，seed关键字参数用来指定随机种子，用于复现结果

~~~python
df = spark.range(0., 30., 2, 3)
df.describe().show()
>>>
+-------+----------------+
|summary|              id|
+-------+----------------+
|  count|              15|
|   mean|            14.0|
| stddev|8.94427190999916|
|    min|               0|
|    max|              28|
+-------+----------------+

dfs = df.randomSplit([1.0, 4.0], 24)
for df in dfs:
    df.show()
 
>>> 
+---+
| id|
+---+
| 12|
| 26|
+---+

+---+
| id|
+---+
|  0|
|  2|
|  4|
|  6|
|  8|
| 10|
| 14|
| 16|
| 18|
| 20|
| 22|
| 24|
| 28|
+---+
~~~

42. rdd返回DataFrame对应的RDD对象，利用这个对象可以调用RDD上的所有的方法，但是这些方法是比较底层的方法，在处理一些特殊任务的时候，顶层的DataFrame的方法可能无法解决，需要转换到更底层的RDD上来进行操作

~~~python
df = spark.range(0., 30., 2, 3)
rdd = df.rdd
rdd.map(lambda x: x.id**2).collect()
>>> [0, 4, 16, 36, 64, 100, 144, 196, 256, 324, 400, 484, 576, 676, 784]
~~~

43. registerTempTable(name)使用给定的名字把当前的DataFrame注册成为一个临时的表

~~~python
df = spark.range(0., 30., 2, 3)
df.registerTempTable('A')
spark.sql('select * from A').show()
>>> 
+---+
| id|
+---+
|  0|
|  2|
|  4|
|  6|
|  8|
| 10|
| 12|
| 14|
| 16|
| 18|
| 20|
| 22|
| 24|
| 26|
| 28|
+---+
~~~
注册的临时表可以通过sparksession上的catalog对象的dropTempView(name)方法来删除

~~~python
spark.catalog.dropTempView('A')
~~~

45. replace(to_replace, value=<no value>, subset=None)这个方法通过第一个参数指定要被替换掉的老的值，第二个参数指定新的值，subset关键字参数指定子集，默认是在整个DataFrame上进行替换。把数据集中的99替换成100

~~~python
df = spark.read.csv('./datas/customers.csv',header=True)
df.columns
df2 = df.replace('99','100')
df2.show()
>>> 
+----------+------+---+------------------+----------------------+
|CustomerID| Genre|Age|Annual Income (k$)|Spending Score (1-100)|
+----------+------+---+------------------+----------------------+
|      0001|  Male| 19|                15|                    39|
|      0002|  Male| 21|                15|                    81|
|      0003|Female| 20|                16|                     6|
|      0004|Female| 23|                16|                    77|
|      0005|Female| 31|                17|                    40|
|      0006|Female| 22|                17|                    76|
|      0007|Female| 35|                18|                     6|
|      0008|Female| 23|                18|                    94|
|      0009|  Male| 64|                19|                     3|
|      0010|Female| 30|                19|                    72|
|      0011|  Male| 67|                19|                    14|
|      0012|Female| 35|                19|                   100|
|      0013|Female| 58|                20|                    15|
|      0014|Female| 24|                20|                    77|
|      0015|  Male| 37|                20|                    13|
|      0016|  Male| 22|                20|                    79|
|      0017|Female| 35|                21|                    35|
|      0018|  Male| 20|                21|                    66|
|      0019|  Male| 52|                23|                    29|
|      0020|Female| 35|                23|                    98|
+----------+------+---+------------------+----------------------+
only showing top 20 rows
~~~

注意上面再替换的过程to_replace和value的类型必须要相同，而且to_replace数据类型只能是：bool、int、long、float、string、list or dict。value数据类型只能是：bool、int、long、float、string、list or None

~~~python
df.columns
>>> ['CustomerID', 'Genre', 'Age', 'Annual Income (k$)', 'Spending Score (1-100)']
df.replace(['Female','Male'],['F','M'],'Genre').show()
>>> 
+----------+-----+---+------------------+----------------------+
|CustomerID|Genre|Age|Annual Income (k$)|Spending Score (1-100)|
+----------+-----+---+------------------+----------------------+
|      0001|    M| 19|                15|                    39|
|      0002|    M| 21|                15|                    81|
|      0003|    F| 20|                16|                     6|
|      0004|    F| 23|                16|                    77|
|      0005|    F| 31|                17|                    40|
|      0006|    F| 22|                17|                    76|
|      0007|    F| 35|                18|                     6|
|      0008|    F| 23|                18|                    94|
|      0009|    M| 64|                19|                     3|
|      0010|    F| 30|                19|                    72|
|      0011|    M| 67|                19|                    14|
|      0012|    F| 35|                19|                    99|
|      0013|    F| 58|                20|                    15|
|      0014|    F| 24|                20|                    77|
|      0015|    M| 37|                20|                    13|
|      0016|    M| 22|                20|                    79|
|      0017|    F| 35|                21|                    35|
|      0018|    M| 20|                21|                    66|
|      0019|    M| 52|                23|                    29|
|      0020|    F| 35|                23|                    98|
+----------+-----+---+------------------+----------------------+
only showing top 20 rows
~~~

46. rollup(*cols) 按照指定的列名进行汇总，这样就可以再汇总的数据集上运用聚合函数

~~~python
df = spark.read.csv('./datas/customers.csv', header=True)
df.rollup('Genre', 'Age').count().orderBy(desc('count'), 'Genre').show()
>>>
+------+----+-----+
| Genre| Age|count|
+------+----+-----+
|  null|null|  200|
|Female|null|  112|
|  Male|null|   88|
|Female|  31|    7|
|Female|  23|    6|
|Female|  49|    6|
|Female|  32|    6|
|Female|  35|    6|
|  Male|  19|    6|
|Female|  30|    5|
|  Male|  32|    5|
|  Male|  48|    5|
|Female|  21|    4|
|Female|  47|    4|
|Female|  50|    4|
|Female|  36|    4|
|Female|  29|    4|
|Female|  27|    4|
|Female|  38|    4|
|  Male|  59|    4|
+------+----+-----+
only showing top 20 rows
~~~

47. sample(withReplacement=None, fraction=None, seed=None), 用于从DataFrame中进行采用的方法，withReplacement关键字参数用于指定是否采用有放回的采样，true为有放回采用，false为无采样放回，fraction指定采样的比例，seed为采样种子，相同的种子为对应的采样总是相同的，用于场景的复现

~~~python
df.count()
>>> 200
df2 = df.sample(withReplacement=True, fraction=0.2, seed=1)
df2.count()
>>> 35
df2.show()
>>>
+----------+------+---+------------------+----------------------+
|CustomerID| Genre|Age|Annual Income (k$)|Spending Score (1-100)|
+----------+------+---+------------------+----------------------+
|      0001|  Male| 19|                15|                    39|
|      0005|Female| 31|                17|                    40|
|      0006|Female| 22|                17|                    76|
|      0009|  Male| 64|                19|                     3|
|      0025|Female| 54|                28|                    14|
|      0027|Female| 45|                28|                    32|
|      0042|  Male| 24|                38|                    92|
|      0060|  Male| 53|                46|                    46|
|      0061|  Male| 70|                46|                    56|
|      0061|  Male| 70|                46|                    56|
|      0065|  Male| 63|                48|                    51|
|      0071|  Male| 70|                49|                    55|
|      0095|Female| 32|                60|                    42|
|      0098|Female| 27|                60|                    50|
|      0107|Female| 66|                63|                    50|
|      0107|Female| 66|                63|                    50|
|      0110|  Male| 66|                63|                    48|
|      0115|Female| 18|                65|                    48|
|      0117|Female| 63|                65|                    43|
|      0122|Female| 38|                67|                    40|
+----------+------+---+------------------+----------------------+
only showing top 20 rows
~~~

48. sampleBy(col, fractions, seed=None) 按照指定的col列根据fractions指定的比例进行分层抽样，seed是随机种子，用于场景的复现

~~~python
df.sampleBy('Genre', {
    'Male': 0.1,
    "Famle": 0.15
}).groupBy('Genre').count().show()
>>> 
+-----+-----+
|Genre|count|
+-----+-----+
| Male|   16|
+-----+-----+
~~~

49. schema 返回DataFrame的schema信息。它是pyspark.sql.types.StructType类型

~~~python
df.schema
>>> StructType(List(StructField(CustomerID,StringType,true),StructField(Genre,StringType,true),StructField(Age,StringType,true),StructField(Annual Income (k$),StringType,true),StructField(Spending Score (1-100),StringType,true)))

df.printSchema()
>>> 
root
 |-- CustomerID: string (nullable = true)
 |-- Genre: string (nullable = true)
 |-- Age: string (nullable = true)
 |-- Annual Income (k$): string (nullable = true)
 |-- Spending Score (1-100): string (nullable = true)
~~~

50. select(*cols) 通过表达式选取DataFrame中符合条件的数据，返回新的DataFrame

~~~python
df.select("*").count()
>>> 200

df.select('Genre', 'Age').show(5)
>>> 
+------+---+
| Genre|Age|
+------+---+
|  Male| 19|
|  Male| 21|
|Female| 20|
|Female| 23|
|Female| 31|
+------+---+
only showing top 5 rows

df.select(df.Age.alias('age')).show(5)
>>> 
+---+
|age|
+---+
| 19|
| 21|
| 20|
| 23|
| 31|
+---+
only showing top 5 rows
~~~

51. selectExpr(*expr) 这个方法是select方法的一个变体，他可以接收一个SQL表达式，返回新的DataFrame

~~~python
df.selectExpr('Age * 2','sqrt(Age)').show(5)
>>> 
+---------+-------------------------+
|(Age * 2)|SQRT(CAST(Age AS DOUBLE))|
+---------+-------------------------+
|     38.0|        4.358898943540674|
|     42.0|         4.58257569495584|
|     40.0|         4.47213595499958|
|     46.0|        4.795831523312719|
|     62.0|       5.5677643628300215|
+---------+-------------------------+
only showing top 5 rows
~~~

52. show(n=20, truncate=True, vertical=False) 这个方法默认返回DataFrame的前20行记录，可以通过truncate指定超过20个字符的记录将会被截断，vertical指定是否垂直显示。

~~~python
df.selectExpr('Age * 2', 'sqrt(Age)').show(5, truncate=False, vertical=True)
>>> 
-RECORD 0---------------------------------------
 (Age * 2)                 | 38.0               
 SQRT(CAST(Age AS DOUBLE)) | 4.358898943540674  
-RECORD 1---------------------------------------
 (Age * 2)                 | 42.0               
 SQRT(CAST(Age AS DOUBLE)) | 4.58257569495584   
-RECORD 2---------------------------------------
 (Age * 2)                 | 40.0               
 SQRT(CAST(Age AS DOUBLE)) | 4.47213595499958   
-RECORD 3---------------------------------------
 (Age * 2)                 | 46.0               
 SQRT(CAST(Age AS DOUBLE)) | 4.795831523312719  
-RECORD 4---------------------------------------
 (Age * 2)                 | 62.0               
 SQRT(CAST(Age AS DOUBLE)) | 5.5677643628300215 
only showing top 5 rows
~~~

53. sortWithinPartitions(* cols, \*** kwargs)和sort(* cols, \*** kwargs) 这两个方法都是用指定的cols列进行排序，通过kwargs参数指定升序降序

~~~python
df.sort(['Age','Genre'],ascending=True).show(5)
>>> 
+----------+------+---+------------------+----------------------+
|CustomerID| Genre|Age|Annual Income (k$)|Spending Score (1-100)|
+----------+------+---+------------------+----------------------+
|      0115|Female| 18|                65|                    48|
|      0066|  Male| 18|                48|                    59|
|      0092|  Male| 18|                59|                    41|
|      0034|  Male| 18|                33|                    92|
|      0116|Female| 19|                65|                    50|
+----------+------+---+------------------+----------------------+
only showing top 5 rows

df.sort(df.Age.desc()).show(5)
>>> 
+----------+------+---+------------------+----------------------+
|CustomerID| Genre|Age|Annual Income (k$)|Spending Score (1-100)|
+----------+------+---+------------------+----------------------+
|      0071|  Male| 70|                49|                    55|
|      0061|  Male| 70|                46|                    56|
|      0058|  Male| 69|                44|                    46|
|      0068|Female| 68|                48|                    48|
|      0091|Female| 68|                59|                    55|
+----------+------+---+------------------+----------------------+
only showing top 5 rows

df.sortWithinPartitions(['Age', 'Genre'], ascending=False).show(5)
>>> 
+----------+------+---+------------------+----------------------+
|CustomerID| Genre|Age|Annual Income (k$)|Spending Score (1-100)|
+----------+------+---+------------------+----------------------+
|      0061|  Male| 70|                46|                    56|
|      0071|  Male| 70|                49|                    55|
|      0058|  Male| 69|                44|                    46|
|      0109|  Male| 68|                63|                    43|
|      0068|Female| 68|                48|                    48|
+----------+------+---+------------------+----------------------+
only showing top 5 rows

df.sortWithinPartitions(desc('Age')).show(2)
>>> 
+----------+-----+---+------------------+----------------------+
|CustomerID|Genre|Age|Annual Income (k$)|Spending Score (1-100)|
+----------+-----+---+------------------+----------------------+
|      0061| Male| 70|                46|                    56|
|      0071| Male| 70|                49|                    55|
+----------+-----+---+------------------+----------------------+
only showing top 2 rows
~~~

54. stat 返回DataFrameStatFunctions对象，这个对象中提供了一些统计的方法

~~~python
df.stat
>>> <pyspark.sql.dataframe.DataFrameStatFunctions at 0x21534ffb940>
dir(df.stat)
>>>
['__class__','__delattr__','__dict__','__dir__','__doc__','__eq__','__format__','__ge__','__getattribute__','__gt__','__hash__','__init__','__init_subclass__','__le__','__lt__','__module__','__ne__','__new__','__reduce__','__reduce_ex__','__repr__','__setattr__','__sizeof__','__str__','__subclasshook__','__weakref__','approxQuantile','corr','cov','crosstab','df','freqItems','sampleBy']
~~~

55. storageLevel 返回DataFrame的缓存级别

~~~python
df.storageLevel
>>> StorageLevel(False, False, False, False, 1)
df.cache()
>>> DataFrame[CustomerID: string, Genre: string, Age: string, Annual Income (k$): string, Spending Score (1-100): string]
df.storageLevel
>>> StorageLevel(True, True, False, True, 1)
~~~

56. subtract(other) 这个方法用来获取再A集合里而不在B集合里的数据，返回新的DataFrame

~~~python
df1 = spark.createDataFrame([('tom', ), ('ming', ), ('yun', )],
                            schema=['name'])
df2 = spark.createDataFrame([('tom', ), ('ming', ), ('feng', )],
                            schema=['name'])
df3 = df1.subtract(df2)
df3.show()
>>> 
+----+
|name|
+----+
| yun|
+----+
~~~

57. summary(*statistics)用传入的统计方法返回概要信息。不传参数回默认计算count，mean，stddev，min，approximate quartiles(percentiles at 25%. 50% and 75%) and max, *statistics参数可以是： count - mean - stddev - min - max - arbitrary approximate percentiles

~~~python
df.summary().show(2)
>>>
+-------+----------+-----+-----+------------------+----------------------+
|summary|CustomerID|Genre|  Age|Annual Income (k$)|Spending Score (1-100)|
+-------+----------+-----+-----+------------------+----------------------+
|  count|       200|  200|  200|               200|                   200|
|   mean|     100.5| null|38.85|             60.56|                  50.2|
+-------+----------+-----+-----+------------------+----------------------+
only showing top 2 rows

df.summary("min",'count','75%').show()
>>> 
+-------+----------+------+----+------------------+----------------------+
|summary|CustomerID| Genre| Age|Annual Income (k$)|Spending Score (1-100)|
+-------+----------+------+----+------------------+----------------------+
|    min|      0001|Female|  18|               101|                     1|
|  count|       200|   200| 200|               200|                   200|
|    75%|     150.0|  null|49.0|              78.0|                  73.0|
+-------+----------+------+----+------------------+----------------------+
~~~

58. take(num) 返回DataFrame的前num个Row数据组成的列表，注意num不要太大，容易造成driver节点的OOM

~~~python
df.take(2)
>>> [Row(CustomerID='0001', Genre='Male', Age='19', Annual Income (k$)='15', Spending Score (1-100)='39'),
 Row(CustomerID='0002', Genre='Male', Age='21', Annual Income (k$)='15', Spending Score (1-100)='81')]
~~~

59. toDF(*cols) 返回新的带有指定cols名字的DataFrame对象

~~~python
df.columns
>>> ['CustomerID', 'Genre', 'Age', 'Annual Income (k$)', 'Spending Score (1-100)']
df1 = df.toDF('id', 'sex', 'age', 'income', 'score')
df1.columns
>>> ['id', 'sex', 'age', 'income', 'score']
df1.show(5)
>>> 
+----+------+---+------+-----+
|  id|   sex|age|income|score|
+----+------+---+------+-----+
|0001|  Male| 19|    15|   39|
|0002|  Male| 21|    15|   81|
|0003|Female| 20|    16|    6|
|0004|Female| 23|    16|   77|
|0005|Female| 31|    17|   40|
+----+------+---+------+-----+
only showing top 5 rows
~~~

60. toJSON(use_unicode=True) 将DataFrame中的Row对象转换为json字符串，默认使用unicode编码。toJSON方法返回的是RDD对象，而不是DataFrame对象

~~~python
df1 = df.toJSON()
df1.collect()
>>>
['{"CustomerID":"0001","Genre":"Male","Age":"19","Annual Income (k$)":"15","Spending Score (1-100)":"39"}',
 '{"CustomerID":"0002","Genre":"Male","Age":"21","Annual Income (k$)":"15","Spending Score (1-100)":"81"}',
 '{"CustomerID":"0003","Genre":"Female","Age":"20","Annual Income (k$)":"16","Spending Score (1-100)":"6"}',
 '{"CustomerID":"0004","Genre":"Female","Age":"23","Annual Income (k$)":"16","Spending Score (1-100)":"77"}',
 '{"CustomerID":"0005","Genre":"Female","Age":"31","Annual Income (k$)":"17","Spending Score (1-100)":"40"}']
~~~

61. toLocalIterator() 将DataFrame中所有数据返回为本地的可迭代的数据，数据量大了容易OOM

~~~python
result = df.toLocalIterator()
for data in result:
    print(data)
>>> Row(CustomerID='0001', Genre='Male', Age='19', Annual Income (k$)='15', Spending Score (1-100)='39')
Row(CustomerID='0002', Genre='Male', Age='21', Annual Income (k$)='15', Spending Score (1-100)='81')
Row(CustomerID='0003', Genre='Female', Age='20', Annual Income (k$)='16', Spending Score (1-100)='6')
~~~

62. toPandas() 将Spark中的DataFrame对象转换为pandas中的DataFrame对象

~~~python
pan_df = df.toPandas()
pan_df.head(5)
~~~
![image.png](attachment:image.png)

63. union(other) 返回两个DataFrame的合集

~~~python
df1 = spark.createDataFrame([('tom', ), ('ming', ), ('yun', )],
                            schema=['name'])
df2 = spark.createDataFrame([('tom', ), ('ming', ), ('feng', )],
                            schema=['name'])
df3 = df1.union(df2)
df3.show()
>>> 
+----+
|name|
+----+
| tom|
|ming|
| yun|
| tom|
|ming|
|feng|
+----+
~~~

64. unionByName(other) 根据名字来找出两个DataFrame的合集，与字段的顺序没有关系，只要字段名称对上即可

~~~python
df1 = spark.createDataFrame([('tom', 11), ('ming', 1), ('yun', 2)],
                            schema=['name', 'score'])
df2 = spark.createDataFrame([('tom', 1), ('ming', 2), ('feng', 3)],
                            schema=['name', 'score'])
df3 = df1.unionByName(df2)
df3.show()
>>> 
+----+-----+
|name|score|
+----+-----+
| tom|   11|
|ming|    1|
| yun|    2|
| tom|    1|
|ming|    2|
|feng|    3|
+----+-----+
~~~

65. unpersist(blocking=False) 这个方法用于将DataFrame上持久化的数据全部清除掉

~~~python
df.storageLevel
>>> StorageLevel(False, False, False, False, 1)
~~~

66. where(condition) 这个方法和filter类似，根据传入的条件做出选择

~~~python
df = spark.read.csv('./datas/customers.csv', header=True)
df.where('Age <= 20').show()
>>> 
+----------+------+---+------------------+----------------------+
|CustomerID| Genre|Age|Annual Income (k$)|Spending Score (1-100)|
+----------+------+---+------------------+----------------------+
|      0001|  Male| 19|                15|                    39|
|      0003|Female| 20|                16|                     6|
|      0018|  Male| 20|                21|                    66|
|      0034|  Male| 18|                33|                    92|
|      0040|Female| 20|                37|                    75|
|      0062|  Male| 19|                46|                    55|
|      0066|  Male| 18|                48|                    59|
|      0069|  Male| 19|                48|                    59|
|      0092|  Male| 18|                59|                    41|
|      0100|  Male| 20|                61|                    49|
|      0112|Female| 19|                63|                    54|
|      0114|  Male| 19|                64|                    46|
|      0115|Female| 18|                65|                    48|
|      0116|Female| 19|                65|                    50|
|      0135|  Male| 20|                73|                     5|
|      0139|  Male| 19|                74|                    10|
|      0163|  Male| 19|                81|                     5|
+----------+------+---+------------------+----------------------+
~~~

67. withColumn(colName, col)返回一个新的DataFrame，这个DataFrame中新增加colName的列，或者原来本神就有colName的列，则替换掉

~~~python
df.withColumn('Age',df.Age**2).show(5)
>>> 
+----------+------+-----+------------------+----------------------+
|CustomerID| Genre|  Age|Annual Income (k$)|Spending Score (1-100)|
+----------+------+-----+------------------+----------------------+
|      0001|  Male|361.0|                15|                    39|
|      0002|  Male|441.0|                15|                    81|
|      0003|Female|400.0|                16|                     6|
|      0004|Female|529.0|                16|                    77|
|      0005|Female|961.0|                17|                    40|
+----------+------+-----+------------------+----------------------+
only showing top 5 rows

df.withColumn('Age2',df.Age**3).show(5)
>>> 
+----------+------+---+------------------+----------------------+-------+
|CustomerID| Genre|Age|Annual Income (k$)|Spending Score (1-100)|   Age2|
+----------+------+---+------------------+----------------------+-------+
|      0001|  Male| 19|                15|                    39| 6859.0|
|      0002|  Male| 21|                15|                    81| 9261.0|
|      0003|Female| 20|                16|                     6| 8000.0|
|      0004|Female| 23|                16|                    77|12167.0|
|      0005|Female| 31|                17|                    40|29791.0|
+----------+------+---+------------------+----------------------+-------+
only showing top 5 rows
~~~

68. withColumnRenamed(existing, new) 对已经存在的列名重命名为new，若名称不存在着这个操作不做任何事情

~~~python
df.withColumnRenamed('Age', 'age').show(5)
>>> 
+----------+------+---+------------------+----------------------+
|CustomerID| Genre|age|Annual Income (k$)|Spending Score (1-100)|
+----------+------+---+------------------+----------------------+
|      0001|  Male| 19|                15|                    39|
|      0002|  Male| 21|                15|                    81|
|      0003|Female| 20|                16|                     6|
|      0004|Female| 23|                16|                    77|
|      0005|Female| 31|                17|                    40|
+----------+------+---+------------------+----------------------+
only showing top 5 rows

df.withColumnRenamed('Age2', 'age').show(5)
>>> 
+----------+------+---+------------------+----------------------+
|CustomerID| Genre|Age|Annual Income (k$)|Spending Score (1-100)|
+----------+------+---+------------------+----------------------+
|      0001|  Male| 19|                15|                    39|
|      0002|  Male| 21|                15|                    81|
|      0003|Female| 20|                16|                     6|
|      0004|Female| 23|                16|                    77|
|      0005|Female| 31|                17|                    40|
+----------+------+---+------------------+----------------------+
only showing top 5 rows
~~~

69. write 借助这个接口将DataFrame的内容保存到外部的系统

~~~python
df.write
>>> <pyspark.sql.readwriter.DataFrameWriter at 0x215364e9c18>
dir(df.write)
>>> 
['__class__','__delattr__','__dict__','__dir__','__doc__','__eq__','__format__','__ge__','__getattribute__','__gt__','__hash__','__init__','__init_subclass__','__le__','__lt__','__module__','__ne__','__new__','__reduce__','__reduce_ex__','__repr__','__setattr__','__sizeof__','__str__','__subclasshook__','__weakref__','_df','_jwrite','_set_opts','_spark','_sq','bucketBy','csv','format','insertInto','jdbc','json','mode','option','options','orc','parquet','partitionBy','save','saveAsTable','sortBy','text']
~~~
