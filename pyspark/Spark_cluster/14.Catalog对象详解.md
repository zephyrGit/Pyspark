
1. Catalog对象里包含了很多判断或查看SparkSQL中表和数据仓库状态的方法，可以通过sparksession上的catalog来获取，它是对Scala实现的org.apache.spark.sql.catalog.Catalog这个类的一个轻量级的封装

~~~python
spark.catalog
>>> 
<pyspark.sql.catalog.Catalog at 0x2956246de80>
dir(spark.catalog)
>>> 
['__class__','__delattr__','__dict__','__dir__','__doc__','__eq__','__format__','__ge__','__getattribute__','__gt__','__hash__','__init__','__init_subclass__','__le__','__lt__','__module__','__ne__','__new__','__reduce__','__reduce_ex__','__repr__','__setattr__','__sizeof__','__str__','__subclasshook__','__weakref__','_jcatalog','_jsparkSession','_reset','_sparkSession','cacheTable','clearCache','createExternalTable','createTable','currentDatabase','dropGlobalTempView','dropTempView','isCached','listColumns','listDatabases','listFunctions','listTables','recoverPartitions','refreshByPath','refreshTable','registerFunction','setCurrentDatabase','uncacheTable']
~~~

2. cacheTable(tableName) 可以通过这个方法，把指定的表缓存再内存中，这样在以后要用到这个表的时候，就可以直接在内存中读取

~~~shell
root@hadoop-maste:~# hdfs dfs -ls /home/hive/workhouse/worker.db
Found 2 items
drwxr-xr-x    - root  supergroup        0 2019-07-06 14:23 /home/hive/warehouse/worker.db/test
drwxr-xr-x    - root  supergroup        0 2019-07-06 14:26 home/hive/warehouse/worker.db/test1
~~~
指定缓存worker库中的test1表
~~~python
spark.catalog.cacheTable("worker.test1")
~~~

3. 缓存之后，可以通过isCached(tableName) 查看指定的表名是否已经缓存在内存中

~~~python
spark.isCached("worker.test1")
>>> True
~~~
**注意：适用isCached方法判断一张表是否缓存，当表名不存在会报错！**

4. 表缓存在内存中是要占内存空间的，在表适用完成后，我们应该及时的释放掉缓存在内存中的表格数据，这个时候就需要调用catalog上的clearCacahe()方法了。这个方法会释放缓存在内存中的所有的表

~~~python
spark.catalog.clearCache()
~~~

5. createExternalTable(tableName, path=None, source=None, schema=None, \***options),可以通过这个方法创建外部表格，tableName指定表格的名字，path指定数据的存储路径，source知道那个数据的来源，默认为spark.sql.sources.default这个配置项指定的类型。下面指定test1表格的路径，通过读取这个路径下的文件来创建外部表格justatest4，当然这个路径可以是dfs文件系统中的任何路径  
首先查看spark.sql.sources.default这个配置项

~~~python
spark.sparkContext.getConf().get('spark.sql.sources.default')
>>> 
~~~
没有任何的打印输出，说明这个配置是没有配置的。因此source这个源不指定【source的取值默认是parquet，当然还可以指定为csv，text等】，这个指定path为句对路径即可加载路径下的文件

~~~python
df = spark.catalog.createExternalTable('justatest4', path='/home/hive/warehouse/worker.db/test')
df.show()
>>> 
~~~
下面尝试读取一下csv文件，通过指定header=True，source=csv来正确的读取csv的元数据信息作为DataFrame的Schema信息

~~~python
df = spark.catalog.createExternalTable('justatest12', path='/home/hive/warehouse/worker.db/test12/', souce='parquet')
df.show()
~~~

6. createTable(tableName, path=None, source=None, schema=None, \***options),这个方法是和createExternalTble这个方法类似的方法，适用path读取指定位置的文件，source指定数据的类型，schema用于指定数据的schema信息。这个方法的返回值仍然是DataFrame对象

~~~python
df = spark.catalog.createTable('table1', path='./data/customer.csv', source='csv', header='True')
df.show()
>>> 
~~~

7. currentDatabase()返回当前适用的默认的数据库，默认的数据库是default数据库

~~~python
spark.catalog.currentDatabase()
>>> 'default'
~~~

8. dropGlobalTempView(viewName) 前面DataFrame适用creatGlobalTempView方法来创建全局的临时表格，那创建之后怎么删除这些表格呢？其实可以借助这个dropGlobalTempView方法来执行全局临时表的删除

~~~python
spark.range(1, 10, 2, 2).createGlobalTempView('aaaa')
spark.sql('select * from global_temp.aaaa').show()
>>> 
spark.catalog.dropGlobalTempView('aaaa')
spark.sql('select * from global_temp.aaaa').show()
>>> 
~~~

9. dropTempView(viewName) 这个方法用于删除注册的临时的表格

~~~python
spark.range(1,10, 2, 2).createTempView('aaa')
spark.sql('select * from aaa').show()
spark.catalog.dropTempView('aaa')
~~~

10. listColumns(tableName, dbName=None) 返回指定库下指定的表明对应的列细信息。如果不指定dbName，这个方法默认会从default库中查看相应的表格

~~~python
spark.catalog.listColumns('test1'.'worker')
~~~

11. 要查看这个spark能够访问到哪些数据仓库呢？可以适用listDatabases()方法返回所有的Database的信息

~~~python
spark.catalog.listDatabases()
[Database(name='default', description='Default Hive database', locationUri='hdfs://hadoop-maste:9000/home/hive/warehouse'), Database(name='worker', description='', locationUri='hdfs://hadoop-maste:9000/home/hive/warehouse/worker.db')]
~~~

12. listFunctions(dbName=None) 这个方法返回在指定的数据库上注册的方法，若不指定dbName则从当前所在的数据库上进行查找。

~~~python
>>> spark.catalog.listFunctions(dbName='worker')
~~~

13. listTable(dbName=None) 返回指定数据库下所有的表格

~~~python
spark.catalog.listTables('worker')
>>> [Table(name='test', database='worker', description=None, tableType='MANAGED', isTemporary=False), Table(name='test2', database='worker', description=None, tableType='MANAGED', isTemporary=False)]
~~~

14. recoverPartitions(tableName) 恢复给定表的所有分区并更新目录。仅对已经分区的表格生效，对视图view不起作用

~~~python
spark.catalog.recoverPartitions('worker.test1')
~~~

15. registerFunction(name, f, returnType=None) 注册用户自定义的函数，和spark.udf.register()这个方法的功能一样

~~~python
from pyspark.sql import SparkSession
from pyspark.sql.types import *


spark = SparkSession.builder.master('local').appName('registerFunction').getOrCreate()

def pow1(m, n):
    return float(m) ** float(n)


spark.catalog.registerFunction('pow1',pow1, returnType=DoubleType())

df = spark.range(1,10,2,2)

df.createOrReplaceTempView('A')

spark.sql('select pow1(id,2) from A').show()                                          
>>> 
+-----------+
|pow1(id, 2)|
+-----------+
|        1.0|
|        9.0|
|       25.0|
|       49.0|
|       81.0|
+-----------+
~~~

16. setCurrentDatabase(dbName) 在当前的session中设置默认的db数据库的名字，其生命周期同当前的session

~~~python
spark.catalog.setCurrentDatabase('worker')
spark.sql('select * from test1').show()
>>> 
~~~

17. uncacheTable(tableName) 从内存中移除指定的表名的数据

~~~python
spark.catalog.setCurrentDatabase('worker')
spark.catalog.cacheTable('test1')
spark.catalog.uncacheTable('test1')
~~~
