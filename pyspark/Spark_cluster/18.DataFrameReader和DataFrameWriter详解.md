
1. DataFrameReader 这个类是用于读取外部存储来生成DataFrame的接口，它位于pyaprk.sql模块下，可以通过sparksession的read属性来获取引用

~~~python
from pyspark.sql import SparkSession

spark = SparkSession.builder.master('local').appName('DFR').getOrCreate()
spark.read
>>> <pyspark.sql.readwriter.DataFrameReader at 0x19bd7ecdb70>
dir(spark.read)
>>> 
['__class__','__delattr__','__dict__','__dir__','__doc__','__eq__','__format__','__ge__','__getattribute__','__gt__','__hash__','__init__','__init_subclass__','__le__','__lt__','__module__','__ne__','__new__','__reduce__','__reduce_ex__','__repr__','__setattr__','__sizeof__','__str__','__subclasshook__','__weakref__','_df','_jreader','_set_opts','_spark','csv','format','jdbc','json','load','option','options','orc','parquet','schema','table','text']
~~~

2. csv 这个方法已经适用过了， csv方法的完整定义如下：  
csv(path, schma=None, sep=None, encoding=None, quote=None, escape=None,comment=None, header=None, inferSchem 有很多关键字参数，例如可以指定编码，指定header，指定timestamp的格式等

~~~python
df = spark.read.csv('./datas/titanic_train.csv', header=True, encoding='utf-8')
df.show()
>>> 
+-----------+--------+------+--------------------+------+----+-----+-----+----------------+-------+-----+--------+
|PassengerId|Survived|Pclass|                Name|   Sex| Age|SibSp|Parch|          Ticket|   Fare|Cabin|Embarked|
+-----------+--------+------+--------------------+------+----+-----+-----+----------------+-------+-----+--------+
|          1|       0|     3|Braund, Mr. Owen ...|  male|  22|    1|    0|       A/5 21171|   7.25| null|       S|
|          2|       1|     1|Cumings, Mrs. Joh...|female|  38|    1|    0|        PC 17599|71.2833|  C85|       C|
|          3|       1|     3|Heikkinen, Miss. ...|female|  26|    0|    0|STON/O2. 3101282|  7.925| null|       S|
|          4|       1|     1|Futrelle, Mrs. Ja...|female|  35|    1|    0|          113803|   53.1| C123|       S|
|          5|       0|     3|Allen, Mr. Willia...|  male|  35|    0|    0|          373450|   8.05| null|       S|
|          6|       0|     3|    Moran, Mr. James|  male|null|    0|    0|          330877| 8.4583| null|       Q|
|          7|       0|     1|McCarthy, Mr. Tim...|  male|  54|    0|    0|           17463|51.8625|  E46|       S|
|          8|       0|     3|Palsson, Master. ...|  male|   2|    3|    1|          349909| 21.075| null|       S|
|          9|       1|     3|Johnson, Mrs. Osc...|female|  27|    0|    2|          347742|11.1333| null|       S|
|         10|       1|     2|Nasser, Mrs. Nich...|female|  14|    1|    0|          237736|30.0708| null|       C|
|         11|       1|     3|Sandstrom, Miss. ...|female|   4|    1|    1|         PP 9549|   16.7|   G6|       S|
|         12|       1|     1|Bonnell, Miss. El...|female|  58|    0|    0|          113783|  26.55| C103|       S|
|         13|       0|     3|Saundercock, Mr. ...|  male|  20|    0|    0|       A/5. 2151|   8.05| null|       S|
|         14|       0|     3|Andersson, Mr. An...|  male|  39|    1|    5|          347082| 31.275| null|       S|
|         15|       0|     3|Vestrom, Miss. Hu...|female|  14|    0|    0|          350406| 7.8542| null|       S|
|         16|       1|     2|Hewlett, Mrs. (Ma...|female|  55|    0|    0|          248706|     16| null|       S|
|         17|       0|     3|Rice, Master. Eugene|  male|   2|    4|    1|          382652| 29.125| null|       Q|
|         18|       1|     2|Williams, Mr. Cha...|  male|null|    0|    0|          244373|     13| null|       S|
|         19|       0|     3|Vander Planke, Mr...|female|  31|    1|    0|          345763|     18| null|       S|
|         20|       1|     3|Masselmani, Mrs. ...|female|null|    0|    0|            2649|  7.225| null|       C|
+-----------+--------+------+--------------------+------+----+-----+-----+----------------+-------+-----+--------+
only showing top 20 rows
~~~

2. format(source) 用于指定从某种类型的数据源读取数据，例如可以指定json，csv，parquet

~~~python
dir(spark.read.format)
>>> 
['__call__','__class__','__delattr__','__dir__','__doc__','__eq__','__format__','__func__','__ge__','__get__','__getattribute__','__gt__','__hash__','__init__','__init_subclass__','__le__','__lt__','__ne__','__new__','__reduce__','__reduce_ex__','__repr__','__self__','__setattr__','__sizeof__','__str__','__subclasshook__']
~~~
这里使用format来读取json文件，这里有一个fengji.json的风机随机数据文件

~~~python
df = spark.read.format('json').load('./datas/fengji.json')
df.show()
>>> 
+--------------------+--------------------+--------------------+--------------------+--------------------+--------------------+--------------------+--------------------+--------------------+--------------------+--------------------+--------------------+--------------------+--------------------+--------------------+--------------------+--------------------+--------------------+--------------------+--------------------+--------------------+--------------------+--------------------+--------------------+--------------------+--------------------+--------------------+--------------------+--------------------+
|               acc_x|               acc_y|     environment_tmp|     generator_speed|               group|             int_tmp|               label|        pitch1_angle|     pitch1_moto_tmp|       pitch1_ng5_DC|      pitch1_ng5_tmp|        pitch1_speed|        pitch2_angle|     pitch2_moto_tmp|       pitch2_ng5_DC|      pitch2_ng5_tmp|        pitch2_speed|        pitch3_angle|     pitch3_moto_tmp|       pitch3_ng5_DC|      pitch3_ng5_tmp|        pitch3_speed|               power|           timestamp|      wind_direction| wind_direction_mean|          wind_speed|        yaw_position|           yaw_speed|
+--------------------+--------------------+--------------------+--------------------+--------------------+--------------------+--------------------+--------------------+--------------------+--------------------+--------------------+--------------------+--------------------+--------------------+--------------------+--------------------+--------------------+--------------------+--------------------+--------------------+--------------------+--------------------+--------------------+--------------------+--------------------+--------------------+--------------------+--------------------+--------------------+
|[-1.023986385, 1....|[0.061109469, -1....|[-0.40391874, -0....|[1.223594525, 1.2...|[1.0, 1.0, 1.0, 1...|[0.014918413, -0....|[1, 1, 1, 1, 1, 1...|[0.555555556, 0.1...|[0.759, 0.769, 0....|[1.36, 0.44, 2.8,...|[1.307692308, 1.3...|[-1.68, 0.0, -1.4...|[0.506666667, 0.1...|[0.6, 0.609, 0.6,...|[0.0, 2.88, 0.32,...|[1.123076923, 1.1...|[-1.72, 0.0, -1.3...|[0.551111111, 0.1...|[0.59, 0.6, 0.569...|[1.56, -2.6, -0.0...|[0.783076923, 0.7...|[-1.68, 0.0, -1.4...|[2.515789697, 2.3...|[1.4463804E9, 1.4...|[-2.072739488, -2...|[-2.073626589, -1...|[1.859993305, 1.9...|[-0.655342752, -0...|[0.030803582, 0.0...|
+--------------------+--------------------+--------------------+--------------------+--------------------+--------------------+--------------------+--------------------+--------------------+--------------------+--------------------+--------------------+--------------------+--------------------+--------------------+--------------------+--------------------+--------------------+--------------------+--------------------+--------------------+--------------------+--------------------+--------------------+--------------------+--------------------+--------------------+--------------------+--------------------+
~~~

3. json方法本身是有定义的，其定义如下：和csv方法一样有很多的可选参数。  
json(path, schema=None, primitivesAsString=None, prefersDecimal=None, allowComments=None, allowUnquotedFieldNames=None, allowSingleQuotes=None, allowNumericLeadingZero=None, allowBackslashEscapingAnyCharacter=None, mode=None, columnNameOfCorruptRecord=None, dateFormat=None, timestampFormat=None, multiLine=None, allowUnquotedControlChars=None)

4. jdbc(url, table, column=None, lowerBound=None, upperBound=None, numPartitions=None, predicates=None, properties=None) 这个方法用来读取关系型数据，其中url是链接数据库的连接地址，table是读取的关系型数据库中的表名，column关键字参数指定用于分区的整数类型的列的名称，lowerBound指定划分分区的下界，upperBound指定划分分区上界，numPartitions指定分区数即并行度，predicates指定where语句中的条件，properties指定连接数据的配置信息，如user和password

- 首先在hadoop-maste容器上使用 mysql -uroot -proot -hhadoop-mysql 连接到msyql数据中，创建test库

~~~sql
mysql> create database test;
mysql> use test;
mysql> create table student(name char(20), age int);
# 向表中插入如下数据：
mysql> insert into student(name, age) value('xiaohei',22);
Query OK, 1 row affected (0.00 sec)

mysql> insert into student(name, age) value('xiaobai',23);
Query OK, 1 row affected (0.00 sec)

mysql> insert into student(name, age) value('xiaohong',23);
Query OK, 1 row affected (0.00 sec)

mysql> insert into student(name, age) value('xiaolan',23);
Query OK, 1 row affected (0.00 sec)

# 执行成功后可以看到表中有如下数据：
mysql> select * from student;
+----------+------+
| name     | age  |
+----------+------+
| xiaohei  |   22 |
| xiaobai  |   23 |
| xiaohong |   23 |
| xiaolan  |   23 |
+----------+------+
4 rows in set (0.00 sec)
~~~
接下来在spark中通过read上的jdbc来读取上面创建的表
~~~python
>>> df = spark.read.jdbc('jdbc:mysql://hadoop-mysql:3306/test?useUnicode=true&characterEncoding=utf8&autoReconnect=true&failOverReadOnly=false','student', numPartitions=2,properties={'user':'root','password':'root','driver':'com.mysql.jdbc.Driver'}, predicates=['age<=23'])
>>> df.show()
+--------+---+                                                                  
|    name|age|
+--------+---+
| xiaohei| 22|
| xiaobai| 23|
|xiaohong| 23|
| xiaolan| 23|
+--------+---+
~~~
这样就把mysql数据库中的数据读取出来了，这里需要注意的一点是分区数不要太大，特别是较大的spark集群链接外部的数据库，分区数过大容易带来高并发，给外部数据库带来特别大的压力，甚至造成外部数据库的崩溃！

5. load(path=None, format=None, schema=None, \**options) 可以通过load方法指定加载path路径下的文件，使用format指定格式如csv、text、json等，使用schema指定DataFrame的列信息

~~~python
df = spark.read.load(path='./datas/fengji.json', format='json')
df.show()
>>> 
+--------------------+--------------------+--------------------+--------------------+--------------------+--------------------+--------------------+--------------------+--------------------+--------------------+--------------------+--------------------+--------------------+--------------------+--------------------+--------------------+--------------------+--------------------+--------------------+--------------------+--------------------+--------------------+--------------------+--------------------+--------------------+--------------------+--------------------+--------------------+--------------------+
|               acc_x|               acc_y|     environment_tmp|     generator_speed|               group|             int_tmp|               label|        pitch1_angle|     pitch1_moto_tmp|       pitch1_ng5_DC|      pitch1_ng5_tmp|        pitch1_speed|        pitch2_angle|     pitch2_moto_tmp|       pitch2_ng5_DC|      pitch2_ng5_tmp|        pitch2_speed|        pitch3_angle|     pitch3_moto_tmp|       pitch3_ng5_DC|      pitch3_ng5_tmp|        pitch3_speed|               power|           timestamp|      wind_direction| wind_direction_mean|          wind_speed|        yaw_position|           yaw_speed|
+--------------------+--------------------+--------------------+--------------------+--------------------+--------------------+--------------------+--------------------+--------------------+--------------------+--------------------+--------------------+--------------------+--------------------+--------------------+--------------------+--------------------+--------------------+--------------------+--------------------+--------------------+--------------------+--------------------+--------------------+--------------------+--------------------+--------------------+--------------------+--------------------+
|[-1.023986385, 1....|[0.061109469, -1....|[-0.40391874, -0....|[1.223594525, 1.2...|[1.0, 1.0, 1.0, 1...|[0.014918413, -0....|[1, 1, 1, 1, 1, 1...|[0.555555556, 0.1...|[0.759, 0.769, 0....|[1.36, 0.44, 2.8,...|[1.307692308, 1.3...|[-1.68, 0.0, -1.4...|[0.506666667, 0.1...|[0.6, 0.609, 0.6,...|[0.0, 2.88, 0.32,...|[1.123076923, 1.1...|[-1.72, 0.0, -1.3...|[0.551111111, 0.1...|[0.59, 0.6, 0.569...|[1.56, -2.6, -0.0...|[0.783076923, 0.7...|[-1.68, 0.0, -1.4...|[2.515789697, 2.3...|[1.4463804E9, 1.4...|[-2.072739488, -2...|[-2.073626589, -1...|[1.859993305, 1.9...|[-0.655342752, -0...|[0.030803582, 0.0...|
+--------------------+--------------------+--------------------+--------------------+--------------------+--------------------+--------------------+--------------------+--------------------+--------------------+--------------------+--------------------+--------------------+--------------------+--------------------+--------------------+--------------------+--------------------+--------------------+--------------------+--------------------+--------------------+--------------------+--------------------+--------------------+--------------------+--------------------+--------------------+--------------------+
~~~

6. parquet(*paths) 用于读取parquet文件，注意 这里可以传入多个parquet文件路径

~~~python
df = spark.read.parquet('./datas/users.parquet')
df.show()
>>> 
+------+--------------+----------------+
|  name|favorite_color|favorite_numbers|
+------+--------------+----------------+
|Alyssa|          null|  [3, 9, 15, 20]|
|   Ben|           red|              []|
+------+--------------+----------------+
~~~

7. schema(schema) 通过schema指定读取数据源的格式之外，还可以通过schema指定自定义的数据源格式

~~~python
s = spark.read.schema('name String, favorite_numbers array<int>')
s.load('./datas/users.parquet').show()
>>> 
+------+----------------+
|  name|favorite_numbers|
+------+----------------+
|Alyssa|  [3, 9, 15, 20]|
|   Ben|              []|
+------+----------------+
~~~

8. table(tableName) 读取表或视图创建的DataFrame

~~~python
df = spark.read.table('worker.test1')
df.show()
>>>
+--------+---+                                                                  
|    name|age|
+--------+---+
|     tom| 22|
|   jerry| 23|
+--------+---+
~~~

9. text(paths, wholetext=False)读取文本文件创建DataFrame， 也可以指定多个路径

~~~ptyhon
df = spark.read.text('./datas/customers.csv', wholetext=True)
df.show()
>>> 
+--------------------+
|               value|
+--------------------+
|CustomerID,Genre,...|
+--------------------+
~~~
指定wholetext为False和True的区别
~~~python
df = spark.read.text('./datas/customers.csv', wholetext=False)
df.show()
>>> 
+--------------------+
|               value|
+--------------------+
|CustomerID,Genre,...|
|  0001,Male,19,15,39|
|  0002,Male,21,15,81|
| 0003,Female,20,16,6|
|0004,Female,23,16,77|
|0005,Female,31,17,40|
|0006,Female,22,17,76|
| 0007,Female,35,18,6|
|0008,Female,23,18,94|
|   0009,Male,64,19,3|
|0010,Female,30,19,72|
|  0011,Male,67,19,14|
|0012,Female,35,19,99|
|0013,Female,58,20,15|
|0014,Female,24,20,77|
|  0015,Male,37,20,13|
|  0016,Male,22,20,79|
|0017,Female,35,21,35|
|  0018,Male,20,21,66|
|  0019,Male,52,23,29|
+--------------------+
only showing top 20 rows
~~~

10. 有读就有写，上面所有的方法都是用于读取外部数据源生成DataFrame，接下来查看DataFrame这个类，这个类上提供了DataFrame中的数据写入外部存储的方法。可以通过DataFrame上的write属性获取DataFrame的引用

~~~python
dir(df.write)
>>> 
['__class__','__delattr__','__dict__','__dir__','__doc__','__eq__','__format__','__ge__','__getattribute__','__gt__','__hash__','__init__','__init_subclass__','__le__','__lt__','__module__','__ne__','__new__','__reduce__','__reduce_ex__','__repr__','__setattr__','__sizeof__','__str__','__subclasshook__','__weakref__','_df','_jwrite','_set_opts','_spark','_sq','bucketBy','csv','format','insertInto','jdbc','json','mode','option','options','orc','parquet','partitionBy','save','saveAsTable','sortBy','text']
~~~
可以看到， 它和DataFrameReader方法很相似，只不过一个是读一个是写

11. bucketBy(numBuckets, col, *cols),这个方法要结合saveAsTable方法使用，它的作用是要把DataFrame按照col分成指定的numBuckets数量的小集合，也就是俗称的分桶

~~~python
df = spark.read.format('json').load('./datas/fengji.json')
df.write.bucketBy(5, 'wind_speed').mode('overwrite').saveAsTable('worker.fengji')
~~~
~~~shell
root@hadoop-maste:~/workspace/datas# hdfs dfs -ls /home/hive/warehouse/worker.db/fengji
....
~~~
用mode指定是overwrite还是append追加
~~~python
df = spark.sql('select * from worker.fengji')
df.rdd.getNumPartitions()
>>> 5
~~~
可以看到从这个分桶表中读取出来的DataFrame它的分区数是和保存的时候设置的桶的个数是一样的

12. 同样的要把数据保存为csv格式，可以使用csv方法，有很多可选的参数。 (path, mode=None, compression=None, sep=None, quote=None, escape=None, header=None, nullValue=None, escapeQuotes=None, quoteAll=None, dateFormat=None, timestampFormat=None, ignoreLeadingWhiteSpace=None, ignoreTrailingWhiteSpace=None, charToEscapeQuoteEscaping=None)

~~~python
df = spark.range(1, 20, 2, 3)
df.write.mode('overwrite').csv('./datas/test/users.csv')
~~~
可以看到HDFS目录下保存好了一份数据
~~~shell
root@hadoop-maste:~/workspace/datas# hdfs dfs -ls /datas/test
...
~~~
要保存header需要指定header='True'， 默认是不保存header信息的
~~~python
df.write.mode('overwrite').csv('./datas/test/users.csv', header='True')
~~~

13. format(source) 这是一个通用的方法，通过指定source的类型，讲数据保存为指定source类型，但是这个方法要和save方法联合使用

~~~python
df = spark.range(1, 20, 2, 3)
df.write.format('parquet').mode('overwrite').save('./datas/test/hello')
~~~
~~~shell
root@hadoop-maste:~/workspace/datas# hdfs dfs -ls /datas/test/hello
...
~~~

14. insertInto(tableName, overwrite=False) 将DataFrame中的数据插入指定的表中，条件是DataFrame和table的schema信息必须一致，默认是追加，把overwrite关键字参数置为True将变为重写

~~~python
df = spark.range(1, 50, 3, 2)
df.write.mode('overwrite').saveAsTable('worker.hh')
df1 = spark.range(1, 100, 3, 2)
df.write.insertInto('worker.hh')
spark.sql("select * from worker.hh").show()
~~~

15. jdbc(url, table, mode=None, properties=None) 通过这个方法，可以将DataFrame中的数据写入外部的关系型数据库

~~~python
df = spark.range(1, 100, 7, 3)
df.write.mode('append').jdbc('jdbc:mysql://hadoop-mysql:3306/test/?useUnicode=true&characterEncoding=utf8&autoReconnect=true&failOverReadOnly=false','numbers', properties={'user':'root','password':'root','driver':'com.mysql.jdbc.Driver'})
~~~
查看mysql数据库
~~~sql
select * from numbers;
~~~

16. json 通过这个方法可以将DataFrame数据保存为json文件格式


17. mode(saveMode) 用于指定数据的保存模式
 1. append: 追加
 2. overwrie：重写
 3. error or errorifexists：数据存在抛出异常
 4. ignore：如果数据存在，则忽略当前的操作

18. parquet方法保存DataFrame保存为parquet格式


19. partitionBy(*cols) 输出到外面进行保存的时候可以指定按照哪些字段来分区

~~~python
df = spark.read.csv('./datas/titanic_train.csv', header=True, encoding='utf-8')
df.write.mode('overwrite').partitionBy('Sex').saveAsTable('worker.titanic')
spark.sql('select * from worker.titanic').rdd.getNumPartitions()
>>> 2
df.write.mode('overwrite').partitionBy('Age').saveAsTable('worker.titanic')
spark.sql('select * from worker.titanic').rdd.getNumPartitions()
>>> 89
~~~
可以看到不同的字段类分区，保存为文件之后，它的分区数是不一样的。因此指定合理的分区字段可以提升性能
~~~shell
root@hadoop-maste: ~/workspace/datas# hdfs dfs -ls /home/hive/warehouse/worker.db/titanic
...
~~~

20. 前面的保存操作都可以使用save(path=None, format=None, mode=None, partitionBy=None, **options)也可以使用save方法来实现，接收一个存储的路径、格式、模式、分区的字段，还有可选的其它参数

~~~python
df = spark.read.csv('./datas/titanic_train.csv', header=True, encoding='utf-8')
df.write.save('./datas/test/titanic/', format='parquet', mode='overwrite', partitionBy='Sex')
~~~

~~~shell
root@hadoop-maste: ~/workspace/datas# hdfs dfs -ls /datas/test//titanic
...
~~~
可以看到指定的分区字段呗增加到HDFS文件的路径上，可以理解为按类别分别存储

21. saveAsTable(name, format=None, mode=None, partitionBy=None, **options) 将DataFrame保存为表格，name指定保存的名字，format指定保存的格式，默认为parque的格式， mode指定是追加还是重写，partitionBy指定分区字段

~~~python
df = spark.read.csv('/datas/titanic_train.csv', header='True', encoding='utf-8')
df.write.saveAsTable('titanic_test', format='csv', mode='overwrite', partitionBy='Sex')
df1 = spark.sql('select * from titanic_test')
df1.rdd.getNumpartitions()
>>> 2
df1.show()
>>> 
~~~
查看hdfs上的存储路径
~~~shell
root@hadoop-maste:~/workspace/datas# hdfs dfs - ls /home/hive/warehouse/titanic_test
....
~~~

22. sortBy(col, *cols)这个方法用于对指定的桶中的数据进行排序。排好序之后写入外部的文件。需要和saveAsTable方法配置使用

~~~python
df = spark.read.csv('./datas/titanic_train.csv', header='True', encoding='utf-8')
df.write.bucketBy(10, 'Sex', 'Age').sortBy('Age').saveAsTable('worker.hehe', format='csv', mode='overwrite')
spark.sql('select * from worker.hehe').show()
>>>
~~~
查询数据， 发现都是排好序的数据

23. text(path, compression=None)， 将DataFrame中的数据保存为text文本问价。可通过compression关键字参数指定压缩的算法。text方法只支持一列数据且为String类型的数据。 可选用的压缩算法有：bzip2、 gzip、lz4、snappy and deflate

~~~python
df = spark.range(0, 100, 3, 6)
df1 = df.select(df['id'].cast('string'))
df1.write.mode('overwrite').text('./datas/hh/', compression='bzip2')
spark.read.text('./datas/hh').show()
>>> 
+-----+
|value|
+-----+
|   15|
|   18|
|   21|
|   24|
|   27|
|   30|
|   66|
|   69|
|   72|
|   75|
|   78|
|   81|
|   84|
|   87|
|   90|
|   93|
|   96|
|   99|
|   33|
|   36|
+-----+
only showing top 20 rows
~~~
~~~shell
root@hadoop-maste:~/workspace/datas# hdfs dfs -ls /datas/hh
...
~~~
**spark内部支持的压缩格式，在读取的时候会使用对应的解压缩算法解压出来**



- **本地文件上传到容器的指定目录**

~~~shell
docker cp ./datas/ 9aa59c5531d7:/usr/local/
~~~

- **容器中文件上传到hadoop目录**

~~~shell
hdfs dfs -put /datas/  /root/workspace/

## 查看
hdfs dfs -ls /root/workspace/
~~~

