
- 5.applicationId,用于获取注册到集群的应用的id  
编写applicationid.py文件


```python
from pyspark import SparkConf, SparkContext
import numpy as np

conf = SparkConf()
conf.set("master", "spark://hadoop-maste:7077")
context = SparkContext(conf=conf)
rdd = context.parallelize(np.arange(10))
print("applicationId:", context.applicationId)
print(rdd.collect())
context.stop()
```

运行 spark-submit applicationId.py  

- 6.binaryFiles读取二进制文件  
该方法用于读取二进制文件例如音频、视频、图片，对于每个文件器返回一个tuple，tuple的第一个元素为文件的路径，第二个参数为二进制文件的内容  
在hdfd的/datas/pics文件目录下上传了两张图片，使用binaryFiles读取/datas/pics目录的二进制图片数据  
新建binaryFiles.py文件，内容如下：

拷贝文件   
$ scp nlp1.png root@172.16.0.2:~    

$ docker exec -it hadoop-maste /bin/bash    

$ hdfs dfs -ls /   

$ hdfs dfs -mkdir -p /datas/pics  

$ hdfs dfs -put nlp* /datas/pics

$ hdfs dfs -ls /datas/pics


```python
from pyspark import SparkContext, SparkConf
import numpy as np

conf = SparkConf()
conf.set("master", "spark://hadoop-maste:7077")
# conf.setMaster("local").setAppName("myapp")
context = SparkContext(conf=conf)
rdd = context.binaryFiles('data/pics')
print("applicationId:", context.applicationId)
result = rdd.collect()
for data in result:
    print(data[0], data[1][:10])

context.stop()
```

运行spark-submit binaryFiles.py  

- 7.broadcast广播变量  
SparkContext上的broadcast方法用于创建广播变量，对于大于5M的共享变量，推荐用广播，广播机制可以最大限度的减少网络IO，从而提升性能  
例： 广播一个‘hello’字符串，在各个task中接收广播变量，拼接返回，新建broadcase.py文件，内容如下：


```python
from pyspark import SparkConf, SparkContext
import numpy as np

conf = SparkConf()
# conf.set("master", "spark://hadoop-maste:7077")
conf.setMaster("local").setAppName("myapp")
context = SparkContext(conf=conf)
broad = context.broadcast('hello')
rdd = context.parallelize(np.arange(27), 3)
print('applicationId:', context.applicationId)
print(rdd.map(lambda x: str(x) + broad.value).collect())

context.stop()
```

    applicationId: local-1559122924768
    ['0hello', '1hello', '2hello', '3hello', '4hello', '5hello', '6hello', '7hello', '8hello', '9hello', '10hello', '11hello', '12hello', '13hello', '14hello', '15hello', '16hello', '17hello', '18hello', '19hello', '20hello', '21hello', '22hello', '23hello', '24hello', '25hello', '26hello']
    

运行spark-submit broadcast.py  
从结果来看，分布式运行中的每个任务中热舞都接收到了广播的变量hello

- 8.defaultMinPartitions获取默认最小的分区数 

~~~python
from pyspark import SparkConf, SparkContext
import numpy as np

conf = SparkConf()
# conf.set("master", "spark://hadoop-maste:7077")
conf.setMaster("local").setAppName("myMP")
sc = SparkContext(conf=conf)
print('defaultMinParlitions:', sc.defaultMinPartitions)
sc.stop()
~~~


```python
print('defaultMinParlitions:', sc.defaultMinPartitions)
sc.stop()
```

    defaultMinParlitions: 1
    

- 9.emptyRDD创建一个空的RDD，该RDD没有分区，也没有任何数据

~~~python
from pyspark import SparkConf, SparkContext

conf = SparkConf()
conf.setMaster("local").setAppName("empty")
sc = SparkContext(conf=conf)
print(sc.emptyRDD)
rdd = sc.emptyRDD()
rdd.getNumPartitions()
rdd.collect()
~~~
- <bound method SparkContext.emptyRDD of \<SparkContext master=local    appName=empty>
- 0

- 10. getConf()方法返回作业的配置对象信息  
sc.getConf().toDebugString()

~~~python
>>> sc.getConf().toDebugString()
>>> 'spark.app.id=local-1559124837513\nspark.app.name=empty\nspark.driver.host=DESKTOP-FB0BDK2\nspark.driver.port=4773\nspark.executor.id=driver\nspark.master=local\nspark.rdd.compress=True\nspark.serializer.objectStreamReset=100\nspark.submit.deployMode=client\nspark.ui.showConsoleProgress=true'
>>> dir(sc.getConf())
>>> 
~~~

- 11.getLocalProperty和setLocalProperty获取和设置在本地线程中的属性信息，通过setLocalProperty设置，设置的属性只能对当前线程提交的作业起作用，对其它作业不起作用 

~~~python
>>> sc.setLocalProperty("abc","hello")
>>> sc.getLocalProperty('abc')
>>> 'hello'
~~~

- 12.setLogLevel设置日志级别，通过这个设置将会覆盖任何用户自定义的日志等级设置，取值有：<ALL, DEBUG, ERROR, FATAL, INFO, OFF, TRACE, WARN>  

~~~python
>>> sc.setLogLevel("ERROR")
>>> sc.parallelize(range(10),3).map(lambda x:x**2).collect()
>>> [0, 1, 4, 9, 16, 25, 36, 49, 64, 81]
>>>
~~~
通过对比不同的日志级别的输出，可以看出不同的日志级别的日志输出量是不同的，可以通过这一点选择核实的日志级别进行调试

- 13.getOrCreate得到或创建一个SparkContext对象，该方法的SparkContext对象为单例对象，该方法可以创建一个Sparkcontext对象 

~~~python
>>> sc1 = sc.getOrCreate()
>>> sc1 == sc
>>> True
~~~

- 14.hadoopFile读取“老”的hadoop接口提供的hdfs文件格式  

~~~python
>>> sc.hadoopFile("/data/num_data", inputFormatClass="org.apache.hadoop.mapred.TextInputFormat",keyClass="org.apache.hadoop.io.Text",valueClass="org.apache.hadoop.io.LongWritable").collect()
>>> [(0, '100')]
~~~
- 第一个参数为文件路径  
- 第二个参数为输入文件的格式  
- 第三个参数为键的格式  
- 第四个参数为值的格式  
**读取出来默认会把行号作为键**

- 15. textFile和saveAsTextfile读取位于HDFS上的文本文件  

~~~python
>>> datas = sc.textFile("/datas/num_data", 3)
>>> datas.collect()
>>> ['100']
~~~
这个方法读取位于hdfs上的文本类型
~~~python
>>> sc.parallelize(range(10),2).collect()
>>> [0, 1, 2, 3, 4, 5, 6, 7, 8, 9]
>>> sc.parallelize(range(10),2).saveAsTextFile('datas/rdd')  # 2为分区文件数
>>> hdfs dfs -ls /data/rdd
>>> hdsf dfs -cat /data/rdd/part-00000
>>> 0 1 2 3 4
>>>
~~~

- 16.parallelize使用python集合创建RDD，可以使用range函数，也可以使用numpy里面的arange方法创建  

~~~python
>>> import numpy as np
>>> sc.parallelize(np.arange(10),3).collect()
>>> [0, 1, 2, 3, 4, 5, 6, 7, 8, 9]
>>> sc.parallelize(range(10),2).collect()
>>> [0, 1, 2, 3, 4, 5, 6, 7, 8, 9]
~~~

- 17.saveAsPickleFile和pickleFile将RDD保存为python中的pickle压缩文件格式  

~~~python
>>> sc.parallelize(range(50),4).saveAsPickleFile('datas/pickle/aaa',5)
>>> sorted(sc.pickleFile('datas/pickle/aaa',3).collect())
>>> [0,1,2,3,....,48,49]
>>> sorted(sc.pickleFile('datas/pickle/aaa',3).collect(), reverse=True)
>>> [49,48,...,3,2,1]
~~~

- 18.range(start, end=None, step=1, numSlices=None)按照提供的起始值和步长，创建RDD；numSlices用于指定分区数  

~~~python
>>> rdd = sc.range(1, 50, 11, 3)
>>> rdd.collect()
>>> [1, 12, 23, 34, 45]
>>> dir(rdd)
>>> rdd.getNumPartitions()
>>> 3
~~~

- 19.runJob(rdd, partitionFunc, partitions=None, allowLocal=False)在给定的分区上运行指定的函数partitions用于指定分区的编号，是一个列表。若不指定分区默认为所有分区运行partitionFunc函数  

~~~python
>>> rdd = sc.range(1, 100, 11, 10)
>>> sc.runJob(rdd, lambda x: [a**2 for a in x])
>>> [1, 144, 529, 1156, 2025, 3136, 4489, 6084, 7921]
# 指定在 0，1，4，6分区上运行 a**2函数
>>> sc.runJob(rdd, lambda x: [a**2 for a in x], partitions=[0, 1, 4, 6])
>>> [1, 1156, 3136]
~~~

- 20.setCheckpointDir(dirName)设置检查点的目录，检查点用于异常发生时错误的恢复，该目录必须为HDFS目录;便于处理流式数据，为防止错误和异常发生，将检查点保存到外部目录。
设置检查点目录为datas/checkpoint

~~~python
>>> sc.setCheckpointDir('datas/checkpoint/')
>>> rdd = sc.range(1, 100, 11, 10)
>>> rdd.checkpoint()
>>> rdd.collect()
>>> [1, 12, 23, 34, 45, 56, 67, 78, 89]
~~~
运行完成之后，查看hdfs的/datas/checkpoint目录
~~~python
>>> hdfs dfs -ls /datas/checkpoint
>>> hdfs dfs -ls /datas/checkpoint/adc7ba9b-e090-4c97-9795-f7444b2d7dfb/rdd-42
>>> --    --  part-00000
~~~

- 21.sparkUser获取运行当前作业的用户名  

~~~python
>>> sc.sparkUser()
>>> 'root' 
~~~

- 22.startTime返回作业启动的时间  

~~~python
>>> sc.startTime
>>> 1559179184570
~~~
它返回的是Long类型的毫秒时间值，可借助[在线时间转换工](https://tool.lu/timestamp/)具查看具体时间

- 23.statusTracker()方法用于获取StatusTracker对象，通过该对象可以获取活动的jobs的id，活动的stage的id。job的信息，stage的信息。可以使用这个对象来实时监控作业运行的中间状态数据。  

~~~python
>>> t = sc.statusTracker()
>>> dir(t)
>>> <bound method StatusTracker.getJobInfo of <pyspark.status.StatusTracker object at 0x00000179669D2710>>
~~~

- 24.stop()方法用于停止SparkContext和cluster的连接，一般在书写程序最后一行都要加上这句。确保作业运行完成后连接和cluster集群断开。  

- 25.uiWebUrl返回web的url

~~~python
>>> sc.uiWebUrl
>>> 'http://DESKTOP-FB0BDK2:4040'
~~~

- 26.union(rdds)用合并多个rdd为一个rdd  

~~~python
>>> rdd1 = sc.parallelize(range(5),3)
>>> rdd2 = sc.parallelize(range(10),3)
>>> rdd3 = sc.union([rdd1, rdd2])
>>> rdd3.collect()
>>> [0, 1, 2, 3, 4, 0, 1, 2, 3, 4, 5, 6, 7, 8, 9]
>>> rdd4 = sc.union([rdd1, rdd2, rdd3])
>>> rdd4.collect()
~~~

- 27.version获取版本号  

~~~python
>>> sc.version
>>> '2.3.2'
~~~
