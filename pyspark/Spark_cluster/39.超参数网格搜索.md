
1. 在pyspark中可以使用网格搜索的功能来搜索超参数，搜索使模型性能达到最佳的参数  


mllib支持使用crossvalidator和trainValidationSplit这些有用的工具。crossvalidation用于交叉验证，而TrainvalidataionSplit用于训练集和验证集的划分。要是用这些功能，需要有如下的准备：  

- Estimator：用于搜索的算法或者pipeline
- ParamMaps：参数、在训练过程中用于选择，也称为参数网格 
- Evaluator：评估器，用于苹果模型的性能  


他的工作流程如下：  
- 将输入数据划分为训练数据和测试数据  
- 对于每个ParamMaps中的参数，使用划分好的训练数据和测试数据进行训练和测试。每个参数都用来尝试着填入算法，然后得到模型，最后使用评估器对性能进行评估 
- 遍历完网格中的所有参数，模型自动选择使性能达到最好的一组参数作为模型的参数  


通常的评估器和数据切分工具：  
- 对于回归，使用RegressionEvaluator进行评估
- 对于二分类，使用BinaryClassificationEvaluator进行评估
- 对于多分类，使用MulticlassClassficationEvaluator进行评估
- 对于交叉验证，使用CrossValidator来完成交叉验证

使用交叉验证和网格搜索训练模型，获取最佳的模型参数  

~~~python

from pyspark.sql import SparkSession
from pyspark.ml import Pipeline
from pyspark.ml.classification import LogisticRegression
from pyspark.ml.evaluation import BinaryClassificationEvaluator
from pyspark.ml.feature import HashingTF, Tokenizer
from pyspark.ml.tuning import CrossValidator, ParamGridBuilder

spark = SparkSession.builder.master('local').appName('fpgrowth').getOrCreate()

training = spark.createDataFrame([(0, 'a b c d e spark', 1.0), (1, 'b d', 0.0),
                                  (2, 'spark f g h', 1.0),
                                  (3, 'hadoop mapreduce', 0.0),
                                  (4, 'b spark who', 1.0), (5, 'g d a y', 0.0),
                                  (6, 'spark flay', 1.0),
                                  (7, 'was map reduce', 0.0),
                                  (8, 'e spark program', 1.0),
                                  (9, 'a e c l', 0.0),
                                  (10, 'spark complie', 1.0),
                                  (11, 'hadoop software', 0.0)],
                                 ['id', 'text', 'label'])

tokenizer = Tokenizer(inputCol='text', outputCol='words')
hashingTF = HashingTF(inputCol=tokenizer.getOutputCol(), outputCol='features')
lr = LogisticRegression(maxIter=2)
pipeline = Pipeline(stages=[tokenizer, hashingTF, lr])

paramGrid = ParamGridBuilder().addGrid(hashingTF.numFeatures,
                                       [10, 100, 1000]).addGrid(
                                           lr.regParam, [0.1, 0.01]).build()

crossval = CrossValidator(estimator=pipeline,
                          estimatorParamMaps=paramGrid,
                          evaluator=BinaryClassificationEvaluator(),
                          numFolds=5)

cvModel = crossval.fit(training)

test = spark.createDataFrame([(4, 'spark i j k'), (5, 'l m n'),
                              (6, 'mapreduce spark'), (7, 'apache hadoop')],
                             ['id', 'text'])

prediction = cvModel.transform(test)
selected = prediction.select('id', 'text', 'probability', 'prediction')
for row in selected.collect():
    print(row)
>>>  
Row(id=4, text='spark i j k', probability=DenseVector([0.6108, 0.3892]), prediction=0.0)
Row(id=5, text='l m n', probability=DenseVector([0.3934, 0.6066]), prediction=1.0)
Row(id=6, text='mapreduce spark', probability=DenseVector([0.5678, 0.4322]), prediction=0.0)
Row(id=7, text='apache hadoop', probability=DenseVector([0.3419, 0.6581]), prediction=1.0)
~~~

除了交叉验证之外，还有一种很直接的数据集的切分方法使，将数据集切成两份，一份训练集，一份验证集。这需要借助TrainValidationSplit来完成  

~~~python

from pyspark.ml.regression import LinearRegression
from pyspark.ml.tuning import ParamGridBuilder, TrainValidationSplit

data = spark.read.format('libsvm').load(
    '/data/mllib/sample_linear_regression_data.txt')
train, test = data.randomSplit([0.9, 0.1], seed=1234)
lr = LinearRegression(maxIter=2)
paramGrid = ParamGridBuilder().addGrid(lr.regParam, [0.1, 0.01]).addGrid(
    lr.fitIntercept, [False, True]).addGrid(lr.elasticNetParam,
                                            [0.0, 0.5, 1.0]).build()

tvs = TrainValidationSplit(estimator=lr,
                           estimatorParamMaps=paramGrid,
                           evaluator=RegressionEvaluator(),
                           trainRatio=0.8)

model = tvs.fit(train)
model.transform(test).select('features', 'label', 'prediction').show()
>>>  
+--------------------+--------------------+--------------------+
|            features|               label|          prediction|
+--------------------+--------------------+--------------------+
|(10,[0,1,2,3,4,5,...| -28.571478869743427|-0.19758765467277423|
|(10,[0,1,2,3,4,5,...| -15.056482974542433|  0.9557087910045516|
|(10,[0,1,2,3,4,5,...| -14.328978509075442|  1.2323111993042541|
|(10,[0,1,2,3,4,5,...| -13.976130931152703|-0.14247155164171038|
|(10,[0,1,2,3,4,5,...|  -9.789294452221961|  1.3264547696596514|
|(10,[0,1,2,3,4,5,...|  -8.680225911784335|  0.5946555167681347|
|(10,[0,1,2,3,4,5,...|  -6.556192430758147| -0.4877862135258635|
|(10,[0,1,2,3,4,5,...| -6.3459370724834265|  1.9474788856879874|
|(10,[0,1,2,3,4,5,...|  -5.615143641864686|  -2.672737271730727|
|(10,[0,1,2,3,4,5,...|  -4.706701061062994| -0.6280915631042162|
|(10,[0,1,2,3,4,5,...| -4.2775224863223915|-0.29171694043997565|
|(10,[0,1,2,3,4,5,...| -3.9916779937384743| -1.8851283030208088|
|(10,[0,1,2,3,4,5,...| -1.6896486293598596|  -1.071358099542677|
|(10,[0,1,2,3,4,5,...| -1.6848276484379352| -0.3669252588685907|
|(10,[0,1,2,3,4,5,...| -1.3101852978323116|  -1.064305602822544|
|(10,[0,1,2,3,4,5,...| -1.1009750789589774| -1.1592104642506396|
|(10,[0,1,2,3,4,5,...|  -1.074065343287859|  2.2160713194796817|
|(10,[0,1,2,3,4,5,...| -0.8995693247765151|  1.4362338358434446|
|(10,[0,1,2,3,4,5,...| -0.5233425797210233|-0.38461207644808776|
|(10,[0,1,2,3,4,5,...|-0.29622788243318465| -0.5921041606767776|
+--------------------+--------------------+--------------------+
only showing top 20 rows
~~~
