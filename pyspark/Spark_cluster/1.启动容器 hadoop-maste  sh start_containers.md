1. 启动容器 hadoop-maste  sh start_containers.sh
2. 进入hadoop-maste 容器  docker exec -it hadoop-maste /bin/bash
3. 使用 mysql mysql  -uroot  -proot -hhadoop-msql

~~~sql
mysql> use hive;
Reading table information for completion of table and column names
You can turn off this feature to get a quicker startup with -A

Database changed
mysql> show tables; 
+---------------------------+
| Tables_in_hive            |
+---------------------------+
| AUX_TABLE                 |
| BUCKETING_COLS            |
| CDS                       |
| COLUMNS_V2                |
| COMPACTION_QUEUE          |
| COMPLETED_COMPACTIONS     |
~~~

4. 查看进程启动情况

~~~shell
root@hadoop-maste:~# jps
400 SecondaryNameNode
193 NameNode
658 Master
568 ResourceManager
1037 Jps
~~~

5. 连接 hadoop-node1

~~~shell
oot@hadoop-maste:~# ssh hadoop-node1
Welcome to Ubuntu 18.04.2 LTS (GNU/Linux 4.18.0-17-generic x86_64)

 * Documentation:  https://help.ubuntu.com
 * Management:     https://landscape.canonical.com
 * Support:        https://ubuntu.com/advantage

root@hadoop-node1:~# jps
257 Worker
439 Jps
185 NodeManager

root@hadoop-node2:~# ssh hadoop-hive
Warning: Permanently added 'hadoop-hive,172.16.0.5' (ECDSA) to the list of known hosts.
Welcome to Ubuntu 18.04.2 LTS (GNU/Linux 4.18.0-17-generic x86_64)

 * Documentation:  https://help.ubuntu.com
 * Management:     https://landscape.canonical.com
 * Support:        https://ubuntu.com/advantage
root@hadoop-hive:~# jps
49 RunJar
313 Jps
79 RunJar
root@hadoop-hive:~#
~~~

6. 重启容器 sh restart_containers.sh

~~~shell
root@ubuntu:~# sh restart_containers.sh 
~~~

7. 初始化hive

- 验证hive语句

**beeline -u "jdbc:hive2://hadoop-hive:10001/default;transportMode=http;
httpPath=cliservice" --color=true -n root**

~~~shell
root@hadoop-maste:~# hiveserver2
2019-05-18 09:23:14: Starting HiveServer2
SLF4J: Class path contains multiple SLF4J bindings.
SLF4J: Found binding in [jar:file:/usr/local/apache-hive-2.3.2-bin/lib/log4j-slf4j-impl-2.6
.2.jar!/org/slf4j/impl/StaticLoggerBinder.class]SLF4J: Found binding in [jar:file:/usr/local/hadoop-2.7.3/share/hadoop/common/lib/slf4j-log
4j12-1.7.10.jar!/org/slf4j/impl/StaticLoggerBinder.class]SLF4J: See http://www.slf4j.org/codes.html#multiple_bindings for an explanation.
SLF4J: Actual binding is of type [org.apache.logging.slf4j.Log4jLoggerFactory]

root@hadoop-maste:~# beeline -u "jdbc:hive2://hadoop-hive:10001/default;transportMode=http;
httpPath=cliservice" --color=true -n root
SLF4J: Class path contains multiple SLF4J bindings.
SLF4J: Found binding in [jar:file:/usr/local/apache-hive-2.3.2-bin/lib/log4j-slf4j-impl-2.6
.2.jar!/org/slf4j/impl/StaticLoggerBinder.class]SLF4J: Found binding in [jar:file:/usr/local/hadoop-2.7.3/share/hadoop/common/lib/slf4j-log
4j12-1.7.10.jar!/org/slf4j/impl/StaticLoggerBinder.class]SLF4J: See http://www.slf4j.org/codes.html#multiple_bindings for an explanation.
SLF4J: Actual binding is of type [org.apache.logging.slf4j.Log4jLoggerFactory]
Connecting to jdbc:hive2://hadoop-hive:10001/default;transportMode=http;httpPath=cliservice
Connected to: Apache Hive (version 2.3.2)
Driver: Hive JDBC (version 2.3.2)
Transaction isolation: TRANSACTION_REPEATABLE_READ
Beeline version 2.3.2 by Apache Hive
0: jdbc:hive2://hadoop-hive:10001/default> show tables;
+-----------+
| tab_name  |
+-----------+
+-----------+
No rows selected (3.103 seconds)
0: jdbc:hive2://hadoop-hive:10001/default>create database worker;
No rows affected (3.741 seconds)
0: jdbc:hive2://hadoop-hive:10001/default> use worker;
No rows affected (0.246 seconds)
0: jdbc:hive2://hadoop-hive:10001/default> create table test(name string,age int)
. . . . . . . . . . . . . . . . . . . . .> ;
No rows affected (1.804 seconds)
0: jdbc:hive2://hadoop-hive:10001/default> show tables;
+-----------+
| tab_name  |
+-----------+
| test      |
+-----------+
1 row selected (0.39 seconds)
0: jdbc:hive2://hadoop-hive:10001/default>
~~~

上表目前为空，运行pyspark向其写入数据

~~~shell
root@ubuntu:~# docker exec -it hadoop-maste bash
root@hadoop-maste:vim .bashrc
# 在文件末尾添加  export PYSPARK_PYTHON=python3
root@hadoop-maste:~# pyspark --master spark://hadoop-maste:7077
/usr/local/spark-2.3.3-bin-hadoop2.7/bin/pyspark: line 45: python: command not found
Python 3.6.7 (default, Oct 22 2018, 11:32:17) 
[GCC 8.2.0] on linux
Type "help", "copyright", "credits" or "license" for more information.
2019-05-18 09:43:14 WARN  NativeCodeLoader:62 - Unable to load native-hadoop library for yo
ur platform... using builtin-java classes where applicableSetting default log level to "WARN".
To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel)
.2019-05-18 09:43:23 WARN  Utils:66 - spark.executor.instances less than spark.dynamicAlloca
tion.minExecutors is invalid, ignoring its setting, please update your configs.Welcome to
      ____              __
     / __/__  ___ _____/ /__
    _\ \/ _ \/ _ `/ __/  '_/
   /__ / .__/\_,_/_/ /_/\_\   version 2.3.3
      /_/

Using Python version 3.6.7 (default, Oct 22 2018 11:32:17)
SparkSession available as 'spark'.
>>> data = [("regan", 27), ("ting", 24)]
>>> spark.createDataFrame(data)ls
DataFrame[_1: string, _2: bigint]
>>> spark.createDataFrame(data).show()
+-----+---+                                      
|   _1| _2|
+-----+---+
|login| 27|
| ting| 24|
+-----+---+
>>> spark.createDataFrame(data, schema=["name", "age"]).show()
+-----+---+                                          
| name|age|
+-----+---+
|login| 27|
| ting| 24|
+-----+---+
>>>df = spark.createDataFrame(data, schema=["name", "age"])       
>>> df.write.saveAsTable("worker.test")
~~~

访问 hive

~~~shell
root@hadoop-maste:~# beeline -u "jdbc:hive2://hadoop-hive:10001/default;transportMode=http;
httpPath=cliservice" --color=true -n root
Connected to: Apache Hive (version 2.3.2)
Driver: Hive JDBC (version 2.3.2)
Transaction isolation: TRANSACTION_REPEATABLE_READ
Beeline version 2.3.2 by Apache Hive
0: jdbc:hive2://hadoop-hive:10001/default> use worker;
0: jdbc:hive2://hadoop-hive:10001/default>show tables;
+-----------+
| tab_name  |
+-----------+
|    test   |
+-----------+
0: jdbc:hive2://hadoop-hive:10001/default>select * from test;
+-----+---+                                          
| name|age|
+-----+---+
|login| 27|
| ting| 24|
+-----+---+
~~~

8. 验证 hdfs 

~~~shell
root@hadoop-maste:~# hdfs dfs -ls /
......
root@hadoop-maste:~# hdfs dfs -ls /home/hive/warehouse
Found 1 items
...........         /home/hive/warehouse/worker.db
root@hadoop-maste:~# hdfs dfs -ls /home/hive/warehouse/worker.db
...................						/worker.db/test #(表，以目录形式存在)
root@hadoop-maste:~# hdfs dfd -ls /home/hive/warehouse/worker.db/test
（使用SEQUENCEFILE、RCFILE文件格式）
~~~





