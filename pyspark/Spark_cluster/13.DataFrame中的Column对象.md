
1. Column 代表的是DataFrame中列的信息，DataFrame由一到多个Column对象组成，它控制对列的一些设置及访问的方法，例如列的数据类型，列的别名，列的排序等都可以通过Column对象来设置


2. 获取DataFrame中指定的列，通常有两种方法，一种是通过“[]”索引列名的形式，第二种是DataFrame上直接调用“.” 调用列名称即可获取

~~~python
df = spark.read.csv('./datas/customers.csv', header=True)
df.Age
>>> Column<b'Age'>
df['Age']
>>> Column<b'Age'>
~~~
要查看DataFrame上有哪些列，可以通过DataFrame上columns属性来获取，他是一个列表，记录DataFrame中的列名称的lis

~~~python
df.columns
>>> ['CustomerID', 'Genre', 'Age', 'Annual Income (k$)', 'Spending Score (1-100)']
~~~

3. alias(*alias, \*** kwargs) 这个方法是用来为列指定别名的，例如一个列的名称很长很复杂，可以给它指定一个比较容易记忆的别名，这样在程序中更容易实现处理，他可以接收一个 \**kwargs 字典类型的参数，用于指定一些额外的信息，这些信息可以通过schma上的metadata来获取

~~~python
df.select(df['Age'].alias('age')).where('age <= 18').show()
>>> 
+---+
|age|
+---+
| 18|
| 18|
| 18|
| 18|
+---+

df.select(df['Age'].alias('age', metadata={
    'max': 120,
    'your name': 'tom'
})).where('age <= 24').schema['age'].metadata['max']
>>> 120

df.select(df['Age'].alias('age', metadata={
    'max': 120,
    'your name': 'tom'
})).where('age <= 24').schema['age'].metadata
>>> {'your name': 'tom', 'max': 120}
~~~

4. asc()/desc() 在Column 上调用该方法，将会返回一个按照升序/降序排列的表达式

~~~pyton
df = spark.read.csv('./datas/customers.csv', header=True)
df.select(df.Age).orderBy(df.Age.asc()).distinct().show(5)
>>> 
+---+
|Age|
+---+
| 18|
| 19|
| 20|
| 21|
| 22|
+---+
only showing top 5 rows

df.select(df.Age).orderBy(df.Age.desc()).distinct().show(5)
>>> 
+---+
|Age|
+---+
| 70|
| 69|
| 68|
| 67|
| 66|
+---+
only showing top 5 rows
~~~

5. astype(dataType) 将列的类型转换为指定的类型

~~~python
df.select(df.Age.astype('int'), df['Spending Score (1-100)'].astype('int'),
          df['Annual Income (k$)'].astype('int')).dtypes
>>>
[('Age', 'int'),
 ('Spending Score (1-100)', 'int'),
 ('Annual Income (k$)', 'int')]     
~~~

astype方法和cast方法的作用是一样的，都是起到类型转换的作用

~~~python
df.select(df.Age.cast('int'), df['Spending Score (1-100)'].cast('int'),
          df['Annual Income (k$)'].cast('int')).dtypes
>>>
[('Age', 'int'),
 ('Spending Score (1-100)', 'int'),
 ('Annual Income (k$)', 'int')]
~~~

6. between(lowerBound, upperBound) 在列上计算数值是否位于lowerBound和upperBound之间，若是返回True，否则返回False

~~~python
df.select(
    df.Age.cast('int').between(18,20), 
    df['Spending Score (1-100)'].cast('int'),
    df['Annual Income (k$)'].cast('int')).show(5)
>>> 
 +-------------------------------------------------------+----------------------+------------------+
|((CAST(Age AS INT) >= 18) AND (CAST(Age AS INT) <= 20))|Spending Score (1-100)|Annual Income (k$)|
+-------------------------------------------------------+----------------------+------------------+
|                                                   true|                    39|                15|
|                                                  false|                    81|                15|
|                                                   true|                     6|                16|
|                                                  false|                    77|                16|
|                                                  false|                    40|                17|
+-------------------------------------------------------+----------------------+------------------+
only showing top 5 rows

df.filter(df.Age.cast('int').between(18,20)).show(5)
>>>
+----------+------+---+------------------+----------------------+
|CustomerID| Genre|Age|Annual Income (k$)|Spending Score (1-100)|
+----------+------+---+------------------+----------------------+
|      0001|  Male| 19|                15|                    39|
|      0003|Female| 20|                16|                     6|
|      0018|  Male| 20|                21|                    66|
|      0034|  Male| 18|                33|                    92|
|      0040|Female| 20|                37|                    75|
+----------+------+---+------------------+----------------------+
only showing top 5 rows
~~~

7. bitwiseAND(other) 本列和other列按位与运算

~~~python
df = spark.createDataFrame([(1, 2), (2, 3), (3, 4)], schema=['a', 'b'])
df.select(df.a.bitwiseAND(df.b)).show()
>>> 
+-------+
|(a & b)|
+-------+
|      0|
|      2|
|      0|
+-------+
~~~
bitwiseOR(other) 按位求和

~~~python
df.select(df.a.bitwiseOR(df.b)).show()
>>> 
+-------+
|(a | b)|
+-------+
|      3|
|      3|
|      7|
+-------+
~~~

bitwiseXOR(other) 亦或计算

~~~python
df.select(df.a.bitwiseXOR(df.b)).show()
>>> 
+-------+
|(a ^ b)|
+-------+
|      3|
|      1|
|      7|
+-------+
~~~ 

8. contains(other) 返回字段中是否包含某个内容

~~~python
df.select(df.a.contains(2)).show()
>>> 
+--------------+
|contains(a, 2)|
+--------------+
|         false|
|          true|
|         false|
+--------------+
~~~
可以由这个返回的bool类型的值， 用filter过滤

~~~python
df.filter(df.a.contains(2)).show()
>>> 
+---+---+
|  a|  b|
+---+---+
|  2|  3|
+---+---+
~~~

9. endwith(other) 用于判断字符型的列中的值是否以xxx字符串结尾

~~~python
df = spark.read.csv('./datas/customers.csv', header=True)
df.select(df.CustomerID.endswith('9')).show(5)
>>> 
+-----------------------+
|endswith(CustomerID, 9)|
+-----------------------+
|                  false|
|                  false|
|                  false|
|                  false|
|                  false|
+-----------------------+
only showing top 5 rows
~~~
过滤出CustomerID以9结尾的记录

~~~python
df.filter(df.CustomerID.endswith('9')).show()
>>> 
+----------+------+---+------------------+----------------------+
|CustomerID| Genre|Age|Annual Income (k$)|Spending Score (1-100)|
+----------+------+---+------------------+----------------------+
|      0009|  Male| 64|                19|                     3|
|      0019|  Male| 52|                23|                    29|
|      0029|Female| 40|                29|                    31|
|      0039|Female| 36|                37|                    26|
|      0049|Female| 29|                40|                    42|
|      0059|Female| 27|                46|                    51|
|      0069|  Male| 19|                48|                    59|
|      0079|Female| 23|                54|                    52|
|      0089|Female| 34|                58|                    60|
|      0099|  Male| 48|                61|                    42|
|      0109|  Male| 68|                63|                    43|
|      0119|Female| 51|                67|                    43|
|      0129|  Male| 59|                71|                    11|
|      0139|  Male| 19|                74|                    10|
|      0149|Female| 34|                78|                    22|
|      0159|  Male| 34|                78|                     1|
|      0169|Female| 36|                87|                    27|
|      0179|  Male| 59|                93|                    14|
|      0189|Female| 41|               103|                    17|
|      0199|  Male| 32|               137|                    18|
+----------+------+---+------------------+----------------------+

df.where(df.CustomerID.endswith('9')).show()
>>> 
+----------+------+---+------------------+----------------------+
|CustomerID| Genre|Age|Annual Income (k$)|Spending Score (1-100)|
+----------+------+---+------------------+----------------------+
|      0009|  Male| 64|                19|                     3|
|      0019|  Male| 52|                23|                    29|
|      0029|Female| 40|                29|                    31|
|      0039|Female| 36|                37|                    26|
|      0049|Female| 29|                40|                    42|
|      0059|Female| 27|                46|                    51|
|      0069|  Male| 19|                48|                    59|
|      0079|Female| 23|                54|                    52|
|      0089|Female| 34|                58|                    60|
|      0099|  Male| 48|                61|                    42|
|      0109|  Male| 68|                63|                    43|
|      0119|Female| 51|                67|                    43|
|      0129|  Male| 59|                71|                    11|
|      0139|  Male| 19|                74|                    10|
|      0149|Female| 34|                78|                    22|
|      0159|  Male| 34|                78|                     1|
|      0169|Female| 36|                87|                    27|
|      0179|  Male| 59|                93|                    14|
|      0189|Female| 41|               103|                    17|
|      0199|  Male| 32|               137|                    18|
+----------+------+---+------------------+----------------------+
~~~

10. eqNullSafe(other) 用于判断是否为空，在pyspark中空用None表示，和Pandas中的Nan不一样，pyspark中的NaN不表示空

~~~python
import numpy as np
df = spark.createDataFrame([(1, None), (2, np.NaN), (3, 999.0)],
                           schema=['a', 'b'])
df.show()
>>> 
+---+-----+
|  a|    b|
+---+-----+
|  1| null|
|  2|  NaN|
|  3|999.0|
+---+-----+

df.select(df['b'].eqNullSafe(None), df['b'].eqNullSafe(999.0),
          df['b'] == 998.0).show()
>>> 
+------------+-------------+-----------+
|(b <=> NULL)|(b <=> 999.0)|(b = 998.0)|
+------------+-------------+-----------+
|        true|        false|       null|
|       false|        false|      false|
|       false|         true|      false|
+------------+-------------+-----------+
~~~

11. getFiled(name) 由字段的名称从Strufiled中返回某个值

~~~python
from pyspark.sql import Row
df = spark.createDataFrame([Row(r=Row(a=1, b='b'))])
df.show()
>>> 
+------+
|     r|
+------+
|[1, b]|
+------+
df.select(df.r.getField('b')).show()
>>>
+---+
|r.b|
+---+
|  b|
+---+
~~~

12. getitem(key) 得到一个字段所对应的列表

~~~python
df = spark.createDataFrame([([1, 2], {'key': 'value'})], schema=['l', 'd'])
df.show()
>>> 
+------+--------------+
|     l|             d|
+------+--------------+
|[1, 2]|[key -> value]|
+------+--------------+

df.select(df.l.getItem(1), df.d.getItem('key')).show()
>>> 
+----+------+
|l[1]|d[key]|
+----+------+
|   2| value|
+----+------+
~~~

13. isNotNull 判断字段是否为空【None】

~~~python
df = spark.createDataFrame([(1, None), (2, 3), (3, None)], schema=['a', 'b'])
df.filter(df.b.isNotNull()).show()
>>> 
+---+---+
|  a|  b|
+---+---+
|  2|  3|
+---+---+
~~~

14. isNull() 判断字段为空【None】

~~~python
df = spark.createDataFrame([(1,None),(2,3.),(3,np.NaN)], schema=['a','b'])
df.filter(df.b.isNull()).show()
>>> 
+---+----+
|  a|   b|
+---+----+
|  1|null|
|  3|null|
+---+----+
df.show()
>>> 
+---+----+
|  a|   b|
+---+----+
|  1|null|
|  2|   3|
|  3| NaN|
+---+----+
~~~
**注意：NaN类型在pyspark看来不是空的，这与Pandas和numpy不一样，而且NaN在pypsark中被认为是Double类型的数据**

~~~python
df.dtypes
>>> 
[('a', 'bigint'), ('b', 'double')]
~~~

15. isin(*cols) 判断字段是否在某个集合中

~~~python
df.select(df.a.isin(1, 2, 3), df.b.isin(None, np.NaN)).show()
>>> 
+----------------+------------------+
|(a IN (1, 2, 3))|(b IN (NULL, NaN))|
+----------------+------------------+
|            true|              null|
|            true|              null|
|            true|              true|
+----------------+------------------+
~~~

16. like(other) 可以使用类似SQL中模糊匹配，返回boolean值

~~~python
df = spark.createDataFrame([(1, 'zm123'), (2, 'zmc123'), (3, 'zmc123')],
                           schema=['a', 'b'])
df.show()
>>> 
+---+------+
|  a|     b|
+---+------+
|  1| zm123|
|  2|zmc123|
|  3|zmc123|
+---+------+

df.select(df.b.like('z%')).show()
>>> 
+---------+
|b LIKE z%|
+---------+
|     true|
|     true|
|     true|
+---------+
df.select(df.b.like('%3')).show()
>>> 
+---------+
|b LIKE %3|
+---------+
|     true|
|     true|
|     true|
+---------+
df.filter(df.b.like('zm%')).show()
>>> 
+---+------+
|  a|     b|
+---+------+
|  1| zm123|
|  2|zmc123|
|  3|zmc123|
+---+------+
~~~
这个方法和rlike(other)类似，但是rlike可以提供正则表达式的支持。可以支持正则表达式中的所有语法。例如说可以通过正则表达式指定b列中的字段以‘z'开头，’3‘结尾

~~~python
df = spark.createDataFrame([(1, 'zm123'), (2, 'zc123'), (3, 'zmc123'),
                            (3, 'qmc123'), (3, 'zmc125')],
                           schema=['a', 'b'])
df.filter(df.b.rlike('^z.+3$')).show()
>>> 
+---+------+
|  a|     b|
+---+------+
|  1| zm123|
|  2| zc123|
|  3|zmc123|
+---+------+
~~~

17. name(*alias, \*** kwargs) 这个方法是alias的别名，是为列取别名

~~~python
df.filter(df.b.rlike('^z.+3$')).select(df.a.name('id'),df.b.name('name')).show()
>>> 
+---+------+
| id|  name|
+---+------+
|  1| zm123|
|  2| zc123|
|  3|zmc123|
+---+------+                     
~~~

18. otherwise(value) 用于当条件不满足时，返回指定的value值

~~~python
from pyspark.sql import functions as F
df = spark.createDataFrame([(1, 'zm123'), (2, 'zc123'), (3, 'zmc123'),
                            (3, 'qmc123'), (3, 'zmc125')],
                           schema=['a', 'b'])
df.select(df.b, F.when(df.a >= 2, 1).otherwise(0), df.a).show()
>>> 
+------+------------------------------------+---+
|     b|CASE WHEN (a >= 2) THEN 1 ELSE 0 END|  a|
+------+------------------------------------+---+
| zm123|                                   0|  1|
| zc123|                                   1|  2|
|zmc123|                                   1|  3|
|qmc123|                                   1|  3|
|zmc125|                                   1|  3|
+------+------------------------------------+---+
~~~

19. startwith(other) 该方法用于判断列字段是否以某个字符串为开头

~~~python
df.select(df.b.startswith('q')).show()
>>> 
+----------------+
|startswith(b, q)|
+----------------+
|           false|
|           false|
|           false|
|            true|
|           false|
+----------------+
~~~
返回值时true或者false， 因此可以使用filter根据startwith来进行过滤

~~~python
df.filter(df.b.startswith('zm')).show()
>>> 
+---+------+
|  a|     b|
+---+------+
|  1| zm123|
|  3|zmc123|
|  3|zmc125|
+---+------+
~~~

20. substr(startPos, length) 用于返回列经过字符串切割后的字符。接收两个字符，第一个参数为字符开始的位置，第二个参数为七个后的长度

~~~python
df.select(df.b, df.b.substr(0, 3)).show()
>>> 
+------+------------------+
|     b|substring(b, 0, 3)|
+------+------------------+
| zm123|               zm1|
| zc123|               zc1|
|zmc123|               zmc|
|qmc123|               qmc|
|zmc125|               zmc|
+------+------------------+
~~~

21. when(condition, value) 在列上接收一个条件判断，若条件成立，返回value值

~~~python
df.select(df.b, F.when(df.a > 2, 1).when(df.a <= 2, -1).otherwise(0)).show()
>>> 
+------+---------------------------------------------------------+
|     b|CASE WHEN (a > 2) THEN 1 WHEN (a <= 2) THEN -1 ELSE 0 END|
+------+---------------------------------------------------------+
| zm123|                                                       -1|
| zc123|                                                       -1|
|zmc123|                                                        1|
|qmc123|                                                        1|
|zmc125|                                                        1|
+------+---------------------------------------------------------+
~~~
