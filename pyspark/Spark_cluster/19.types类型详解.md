
1. sparksql中的类型，位于pyspark.sql.types这个模块下有哪些数据类型：

~~~python
from pyspark.sql import types
dir(types)
>>> 
['ArrayType','AtomicType','BinaryType','BooleanType','ByteType','CloudPickleSerializer','DataType','DataTypeSingleton','DateConverter','DateType','DatetimeConverter','DecimalType','DoubleType','FloatType','FractionalType','IntegerType','IntegralType','JavaClass','LongType','MapType','NullType','NumericType','Row','ShortType','SparkContext','StringType','StructField','StructType','TimestampType','UserDefinedType','_BRACKETS','_FIXED_DECIMAL','__all__','__builtins__','__cached__','__doc__','__file__','__loader__','__name__','__package__','__spec__','_acceptable_types','_all_atomic_types','_all_complex_types','_array_signed_int_typecode_ctype_mappings','_array_type_mappings','_array_unsigned_int_typecode_ctype_mappings','_atomic_types','_check_dataframe_convert_date','_check_dataframe_localize_timestamps','_check_series_convert_timestamps_internal','_check_series_convert_timestamps_local_tz','_check_series_convert_timestamps_localize','_check_series_convert_timestamps_tz_local','_create_converter','_create_row','_create_row_inbound_converter','_exception_message','_get_local_timezone','_has_nulltype','_ignore_brackets_split','_infer_schema','_infer_type','_int_size_to_type','_make_type_verifier','_merge_type','_need_converter','_parse_datatype_json_string','_parse_datatype_json_value','_parse_datatype_string','_test','_type_mappings','_typecode','array','base64','basestring','calendar','ctypes','datetime','decimal','dt','from_arrow_schema','from_arrow_type','json','long','platform','re','register_input_converter','size','sys','time','to_arrow_schema','to_arrow_type','unicode']
~~~
有很多种类型， 其中DataType是这些数据类型的基类

2. Nulltype 位于pysaprk.sql.types.Nulltype。他表示的是一种空的类型。代表着None数据，用于表示无法进行推断的数据类型

~~~python
from pyspark.sql import SparkSession
from pyspark.sql.types import *

spark = SparkSession.builder.master('local').appName('types').getOrCreate()
sc = spark.sparkContext

schema = StructType([StructField('name', NullType(),True),StructField('age',IntegerType(),True)])
l = [(None, 14),(None, 20)]
rdd = sc.parallelize(l, 3)
df = spark.createDataFrame(rdd, schema)
df.show()
>>> 
+----+---+
|name|age|
+----+---+
|null| 14|
|null| 20|
+----+---+
df.dtypes
>>> [('name', 'null'), ('age', 'int')]
~~~

3. StringType代表的是String类型，而IntegerType代表的是整数类型

~~~python
schema = StructType([
    StructField('name', StringType(), True),
    StructField('age', IntegerType(), True)
])
l = [('tom', 14), ('jerry', 13)]
rdd = sc.parallelize(l, 3)
df = spark.createDataFrame(rdd, schema)
df.show()
>>> 
+-----+---+
| name|age|
+-----+---+
|  tom| 14|
|jerry| 13|
+-----+---+
df.dtypes
>>> [('name', 'string'), ('age', 'int')]
~~~

4. BolleanType 代表的是布尔类型

~~~python
schema = StructType([StructField('name', BooleanType(), True),
                    StructField('age', IntegerType(), True)])
l = [(True, 13), (False, 12)]
rdd = sc.parallelize(l, 3)
df = spark.createDataFrame(rdd, schema)
df.show()
>>> 
+-----+---+
| name|age|
+-----+---+
| true| 13|
|false| 12|
+-----+---+
df.dtypes
>>> [('name', 'boolean'), ('age', 'int')]
~~~

5. DataType 日期类型

~~~python
from datetime import datetime

today = datetime.now().date()
todaytime = datetime.now()
schema = StructType([StructField('date', DateType(),True)])
l = [(today,)]
rdd = sc.parallelize(l, 3)
df = spark.createDataFrame(rdd,schema)
df.show()
>>> 
+----------+
|      date|
+----------+
|2019-07-11|
+----------+
df.dtypes
>>> [('date', 'date')]
~~~

6. TimestampType 时间戳类型

~~~python
time = datetime.now().date()
todaytime = datetime.now()
schema = StructType([StructField('date', TimestampType(), True)])
l = [(todaytime, )]
rdd = sc.parallelize(l, 3)
df = spark.createDataFrame(rdd, schema)
df.show()
>>> 
+--------------------+
|                date|
+--------------------+
|2019-07-11 15:11:...|
+--------------------+
df.dtypes
>>> [('date', 'timestamp')]
~~~

7. DecimalType(precision=10, scale=0) 十进制类型，关键字参数precesion治党数字的个数，scale指定小数点后的位数

~~~python
from decimal import Decimal

schema = StructType([StructField('decimal', DecimalType(20, 10), True)])
l = [(Decimal(100), ), (Decimal(9876), )]
rdd = sc.parallelize(l, 3)
df = spark.createDataFrame(rdd, schema)
df.show()
>>> 
+---------------+
|        decimal|
+---------------+
| 100.0000000000|
|9876.0000000000|
+---------------+
df.dtypes
>>> [('decimal', 'decimal(20,10)')]
~~~

8. DoubleType 双精度实数类型

~~~python
import math

schema = StructType([StructField('decimal', DoubleType(), True)])
l = [(math.e,), (math.pi,)]
rdd = sc.parallelize(l, 3)
df = spark.createDataFrame(rdd, schema)
df.show()
>>> 
+-----------------+
|          decimal|
+-----------------+
|2.718281828459045|
|3.141592653589793|
+-----------------+
df.dtypes
>>> [('decimal', 'double')]
~~~

9. FloatType 单精度浮点数类型

~~~python
import math

schema = StructType([StructField('decimal', FloatType(), True)])
l = [(math.e, ), (math.pi, )]
rdd = sc.parallelize(l, 3)
df = spark.createDataFrame(rdd, schema)
df.show()
>>> 
+---------+
|  decimal|
+---------+
|2.7182817|
|3.1415927|
+---------+
df.dtypes
>>> [('decimal', 'float')]
~~~

10. ByteType 字节类型，实际上是一个tinyint类型，8位

~~~python
schema = StructType([StructField('byte', ByteType(), True)])
l = [(1, ), (2, )]
rdd = sc.parallelize(l, 3)
df = spark.createDataFrame(rdd, schema)
df.show()
>>> 
+----+
|byte|
+----+
|   1|
|   2|
+----+
df.dtypes
>>> [('byte', 'tinyint')]
~~~

11. ShortType 短整型，16位

~~~python
schema = StructType([StructField('ShortType', ShortType(), True)])
l = [(19, ), (211, )]
rdd = sc.parallelize(l, 3)
df = spark.createDataFrame(rdd, schema)
df.show()
>>> 
+---------+
|ShortType|
+---------+
|       19|
|      211|
+---------+
df.dtypes
>>>  [('ShortType', 'smallint')]
~~~

12. IntegerType 32位整型

~~~python
schema = StructType([StructField('IntegerType', IntegerType(), True)])
l = [(19, ), (211, )]
rdd = sc.parallelize(l, 3)
df = spark.createDataFrame(rdd, schema)
df.show()
>>> 
+-----------+
|IntegerType|
+-----------+
|         19|
|        211|
+-----------+
df.dtypes
>>> [('IntegerType', 'int')]
~~~

13. LongType 长类型，64位

~~~python
schema = StructType([StructField('LongType', LongType(), True)])
l = [(19, ), (211, )]
rdd = sc.parallelize(l, 3)
df = spark.createDataFrame(rdd, schema)
df.show()
>>> 
+--------+
|LongType|
+--------+
|      19|
|     211|
+--------+
df.dtypes
>>> [('LongType', 'bigint')]
~~~

14. ArrayType(elementType, containsNull=True), elementType指定Array中的元素的数据类型， containsNull指定是否包含None值

~~~python
schema = StructType([StructField('ArrayType', ArrayType(LongType()), True)])
l = [([3, 4, 5], ), ([2, 3, 4, 5, 6], )]
rdd = sc.parallelize(l, 3)
df = spark.createDataFrame(rdd, schema)
df.show()
>>> 
+---------------+
|      ArrayType|
+---------------+
|      [3, 4, 5]|
|[2, 3, 4, 5, 6]|
+---------------+
df.dtypes
>>> [('ArrayType', 'array<bigint>')]
~~~

15. MapType(keyType, valueType, valueContainsNull=True), keyType指定key的类型，valueType指定值的类型，valueContainsNull指定是否为空

~~~python
schema = StructType([StructField('MapType', MapType(StringType(),StringType()), True)])
l = [({'age': '76'},), ({'age': '90'}, )]
rdd = sc.parallelize(l, 3)
df = spark.createDataFrame(rdd, schema)
df.show()
>>> 
+-----------+
|    MapType|
+-----------+
|[age -> 76]|
|[age -> 90]|
+-----------+
df.dtypes
>>> [('MapType', 'map<string,string>')]
~~~

16. StructType命名了列信息之后， 可以使用fieldNames获取字段的名称

~~~python
schema = StructType([StructField('MapType', MapType(StringType(), StringType()), True)])
l = [({'age': '88'},),({'age': '99'},)]
rdd = sc.parallelize(l, 3)
df = spark.createDataFrame(rdd, schema)
df.show()
>>> 
+-----------+
|    MapType|
+-----------+
|[age -> 88]|
|[age -> 99]|
+-----------+
df.dtypes
>>> [('MapType', 'map<string,string>')]
~~~
