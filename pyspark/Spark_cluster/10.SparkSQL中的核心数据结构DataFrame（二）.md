
### SparkSQL中的核心数据结构DataFrame（二）

11. columns()以列表的形式返回DataFrame的所有列名

~~~python
from pyspark.sql import SparkSession

spark = SparkSession.builder.master('local')\
                    .appName('test')\
                    .config('spark','value')\
                    .getOrCreate()
df = spark.read.csv('./datas/customers.csv', header=True)
df.columns
>>> ['CustomerID', 'Genre', 'Age', 'Annual Income (k$)', 'Spending Score (1-100)']
~~~

12. corr(col1, col2, method=None)计算DataFrame中两列的相关系数，现在只支持皮尔森相关系数perason，下面计算年龄和收入之间的相关性。相关系数计算公式：
![image.png](attachment:image.png)
等于协方差除以标准差的乘积

~~~python
df = spark.read.csv('./datas/customers.csv', header=True)
pdf = df.toPandas()
pdf['Age'] = pdf['Age'].astype('int')
pdf['Annual Income (k$)'] = pdf['Annual Income (k$)'].astype('int')
pdf['Spending Score (1-100)'] = pdf['Spending Score (1-100)'].astype('int')
pdf.info()
>>> 
<class 'pandas.core.frame.DataFrame'>
RangeIndex: 200 entries, 0 to 199
Data columns (total 5 columns):
CustomerID                200 non-null object
Genre                     200 non-null object
Age                       200 non-null int32
Annual Income (k$)        200 non-null int32
Spending Score (1-100)    200 non-null int32
dtypes: int32(3), object(2)
memory usage: 5.5+ KB
     
df1 = spark.createDataFrame(pdf)
df1.corr('Age', 'Annual Income (k$)')
>>> -0.01239804273606026
df1.corr('Spending Score (1-100)', 'Annual Income (k$)')
>>> 0.009902848094037492
df1.corr('Spending Score (1-100)', 'Age')
>>> -0.32722684603909025
~~~
相关系数的绝对值非常接近于0，因此说Age和收入的相关性几乎为零，几乎没有任何相关性

13. cov(col1,col2)计算两列的协方差，协方差也是用来度量两个变量相关性的一个指标  
$$cov(X,Y) = \frac {\sum_{i=1}^n \left(X_i - \overline{X} \right) \left(Y_i - \overline{Y} \right)} {n - 1} $$

~~~python
df = spark.read.csv('./datas/customers.csv', header=True)
pdf = df.toPandas()
pdf['Age'] = pdf['Age'].astype('int')
pdf['Annual Income (k$)'] = pdf['Annual Income (k$)'].astype('int')
pdf['Spending Score (1-100)'] = pdf['Spending Score (1-100)'].astype('int')
pdf.info()
>>>
<class 'pandas.core.frame.DataFrame'>
RangeIndex: 200 entries, 0 to 199
Data columns (total 5 columns):
CustomerID                200 non-null object
Genre                     200 non-null object
Age                       200 non-null int32
Annual Income (k$)        200 non-null int32
Spending Score (1-100)    200 non-null int32
dtypes: int32(3), object(2)
memory usage: 5.5+ KB

df = spark.createDataFrame(pdf)
df.cov('Age', 'Annual Income (k$)')
>>> -4.548743718593002
~~~

14. createGlobalTempView(name)使用DataFrame创建一个全局的临时表，其生命周期和启动的app的周期一致，即启动的spark应用存在则这个临时的表就能一直访问。直到sparkcontext的stop方法的调用退出应用为止。创建的临时表保存在global_temp这个库中

~~~python
df = spark.read.csv('./datas/customers.csv', header=True)
df.createGlobalTempView('T')
spark.sql('select * from global_temp.T where CustomerID <= 0005').show()
>>>
+----------+------+---+------------------+----------------------+
|CustomerID| Genre|Age|Annual Income (k$)|Spending Score (1-100)|
+----------+------+---+------------------+----------------------+
|      0001|  Male| 19|                15|                    39|
|      0002|  Male| 21|                15|                    81|
|      0003|Female| 20|                16|                     6|
|      0004|Female| 23|                16|                    77|
|      0005|Female| 31|                17|                    40|
+----------+------+---+------------------+----------------------+
~~~

15. createPrReplaceGlobalTempView(name)上面的方法当遇到已经创建了的临时表名的话会报错，而这个方法遇到已经存在的临时表会进行替换，没有则创建

~~~python
df.createOrReplaceGlobalTempView('TT')
spark.sql('select * from global_temp.TT where CustomerID < 0005').show()
>>> 
+----------+------+---+------------------+----------------------+
|CustomerID| Genre|Age|Annual Income (k$)|Spending Score (1-100)|
+----------+------+---+------------------+----------------------+
|      0001|  Male| 19|                15|                    39|
|      0002|  Male| 21|                15|                    81|
|      0003|Female| 20|                16|                     6|
|      0004|Female| 23|                16|                    77|
+----------+------+---+------------------+----------------------+
~~~

16. createOrReplaceTempView(name)使用DataFrame创建本地的临时视图，其生命周期只限于当前的SparkSession，当调用了SparkSession的stop方法停止SparkSession后，其生命周期就到此为止了

~~~python
df.createOrReplaceTempView('TTT')
spark.sql('SELECT * FROM TTT WHERE CUSTOMERID < 0005').show()
>>>
+----------+------+---+------------------+----------------------+
|CustomerID| Genre|Age|Annual Income (k$)|Spending Score (1-100)|
+----------+------+---+------------------+----------------------+
|      0001|  Male| 19|                15|                    39|
|      0002|  Male| 21|                15|                    81|
|      0003|Female| 20|                16|                     6|
|      0004|Female| 23|                16|                    77|
+----------+------+---+------------------+----------------------+
~~~
还有一个孪生的方法叫createTempView(name)，这个方法在创建临时视图时若遇到已经创建过的视图的名字会报错，因此需要指定另外的名字

~~~python
df.createTempView('TTT')
>>> AnalysisException: "Temporary view 'TTT' already exists;"
spark.sql('select * from TTT WHERE CUSTOMERID < 0005').show()
>>> +----------+------+---+------------------+----------------------+
|CustomerID| Genre|Age|Annual Income (k$)|Spending Score (1-100)|
+----------+------+---+------------------+----------------------+
|      0001|  Male| 19|                15|                    39|
|      0002|  Male| 21|                15|                    81|
|      0003|Female| 20|                16|                     6|
|      0004|Female| 23|                16|                    77|
+----------+------+---+------------------+----------------------+
~~~

17. crossJoin(other)返回两个DataFrame的笛卡尔积组合。不要轻易尝试这个方法，非常耗时且费资源，下面是crossJoin的用法

~~~python
df3 = df1.coalesce(3).crossJoin(df.coalesce(3))
df3.show()
>>> 
+----------+-----+---+------------------+----------------------+----------+------+---+------------------+----------------------+
|CustomerID|Genre|Age|Annual Income (k$)|Spending Score (1-100)|CustomerID| Genre|Age|Annual Income (k$)|Spending Score (1-100)|
+----------+-----+---+------------------+----------------------+----------+------+---+------------------+----------------------+
|      0001| Male| 19|                15|                    39|      0001|  Male| 19|                15|                    39|
|      0001| Male| 19|                15|                    39|      0002|  Male| 21|                15|                    81|
........
~~~

18. cube(*col)在当前的DataFrame上创建多维的数据立方体

~~~python
from pyspark.sql import SparkSession
from pyspark.sql.functions import *


spark = SparkSession.builder.master('local')\
                    .appName('test')\
                    .config('spark','value')\
                    .getOrCreate()
df = spark.read.csv('./datas/customers.csv', header=True)
df.cube('Age', 'Genre').count().orderBy(desc('count'), asc('Age')).show()
>>> 
+----+------+-----+
| Age| Genre|count|
+----+------+-----+
|null|  null|  200|
|null|Female|  112|
|null|  Male|   88|
|  32|  null|   11|
|  35|  null|    9|
|  19|  null|    8|
|  31|  null|    8|
|  30|  null|    7|
|  31|Female|    7|
|  49|  null|    7|
|  19|  Male|    6|
|  23|Female|    6|
|  23|  null|    6|
|  27|  null|    6|
|  32|Female|    6|
|  35|Female|    6|
|  36|  null|    6|
|  38|  null|    6|
|  40|  null|    6|
|  47|  null|    6|
+----+------+-----+
only showing top 20 rows
~~~

19. describe(*col)统计对应的基本的统计信息，包括数量，最大值，最小值，均值及标准差

~~~python
df = spark.read.csv('./datas/customers.csv', header=True)
df.describe('Age')
>>> DataFrame[summary: string, Age: string]
df.describe('Age').show()
>>> 
+-------+-----------------+
|summary|              Age|
+-------+-----------------+
|  count|              200|
|   mean|            38.85|
| stddev|13.96900733155888|
|    min|               18|
|    max|               70|
+-------+-----------------+

df.describe('Age', 'Genre').show()
>>> 
+-------+-----------------+------+
|summary|              Age| Genre|
+-------+-----------------+------+
|  count|              200|   200|
|   mean|            38.85|  null|
| stddev|13.96900733155888|  null|
|    min|               18|Female|
|    max|               70|  Male|
+-------+-----------------+------+

# 不指定参数回默认计算所有列的描述统计信息
df.describe().show()
>>> 
+-------+------------------+------+-----------------+------------------+----------------------+
|summary|        CustomerID| Genre|              Age|Annual Income (k$)|Spending Score (1-100)|
+-------+------------------+------+-----------------+------------------+----------------------+
|  count|               200|   200|              200|               200|                   200|
|   mean|             100.5|  null|            38.85|             60.56|                  50.2|
| stddev|57.879184513951124|  null|13.96900733155888| 26.26472116527124|    25.823521668370173|
|    min|              0001|Female|               18|               101|                     1|
|    max|              0200|  Male|               70|                99|                    99|
+-------+------------------+------+-----------------+------------------+----------------------+
~~~

20. distinct()返回DataFrame中非重复的数据

~~~python
df = spark.createDataFrame([(1, 1), (1, 2), (1, 2), (5, 5)])
df.count()
>>> 4
df.distinct().count()
>>> 3
~~~

21. drop(*col)按照列名删除DataFrame中的列，返回新的DataFrame

~~~python
df = spark.read.csv('./datas/customers.csv', header=True)
df.columns
>>> ['CustomerID', 'Genre', 'Age', 'Annual Income (k$)', 'Spending Score (1-100)']
df1 = df.drop('Age')
df1.columns
>>> ['CustomerID', 'Genre', 'Annual Income (k$)', 'Spending Score (1-100)']
df2 = df1.drop('Genre')
df2.columns
>>> ['CustomerID', 'Annual Income (k$)', 'Spending Score (1-100)']
~~~

22. dropDuplicates(subset=None)删除重复行，subset用于指定在删除重复行的时候参考哪几列

~~~python
from pyspark.sql import Row
sc = spark.sparkContext

df = sc.parallelize([
    Row(name='tom', age=27, height=170),
    Row(name='tom', age=27, height=170),
    Row(name='tom', age=27, height=153)
], 3).toDF()
df.show()
>>> 
+---+------+----+
|age|height|name|
+---+------+----+
| 27|   170| tom|
| 27|   170| tom|
| 27|   153| tom|
+---+------+----+
df.dropDuplicates().show()
>>> 
+---+------+----+
|age|height|name|
+---+------+----+
| 27|   170| tom|
| 27|   153| tom|
+---+------+----+
df.dropDuplicates(subset=['age', 'name']).show()
>>> 
+---+------+----+
|age|height|name|
+---+------+----+
| 27|   170| tom|
+---+------+----+
# 使用distinct也可以完成相同的去重工作
df.distinct().show()
>>> 
+---+------+----+
|age|height|name|
+---+------+----+
| 27|   170| tom|
| 27|   153| tom|
+---+------+----+
~~~

23.  df.dropna(how='any', thresh=None, subset=None)删除DataFrame中的na数据，关键字参数how指定如何删除，有“any”和“all”两种选项，thresh指定行中na数据有多少个时删除行数据，这个设置将覆盖how关键字参数的设置，subset指定在哪几列中删除na数据

~~~python
from pyspark.sql import SparkSession
import numpy as np


spark = SparkSession.builder.master('local').appName('spark').getOrCreate()
sc = spark.sparkContext

df = spark.createDataFrame([(np.nan, 27., 170.), (44., 27., 170.),
                            (np.nan, np.nan, 170.)],
                           schema=['luck', 'age', 'weight'])
df.show()
>>> 
+----+----+------+
|luck| age|weight|
+----+----+------+
| NaN|27.0| 170.0|
|44.0|27.0| 170.0|
| NaN| NaN| 170.0|
+----+----+------+
df.dropna(how='any').show()
>>>
+----+----+------+
|luck| age|weight|
+----+----+------+
|44.0|27.0| 170.0|
+----+----+------+
df.dropna(how='all').show()
>>> 
+----+----+------+
|luck| age|weight|
+----+----+------+
| NaN|27.0| 170.0|
|44.0|27.0| 170.0|
| NaN| NaN| 170.0|
+----+----+------+
df.dropna(thresh=2).show()
>>> 
+----+----+------+
|luck| age|weight|
+----+----+------+
| NaN|27.0| 170.0|
|44.0|27.0| 170.0|
+----+----+------+
~~~

24. dtypes返回DataFrame列的名字及对应的数据类型组成tuple列表

~~~python
df = spark.createDataFrame([(np.nan, 27., 170.), (44., 27., 170.),
                            (np.nan, np.nan, 170.)],
                           schema=['luck', 'age', 'weight'])
df.dtypes
>>> [('luck', 'double'), ('age', 'double'), ('weight', 'double')]
~~~

25. explain(extended=False)打开逻辑执行计划和物理执行计划执行中的中间过程，用于调试。特别是在调试一些特别复杂的SQL语句的时候特别有用

~~~python
df.explain(extended=True)
>>>
== Parsed Logical Plan ==
LogicalRDD [luck#62, age#63, weight#64], false

== Analyzed Logical Plan ==
luck: double, age: double, weight: double
LogicalRDD [luck#62, age#63, weight#64], false

== Optimized Logical Plan ==
LogicalRDD [luck#62, age#63, weight#64], false

== Physical Plan ==
Scan ExistingRDD[luck#62,age#63,weight#64]

df.show()
>>>
+----+----+------+
|luck| age|weight|
+----+----+------+
| NaN|27.0| 170.0|
|44.0|27.0| 170.0|
| NaN| NaN| 170.0|
+----+----+------+                 
~~~

26. fillna(value, subset=None)用于DataFrame中空数据的填充

~~~python
df.fillna(0.0).show()
>>>
+----+----+------+
|luck| age|weight|
+----+----+------+
| 0.0|27.0| 170.0|
|44.0|27.0| 170.0|
| 0.0| 0.0| 170.0|
+----+----+------+
df.na.fill(0.0).show()
>>>
+----+----+------+
|luck| age|weight|
+----+----+------+
| 0.0|27.0| 170.0|
|44.0|27.0| 170.0|
| 0.0| 0.0| 170.0|
+----+----+------+

# 也可以给fill方法传递False参数，从而不填充
df.na.fill(False).show()
>>>
+----+----+------+
|luck| age|weight|
+----+----+------+
| NaN|27.0| 170.0|
|44.0|27.0| 170.0|
| NaN| NaN| 170.0|
+----+----+------+

# 或者给fill传入一个字典，指定每个列的填充形式
df.na.fill({'luck': 0.0, 'age': 50, 'weight': 150}).show()
>>> 
+----+----+------+
|luck| age|weight|
+----+----+------+
| 0.0|27.0| 170.0|
|44.0|27.0| 170.0|
| 0.0|50.0| 170.0|
+----+----+------+
~~~

27. filter(conditon)按照传入的条件进行过滤，其实where方法就是filter方法的一个别名而已

~~~python
df = spark.createDataFrame([(np.nan, 27., 170.), (44., 27., 170.),
                            (np.nan, np.nan, 170.)],
                           schema=['luck', 'age', 'weight'])
df.filter(df.luck != np.nan).show()
>>>
+----+----+------+
|luck| age|weight|
+----+----+------+
|44.0|27.0| 170.0|
+----+----+------+
~~~
除了通过指定布尔表达式的方式之外，还有一种方法就是直接把条件以字符串的方式传入

~~~python
df.filter('luck <> "NaN"').show()
>>> 
+----+----+------+
|luck| age|weight|
+----+----+------+
|44.0|27.0| 170.0|
+----+----+------+
~~~

28. first()返回DataFrame的第一条记录

~~~python
df.first()
>>> Row(luck=nan, age=27.0, weight=170.0)
~~~

29. foreach(f),在每一个Row上运用f方法，实际上它调用的是df.rdd.foreach这个基于RDD上的foreach方法

~~~python
df = spark.createDataFrame([(np.nan, 27., 170.), (44., 27., 170.),
                            (np.nan, np.nan, 170.)],
                           schema=['luck', 'age', 'weitht'])


def myprint(x):
    print(x.age)

df.foreach(lambda x: print(x))    
~~~
还有一个foreachPartition方法，是在整个分区上条用传入的f方法，效率比foreach方法更加高效，因为foreach方法是在每个Row上进行调用

~~~python
def pprint(x):
    for p in x:
        print(p.age)
        
df.foreachPartition(pprint)
~~~

30. groupBy(*cols)使用给定的列进行分组，返回GroupedData对象，该对象上提供了几乎所有的对数据 进行聚合的方法groupby整个方法其实是groupBy这个方法的一个别名

~~~python
df = spark.read.csv('./datas/customers.csv', header=True)
df.columns
>>> ['CustomerID', 'Genre', 'Age', 'Annual Income (k$)', 'Spending Score (1-100)']
df.groupby('Genre').agg({'Age':'mean'}).show()
>>> 
+------+------------------+
| Genre|          avg(Age)|
+------+------------------+
|Female|38.098214285714285|
|  Male| 39.80681818181818|
+------+------------------+
~~~
GroupedData对象上方法，可以使用dir查看

~~~python
dir(df.groupBy('Genre'))
>>>
['__class__','__delattr__','__dict__','__dir__','__doc__','__eq__','__format__''__ge__','__getattribute__','__gt__','__hash__','__init__','__init_subclass__','__le__','__lt__','__module__','__ne__','__new__','__reduce__','__reduce_ex__','__repr__','__setattr__','__sizeof__','__str__','__subclasshook__','__weakref__','_df','_jgd','agg','apply','avg','count','max','mean','min','pivot','sql_ctx','sum']
~~~
