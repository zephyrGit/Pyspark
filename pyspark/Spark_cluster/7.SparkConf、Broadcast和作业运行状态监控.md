
## SparkConf对象详解

- 1.pyspark.SparkConf(loadDefaults=True,_jvm=None,_iconf=None)该类用于配置spark中的参数，配置以键值对形式进行

- 2.contains(key)判断配置中是否包含key的配置

~~~python
conf.contains('memory')
>>> False
~~~

- 3.get(key,defaultValue=None)从Sparkconf对象中取出key对应的配置，若没有则返回defaultValue

~~~python
conf.get('executor.memory',"2g")
>>> '2g'
~~~

- 4.getAll()得到SparkConf对象中所有的配置，以列表的形式返回

~~~python
conf.getAll()
>>> dict_items([('spark.master', 'local'), ('spark.app.name', 'test')])
~~~
上面的SparkConf虽然没有调用set方法进行设置，但是SparkConf会读取spark-defaults.conf和spark-env.sh中的配置，spark-defaults.conf中的配置

~~~python
spark.executor.memory=2G
spark.driver.memory=2G
spark.executor.cores=2
#spark.sql.codegen.wholeStage=false#spark.memory.offHeap.enabled=true#spark.memory.offHeap.size=4G
#spark.memory.fraction=0.9#spark.memory.storageFraction=0.01#spark.kryoserializer.buffer.max=64m
#spark.shuffle.manager=sort
#spark.sql.shuffle.partitions=600spark.speculation=truespark.speculation.interval=5000spark.speculation.quantile=0.9spark.speculation.multiplier=2spark.default.parallelism=1000spark.driver.maxResultSize=1g
#spark.rdd.compress=falsespark.task.maxFailures=8spark.network.timeout=300spark.yarn.max.executor.failures=200spark.shuffle.service.enabled=truespark.dynamicAllocation.enabled=truespark.dynamicAllocation.minExecutors=4spark.dynamicAllocation.maxExecutors=8spark.dynamicAllocation.executorIdleTimeout=60#spark.serializer=org.apache.spark.serializer.JavaSerializer
#spark.sql.adaptive.enabled=true#spark.sql.adaptive.shuffle.targetPostShuffleInputSize=100000000#spark.sql.adaptive.minNumPostShufflePartitions=1##for spark2.0#spark.sql.hive.verifyPartitionPath=true#spark.sql.warehouse.dir
spark.sql.warehouse.dir=/spark/warehouse
~~~

- 5.set(key, value)设置属性

~~~python
conf.set('test', 'just a test')
conf.get('test')
>>> 'just a test'
~~~

- 6.setAll(pairs)一次性设置多个属性，多个属性以tuple表的形式传入

~~~python
conf.setAll([('test1',1),('test2',2),('test3',3)])
conf.getAll()
>>> dict_items([('spark.master', 'local'), ('spark.app.name', 'test'), ('test', 'just a test'), ('test1', '1'), ('test2', '2'), ('test3', '3')])
~~~

- 7.setAppName(value)设置应用的名称  
编写appname.py文件，使用spark-submit运行
~~~python
from pyspark improt SparkConf,SparkContext
conf = SparkConf()
conf.setAppName('Spark')
print(sc.appName)
sc.stop()
>>> 'Spark'
~~~

- 8.setExecutorEnv(key=None,value=None,pairs=None)设置参数，该参数将传递给Executor

~~~python
conf.setExecutorEnv("test", "study spark")
>>> <pyspark.conf.SparkConf at 0x1927e8c64e0>
~~~

- 9.setIfMissing(key, value)若没有这个值，则用value创建这个配置

- 10.setMaster(value)设置master地址

~~~python
conf.setMaster("spark://hadoop-maste:7077")
~~~

- 11.setSparkHome(value)设置spark的安装路径

~~~python
conf.setSparkHone("/usr/local/spark-2.3.2-bin-hadoop2.7")
~~~

- 12.toDebugString()将所有的配置打印出来

~~~python
conf.setAll([('test1',1),('test2',2),('test3',3)])
conf.toDebugString()
>>> 'spark.master=local\nspark.app.name=Spark\ntest=just a test\ntest1=1\ntest2=2\ntest3=3\nspark.executorEnv.test=study spark'
~~~

## Broadcast广播变量

- 1.pyspark.Broadcast(sc=None,value=None,pickle_registry=None,path=None)可以通过sparkcontext上的broadcast来创建

- 2.通常建议共享变量大于5M就使用广播机制，因为这会大大减少网络传输的数据量

- 3.value，在广播变量上调用value即可以获取广播变量的值

~~~python
data = [1,2,3,4,5]
broad_data = sc.broadcast(data)
rdd = sc.parallelize(range(10), 3).flatMap(lambda x: broad_data.value).glom()
rdd.collect()
>>> [[1, 2, 3, 4, 5, 1, 2, 3, 4, 5, 1, 2, 3, 4, 5],
     [1, 2, 3, 4, 5, 1, 2, 3, 4, 5, 1, 2, 3, 4, 5],
     [1, 2, 3, 4, 5, 1, 2, 3, 4, 5, 1, 2, 3, 4, 5, 1, 2, 3, 4, 5]]
~~~

- 4.destroy()该方法用于释放广播变量，该方法会阻塞直到所有广播变量释放完成为止，该方法调用之后，广播变量就不能被使用了，因为存储广播变量的的文件已经被删除

~~~python
broad_data.destroy()
broad_data.value
>>> ' [Errno 2] No such file or directory'
~~~

- 5.unpersist(blocking=False)该方法用于释放excutor上存储的广播变量，用于释放内存资源。参数blocking默认为False，表示在完成释放之前是否阻塞

~~~python
data = [5,4,3,2,1]
broad_data = sc.broadcast(data)
rdd = sc.parallelize(range(10), 3).flatMap(lambda x: broad_data.value).glom()
rdd.collect()
>>> [[5, 4, 3, 2, 1, 5, 4, 3, 2, 1, 5, 4, 3, 2, 1],
     [5, 4, 3, 2, 1, 5, 4, 3, 2, 1, 5, 4, 3, 2, 1],
     [5, 4, 3, 2, 1, 5, 4, 3, 2, 1, 5, 4, 3, 2, 1, 5, 4, 3, 2, 1]]
broad_data.unpersist()
broad_data.value
>>> [5, 4, 3, 2, 1]
~~~
**该方法是释放excutor上的broadcast变量，而driver节点上的广播变量不会被释放，会一直持续到application的结束**

## StatusTracker Spark作业运行状态监控

- 1.pyspark.StatusTracker(tracker)这个类是一个低层的API，用于监控job和stage的运行情况，可以通过类中的方法，获取这些信息，用于作业的监控

- 2.通过sparkcontext上的statusTracker()方法获取StatusTracker对象

~~~python
status = sc.statusTracker()
type(status)
>>> pyspark.status.StatusTracker
~~~

- 3.getActiveJobsIds()该方法用于获得活动的job的id，返回一个job id的列表

- 4.getActiveStageIds()该方法用于获得活动的job的stage的id，返回一个stage id列表

- 5.getJobIdsForGroup(jobsGroup=None)该方法返回jobGroup作业组对应的job id列表，当jobGroup没有指定时，返回所有的zuoye组的job id

- 6.getJobInfo(jobid)通过给定的一个jobid，返回该作业对应的job详细信息。该方法封装在SparkJobInfo对象中，若没有相关信息，返回None

- 7.getStageInfo(stageid)由stageid，返回stage信息，这些信息存储在SparkStageInfo对象中，若没有则返回None

- 8.pyspark.SparkStageInfo的方法和属性

~~~python
from pyspark import SparkStageInfo
dir(SparkStageInfo)
>>> ['__add__','__class__','__contains__','__delattr__','__dict__','__dir__','__doc__','__eq__','__format__','__ge__','__getattribute__','__getitem__','__getnewargs__','__gt__','__hash__','__init__','__init_subclass__','__iter__','__le__','__len__','__lt__','__module__','__mul__','__ne__','__new__','__reduce__','__reduce_ex__','__repr__','__rmul__','__setattr__','__sizeof__','__slots__','__str__','__subclasshook__','_asdict','_fields','_is_namedtuple_','_make','_replace','_source','count','currentAttemptId','index','name','numActiveTasks','numCompletedTasks','numFailedTasks','numTasks','stageId']
~~~

- 9.pyspark.SparkJobInfo对应的属性和方法

~~~python
from pyspark import SparkJobInfo
dir(SparkJobInfo)
>>> ['__add__','__class__','__contains__','__delattr__','__dict__','__dir__','__doc__','__eq__','__format__','__ge__','__getattribute__','__getitem__','__getnewargs__','__gt__','__hash__','__init__','__init_subclass__','__iter__','__le__','__len__','__lt__','__module__','__mul__','__ne__','__new__','__reduce__','__reduce_ex__','__repr__','__rmul__','__setattr__','__sizeof__','__slots__','__str__','__subclasshook__','_asdict','_fields','_is_namedtuple_','_make','_replace','_source','count','index','jobId','stageIds','status']
~~~

### Spark中自定义时间分析器，用于程序的调试

1. pyspark.Profiler(ctx),这个类是Spark中事件探查器的基类，自定义探查器需要集成这个类

2. Profiler(ctx)这个类属性和方法
3. 