
1. GroupedData对象是由DataFrame上面的groupBy()方法创建，它提供了在DataFrame上用于数据聚合的系列方法

2. agg(*exprs) 这个方法是GroupedData上用于聚合数据的方法，*exprs可以是一个string到string组成的字典，key是需要聚合的列名，value是用于计算的聚合函数的名称，聚合函数可以的取值由： avg, max, min, sum, count, mean, agg方法返回一个聚合后的DataFrame对象

~~~python
df = spark.read.csv('./datas/customers.csv', header=True)
df.columns
>>> ['CustomerID', 'Genre', 'Age', 'Annual Income (k$)', 'Spending Score (1-100)']

df.groupBy('Genre').agg({
    'Age': 'mean',
    'Annual Income (k$)': 'max',
    'Spending Score (1-100)': 'count'
}).show()
>>> 
     +------+-----------------------------+-----------------------+------------------+
| Genre|count(Spending Score (1-100))|max(Annual Income (k$))|          avg(Age)|
+------+-----------------------------+-----------------------+------------------+
|Female|                          112|                     99|38.098214285714285|
|  Male|                           88|                     99| 39.80681818181818|
+------+-----------------------------+-----------------------+------------------+
~~~
除了使用字符串以字典的方式指定，其实还可以使用聚合的列的表达式来做相同的是，这需要借助pyspark.sql.functions模块中的方法

~~~python
from pyspark.sql.functions import *

df.columns
>>> ['CustomerID', 'Genre', 'Age', 'Annual Income (k$)', 'Spending Score (1-100)']

df.groupBy('Genre').agg(mean(df.Age)).show()
>>> 
+------+------------------+
| Genre|          avg(Age)|
+------+------------------+
|Female|38.098214285714285|
|  Male| 39.80681818181818|
+------+------------------+                                          
~~~

3. apply(udf) 使用pandas中的自定义函数作用在GroupedData的每一组数据之上，返回结果作为DataFrame。udf用户自定义函数接收pandas.DataFrame作为阐述，返回另一个pandas.DataFrame对象。<font color=red>这个方法是pyspark2.3中新加入的方法，通过 @pandas_udf表示这是一个pandas的方法，参数为id long，v double，指定PandasUDFType为分组map操作</font>  
为了方便，这里新建了一个apply.py文件，文件内容如下：

~~~python
from pyspark.sql.functions import pandas_udf, PandasUDFType
from pyspark.sql import SparkSession

spark = SparkSession.builder.master('local').appName('apply').getOrCreate()

df = spark.createDataFrame([(1, 10.0), (1, 21.0), (2, 34.0), (2, 56.0),
                            (2, 19.0)], ('id', 'v'))


@pandas_udf("id long, v double", PandasUDFType.GROUPED_MAP)
def normalize(pdf):
    v = pdf.v
    print(type(v), type(pdf))
    return pdf.assign(v=(v - v.mean()) / v.std())
df.groupby('id').apply(normalize).show()
spark.stop()
>>> 
+---+-------------------+
| id|                  v|
+---+-------------------+
|  1|-0.7071067811865476|
|  1| 0.7071067811865476|
|  2| -0.125380396490799|
|  2| 1.0567776275653045|
|  2|-0.9313972310745059|
+---+-------------------+
~~~
4. avg(*cols) 计算给定的数值类型的列的平均值

~~~python
df = spark.read.csv('./datas/customers.csv', header=True)
df.select(df.Age.cast('int').alias('age'),
          'Genre').groupBy('Genre').avg('age').show()
>>> 
+------+------------------+
| Genre|          avg(age)|
+------+------------------+
|Female|38.098214285714285|
|  Male| 39.80681818181818|
+------+------------------+
~~~

5. count() 返回每个分组中的数据的条数

~~~python
df.select(df.Age.cast('int').alias('age'),
          'Genre').groupBy('Genre').count().show()
>>>
+------+-----+
| Genre|count|
+------+-----+
|Female|  112|
|  Male|   88|
+------+-----+
~~~

6. max(*cols) 计算给定列中数值最大的值

~~~python
df.select(df.Age.cast('int').alias('age'),'Genre').groupBy('Genre').max().show()
>>> 
+------+--------+
| Genre|max(age)|
+------+--------+
|Female|      68|
|  Male|      70|
+------+--------+
~~~

7. mean(*col) 计算对应列的均值，列需要时数值类型

~~~python
df.select(df.Age.cast('int').alias('age'),'Genre').groupBy('Genre').mean().show()
>>> 
+------+------------------+
| Genre|          avg(age)|
+------+------------------+
|Female|38.098214285714285|
|  Male| 39.80681818181818|
+------+------------------+
~~~

8. min(*col) 计算对应列的最小值，列数值雷松需要数值类型

~~~python
df.select(df.Age.cast('int').alias('age'),'Genre').groupBy('Genre').min().show()
>>> 
+------+--------+
| Genre|min(age)|
+------+--------+
|Female|      18|
|  Male|      18|
+------+--------+
~~~

9. sum(*col) 计算指定列的和，列的类型需要时数值类型

~~~python
df.select(df.Age.cast('int').alias('age'), 'Genre',
          'Annual Income (k$)').groupBy('Genre').sum().show()
>>> 
+------+--------+
| Genre|sum(age)|
+------+--------+
|Female|    4267|
|  Male|    3503|
+------+--------+
~~~
