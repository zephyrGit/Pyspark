
1. 回归算法和分类算法的不同点是回归算法输出的是连续的值，分类算法的输出值为离散类别值，回归是对真实分布的逼近预测，而分类算法给事务对上一个标签。

2. 回归算法中常见的就是Linear regression线性回归了  
例如在贷款额度评测中，根据工资和年龄去判断工资和年龄去判断可以贷多少款  
$$h_\theta(x) = \theta_0 + \theta_1x_1 + \theta_2x_2$$
$$h_\theta(x) = \Sigma^n_{i=1} \theta_ix_i = \theta^T x$$  
能贷多少款是一个连续的值，可以使用工资和年龄作为线性方程的两个变量，通过大量的历史数据拟合出一条线性的模型，并通过这个模型在只有工资和年龄的数据计算预测出能放多少款  
上面的线性方程使用向量的表达式可以写成如下形式：  
$$y^{(i)} = \theta^T x^{i} + \epsilon ^{(i)}$$
其中
$$\epsilon^{(i)}$$
是独立并且具有相同分布的服从均值为0，方差为西格玛平方的偏置项 
由西格玛服从标准的高斯分布的假设，因此有  
$$y^{(i)} - \theta^Tx^{(i)}$$ 
也是服从标准的高斯分布的，可得到如下的等式：  
$$p(\epsilon^{(i)}) = \frac {1}{\sqrt{(2 \pi \delta)}} exp (- \frac {\epsilon^{(i)^2}}{2\delta^2})$$

$$p(y^{(i)|x^{(i);\theta}})=\frac{1}{\sqrt(2\pi\delta)} 
exp (- \frac{(y^{(i)}\theta^Tx^{(i)})^2}{2\delta^2})$$
整个训练集中，所有样本都服从正态分布，注意上面的式子不就是最大似然估计吗？最大似然估计原理：最合理的参数估计量应该是使得从模型中抽取该n组样本观测值的概率最大。我们想让这个模型在整个数据集上预测最准，因此得到下面的损失函数：  
$$L(\theta) = \prod^m_{(i=1)}p(y^{(i)}|x^{(i)};\theta)$$  
$$=\prod^m_{(i=1)}\frac{1}{\sqrt(2\pi\delta)}exp(-\frac{(y^{(i)}-\theta^Tx^{(i)})^2}{2\delta^2})$$  
上式即为假设整体偏置项服从标准正态分布的目标函数，现在需要使得这个函数的值最大，并且为了求解在等式的两边取对数，将乘积操作转换为求和操作。这就把问题转换成了最大似然估计了  
$$l(\theta) = logL(\theta)$$  
$$= log\prod^m_{(I=1)}\frac{1}{\sqrt(2\pi\delta)}exp(-\frac{(y^{(i)}-\theta^Tx^{(i)})^2}{2\delta^2})$$
$$=\Sigma^m_{(I=1)}log\frac{1}{\sqrt(2\pi\delta)}exp(-\frac{(y^{(i)}-\theta^Tx^{(i)})^2}{2\delta^2})$$
$$=m log\frac{1}{\sqrt(2\pi\delta)}-\frac{1}{\delta^2}·\frac{1}{2}\Sigma^m_{(i=1)}(y^{(i)}-\theta^Tx^{(i)})^2$$
要使上式最大，需要使得 
$$\frac{1}{\delta^2}·\frac{1}{2}\Sigma^m_{(i=1)}(y^{(i)}-\theta^Tx^{(i)})^2$$
最小，因此最终的目标函数为j(0)求最小值：  
$$J(\theta)=\frac{1}{2}\Sigma^m_{(i=1)}(h_\theta(x^{()}-y^{(i)})^2$$
现在对上面的目标函数用向量来表示：  
$$J(\theta)=\frac{1}{2}\Sigma^m_{(i=1)}(h_\theta(x^{()}-y^{(i)})^2=\frac{1}{2}(X\theta-y)^T(X\theta-y)$$
怎么求最小值？因为上式为一个凸函数，求最小值一个很有用的方式就是导数等于0，用损失函数对θ求偏导 
$$\triangledown_\theta J(\theta)=\triangledown_\theta(\frac{1}{2}(X\theta-y)^T(X\theta-y))=\triangledown_\theta(\frac{1}{2}(\theta^TX^T-y^T)(X\theta-y))$$
$$=\triangledown_\theta(\frac{1}{2}(\theta^TX^TX\theta-\theta^TX^Ty-y^TX\theta+y^Ty))$$
$$=\frac{1}{2}(2X^TX\theta-X^Ty-(y^TX)^T)=X^TX\theta-X^Ty$$
$$\theta=(X^TX)^{-1}X^Ty$$
最终求得线性方程的权重参数为：  
$$\theta = (X^TX)^{-1}X^Ty$$
那么上面的公式呢？有什么问题呢，就是X的转置乘X再求逆的。需要时满秩矩阵才能求矩阵的逆，而且现实中大多数的数据的矩阵都不是满秩矩阵，因此上面推导的这种求线性方程的方法不是很常用。另外一种比较常用的方式是通过梯度下降的方式来求解  

3. 梯度下降法求解线性方程系数  
在选定线性回归模型后，只需要确定参数在选定线性回归模型后，只需要确定参数θ，就可以将莫i选哪个用来预测。然而，就可以将模型用来预测。然后就可以将模型用来预测。然而θ需要在J(θ)最小的情况下才能确定。因此问题归结为最小的情况下才能确定。因此问题归结为求极小值问题，使用梯度下降法。求极小值问题，使用梯度下降法。梯度下降法最大的问题是求得有可能全局极小值，这与初始点选取有关。  
梯度下降法是按面的流程进行：  
1）首先对θ赋值，这个可以是随机的也让θ是一个全零向量  
2）改变θ的值，使得J(θ)按梯度下降的方向进行减少  
目标函数如下：  
$$J(θ)=\frac{1}{2}\Sigma^m_{(i=1)}(h_\theta(x^{(i)})-y^{(i)})^2$$  
梯度方向由J(θ)对θ的偏导数确定，由于求的是极小值，因此梯度方向是偏导数的反方向
$$\Theta_j:=\Theta_j + \alpha(y^{(i)}-h_\theta(x^{(i)}))x^{(i)}_j$$  
迭代更新的方式有两种，一种是批梯度下降也就是对全部训练数据求得误差后再对θ进行更新，另外一种是增量梯度下降，每扫描一步都要对θ进行更新。前一种方法能够不断收敛，后一种方法结果可能不断在收敛处徘徊。  

4. pyspark中线性回归的使用

~~~python

from pyspark.sql import SparkSession
from pyspark.ml.regression import LinearRegression

spark = SparkSession.builder.master('local').appName('lr').getOrCreate()

df = spark.read.csv('/data/mllib/house_data.csv', header=True)
datas = df.select(df.price.cast('double'), df.sqft_living.cast('double'),
                  df.bathrooms.cast('double'), df.bedrooms.cast('double'),
                  df.floors.cast('double'))

# 使用sqft_living房屋面积，bedrooms卧室个数，bathrooms浴室个数和floors作为特征向量
from pyspark.ml.linalg import Vectors
from pyspark.ml.feature import VectorAssembler

assembler = VectorAssembler(inputCols=['sqft_living', 'bedrooms', 'floors'],
                            outputCol='features')
output = assembler.transform(datas)
label_features = output.select('features', 'price').toDF('features', 'label')
label_features.show(truncate=False)
>>> 
+----------------+---------+
|features        |label    |
+----------------+---------+
|[1180.0,3.0,1.0]|221900.0 |
|[2570.0,3.0,2.0]|538000.0 |
|[770.0,2.0,1.0] |180000.0 |
|[1960.0,4.0,1.0]|604000.0 |
|[1680.0,3.0,1.0]|510000.0 |
|[5420.0,4.0,1.0]|1230000.0|
|[1715.0,3.0,2.0]|257500.0 |
|[1060.0,3.0,1.0]|291850.0 |
|[1780.0,3.0,1.0]|229500.0 |
|[1890.0,3.0,2.0]|323000.0 |
|[3560.0,3.0,1.0]|662500.0 |
|[1160.0,2.0,1.0]|468000.0 |
|[1430.0,3.0,1.5]|310000.0 |
|[1370.0,3.0,1.0]|400000.0 |
|[1810.0,5.0,1.5]|530000.0 |
|[2950.0,4.0,2.0]|650000.0 |
|[1890.0,3.0,2.0]|395000.0 |
|[1600.0,4.0,1.5]|485000.0 |
|[1200.0,2.0,1.0]|189000.0 |
|[1250.0,3.0,1.0]|230000.0 |
+----------------+---------+
only showing top 20 rows

# 接下来使用label_features数据训练模型
lr = LinearRegression(maxIter=10, regParam=0.3, elasticNetParam=0.8)
lrModel = lr.fit(label_features)

print('Coefficients: %s' % str(lrModel.coefficients))
print('Intercept: %s' % str(lrModel.intercept))
>>> 
Coefficients: [313.438107054,-57031.0009004,3228.14662708]
Intercept: 75680.94574157595

trainingSummary = lrModel.summary
print('numIterations: %d' % trainingSummary.totalIterations)
print('objectiveHistory: %s' % trainingSummary.objectiveHistory)
trainingSummary.residuals.show()
print('RMSE: %f' % trainingSummary.rootMeanSquaredError)
print('r2: %f' % trainingSummary.r2)
>>> 
numIterations: 11
objectiveHistory: [0.5, 0.46159310764370204, 0.3270622510998533, 0.28115393909518527, 0.2632980067240763, 0.25506912298665496, 0.2502441435862773, 0.24747962051205039, 0.24658090890268994, 0.24658074871903235, 0.24658072831531183]
+-------------------+
|          residuals|
+-------------------+
| -55773.05599079479|
|-178580.17142241867|
|-26194.432999187236|
|  138876.2214077544|
|  75607.89048239001|
| -319619.6289978069|
| -191090.5898915648|
|  51789.51685564086|
|-236235.92022297293|
|-180442.25862595008|
|-361155.75077843503|
|  139564.7052498969|
| -47646.65606774128|
| 62773.703669015435|
| 167308.86505264108|
|-128655.65120241744|
|-108442.25862595008|
|  131099.8666335225|
|-151972.81903224834|
| -69613.72348454891|
+-------------------+
only showing top 20 rows

RMSE: 257975.485494
r2: 0.506840
~~~
上面的r2是拟合优度指标，越接近1表示拟合的越好。可以看到r2指标约为0.5，那r2指标能够达到0.5拟合得一般，如果要进一步拟合优度除了调整模型参数外，最重要得就是特征工程选择及特征工程了  

**下面是r2指标计算的一些说明，来自百度百科**
当R2越接近1时，表示相关的方程式参考价值越高；相反，越接近0时，表示参考价值越低。
这是在一元回归分析中的情况。但从本质上说决定系数和回归系数没有关系，
就像标准差和标准误差在本质上没有关系一样。
在多元回归分析中，决定系数是通径系数的平方。
表达式：R2=SSR/SST=1-SSE/SST
其中：SST=SSR+SSE，
SST (total sum of squares)为总平方和，
SSR (regression sum of squares)为回归平方和，
SSE (error sum of squares) 为残差平方和。
注：（不同书命名不同）
回归平方和：SSR(Sum of Squares for regression) = ESS (explained sum of squares)
残差平方和：SSE（Sum of Squares for Error） = RSS (residual sum of squares)
总离差平方和：SST(Sum of Squares for total) = TSS(total sum of squares)
SSE+SSR=SST RSS+ESS=TSS
意义：拟合优度越大，自变量对因变量的解释程度越高，自变量引起的变动占总变动的百分比高。
观察点在回归直线附近越密集。
