
1. 对于现行可分的数据集，有很多划分超平面的方法将不同类别的数据区分开。当然对于线性不可分的数据集，可以通过核函数把低维度的数据转换到更高的维度，这样就可以用超平面来划分了。  

2. 实际上对于线性可分的数据集,超平面有很多不同的方法，但是哪种更好呢?可以看到支持向量到超平面的距离都很小，而对于这种情况很可能会送到异常数据的影响而影响分类的精度。SVM算法的思想就是找到离分离超平面最近的点使得这些最近的点到分离超平面的距离最远的参数W和b  

数据集(X1,Y1)(X2,Y2)到(Xn,Yn): $$y(x)=w^T \Phi(x)+b$$
Y为样本的类别：  
当X为正例时候Y = +1   $$y(x_i)>0 \Longleftrightarrow y_i = +1$$
当X为负例时候Y = -1   $$y(x_i)<0 \Longleftrightarrow y_i = -1$$
可推出  $$y_i · y(x_i) > 0$$

找到一个条线(w和b)，使得离该线最近的点能够最远argmax(w,b)使得min(最近的点到该线的距离)  
$$\frac {y_i · (w^T · \Phi(x_i) + b)}{||w||}$$

这个地方φ(xi)表示的核函数，利用这个核函数可以把低维的数据映射到更高的维度，常见的核函数有线性核、高斯核、径向基核函数等。其实就是一个方法将低维数据映射到高维。例如[a,b]通过如下的映射[a^2,b^2,a,b ,ab]从2维就变成了5维了这样就把线性不可分的变成了线性可分的了  
其实对于高斯核函数它的作用也是类似的，使用高斯分布增大类间距离  
$$K(X,Y) = exp \lbrace{- \frac {|x - y||^2} 2a^2 } \rbrace$$
使用核函数还有很多好处，例如可以在低纬度完成高纬度中向量内积的计算  

3. 举个栗子：  
假设我们有两个数据，x=(x1,x2,x3);y=(y1,y2,y3)，此时在3D空间已经不能对其径行线性划分了，我们通过一个函数将数据映射到更高维度的空间，比如9维的话，那么f(x)=(x1x1,x1x2,x1x3,x2x1,x2x2,x2x3,x3x1,x3x2,x3x3),由于需要计算内积，所以在新的数据在9维空间，需要计算<f(x),f(y)>的内积，需要花费O(n^2)  
再具体点，令x=(1,2,3);y=(4,5,6)，那么f(x)=(1,2,3,2,4,6,3,6,9),f(y)=(16,20,24,20,25,36,24,30,36)，此时<f(x),f(y)>=16+40+72+40+100+180+72+180+324=1024  
似乎还能计算，但是如果将维度扩大到一个非常大数的时候，计算起来可就不是一丁点的问题了  
但是发现：K(x,y) = (<f,y>)^2  
K(x,y)=(4+10+18)^2=32^2=1024  
两者相等，K(x,y)=(<x,y>)^2=<f(x),f(y)>，但是K(x,y)计算起来却比<f(x),f(y)>简单的多，也就是说只要用K(x,y)来计算，效果和<f(x),f(y)>是一样的，但是计算效率却大幅度提高了。如：K(x,y)是O(n)，而<f(x),f(y)>是O(n^2)，所以使用核函数的好处就是，可以在一个低维空间去完成高纬度（或者无限维度）样本内积的计算，比如K(x,y)=(4+10+18)^2的3D空间对比<f(x),f(y)>=16+40+72+40+100+180+72+180+324的9D空间  
**要求这样一个支持向量的问题，首先要找到优化的目标函数**  
对于线(w,b)可以通过缩放使得其结果值|Y|>=1   
$$y_1·(y^T·\Phi (x_1) + b)\ge 1$$
$$argmax_{w,b} \lbrace \frac {1}{||w||} min_{i}· \lceil y_i · (w_T · \Phi (x_1) +b)\rceil \rbrace$$  
搞定目标函数
$$argmax_{w,b} \frac {1}{||w||}$$   
通过拉[格朗日乘子法](https://baike.baidu.com/item/%E6%8B%89%E6%A0%BC%E6%9C%97%E6%97%A5%E4%B9%98%E5%AD%90%E6%B3%95/1946079?fr=aladdin)转换min-max对偶问题既可以求解出w和b

3. 接下来使用pyspark中使用SVM支持向量机来完成二分类  

~~~python

from pyspark.sql import SparkSession
from pyspark.ml.classification import LinearSVC
from pyspark.ml.evaluation import MulticlassClassificationEvaluator

spark = SparkSession.builder.master('local').appName('SVM').getOrCreate()

data = spark.read.format('libsvm').load('D:/data/mllib/sample_libsvm_data.txt')
lsvc = LinearSVC(maxIter=10, regParam=0.1)
splits = data.randomSplit([0.6, 0.4], 1234)
train = splits[0]
test = splits[1]
lsvcModel = lsvc.fit(train)
result = lsvcModel.transform(test)
predictionAndLabels = result.select('prediction', 'label')
evaluator = MulticlassClassificationEvaluator(metricName='accuracy')
print('Test set accuracy = ' + str(evluator.evaluate(predictionAndLabels)))
>>> 
Test set accuracy = 1.0
~~~
测试精度很高，达到了100% 
