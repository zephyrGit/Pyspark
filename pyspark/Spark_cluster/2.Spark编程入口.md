
1. SparkContext 是pyspark的编程入口，作业的提交，任务的分发  
应用的注册都会在SparkContext中进行，一个SparkContext实例代表着和Spark的一个连接，只有建立了连接才可以把作业提交到集群中去，实例化SparkContext之后才能创建RDD和Broadcast广播变量。  

2. SparkContext 获取  
启动pyspark --master spark://hadoop-maste:7077之后，可以通过SparkSession获取SparkContext对象

spark.SparkContext  [SparkContext master=spark://hadoop-maste:7077 appName=Pysparkshell]

从打印的记录来看，sparkContext连接的Spark集群的地址是spark://hadoop-maste:7077

另一种获取sparcontext的方法是，引入pyspark.SparkContext进行创建，新建sparkContext.py文件


```python
from pyspark import SparkContext
from pyspark import SparkConf
```


```python
conf = SparkConf()
conf.set("master", "local")
```




    <pyspark.conf.SparkConf at 0x2a58b2dcc50>




```python
sparkContext = SparkContext(conf=conf)
```


```python
rdd = sparkContext.parallelize(range(100))
print(rdd.collect())
```

    [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 62, 63, 64, 65, 66, 67, 68, 69, 70, 71, 72, 73, 74, 75, 76, 77, 78, 79, 80, 81, 82, 83, 84, 85, 86, 87, 88, 89, 90, 91, 92, 93, 94, 95, 96, 97, 98, 99]
    


```python
sparkContext.stop()
```

进入 hadoop 集群
~~~shell
 root@machime:$ docker exec -it hadoop-maste bin/bash
 root@machime:$ ll
 root@machime:$ mkdir workspace
 root@machime:$ ll
 root@machime:$ scp -r sparkcontext/ root@172.16.0.2:~
 root@machime:$ mv sparkcontext/ ./workspace
 root@machime:$ cd workspace/
 root@machime:$ cat sparkContext.py
# 查看spark ip
 root@machime:$ docker network inspect spark 
# 运行文件 
 root@machime:$ spark-submit sparkContext.py
~~~

- 运行之前先将spark 目录下的log4j.properties配置文件中的日志级别改为如下：
/usr/local/spark-2.3.0-bin-hadoop/2.7/conf
~~~
Set everything to be logged  to the console
log4j.rootCategory=WARN, colsole
log4j.appender.console=rog.apache.log4j.ConsoleAppender
~~~
这样后台打印的日志不至于他多印象查看！重启Spark集群  
需要把此文件拷贝到hadoop-node1、hadoop-node2  
scp log4j.properties root@hadoop-node1:/usr/local/spark-2.3.0-bin-hadoop2.7/conf  
scp log4j.properties root@hadoop-node2:/usr/local/spark-2.3.0-bin-hadoop2.7/conf  
之后需要重启集群  
cd usr/local/spark-2.3.0-bin-hadoop/2.7/sbin  
./stop-all.sh  
./start-all.sh  

3. accumulator是sparkcontext上用来创建累加器的方法  
创建的累加器可以在各个task中进行累加，兵器只能够支持add操作。该方法支持传入累加器的初始值，这里通过Accumulator累加器做1到50的加法作为例子。


```python
from pyspark import SparkConf, SparkContext
import numpy as np

conf = SparkConf()
conf.set("master", "spark://hadoop-maste:7077")
context = SparkContext(conf=conf)
acc = context.accumulator(0)
print(type(acc), acc.value)
```

    <class 'pyspark.accumulators.Accumulator'> 0
    


```python
rdd = context.parallelize(np.arange(101), 5)


def acc_add(a):
    acc.add(a)
    return a


rdd2 = rdd.map(acc_add)
print(rdd2.collect())
print(acc.value)
context.stop()
```

    [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 62, 63, 64, 65, 66, 67, 68, 69, 70, 71, 72, 73, 74, 75, 76, 77, 78, 79, 80, 81, 82, 83, 84, 85, 86, 87, 88, 89, 90, 91, 92, 93, 94, 95, 96, 97, 98, 99, 100]
    5050
    

4.  addFile方法添加文件，使用Spark Files.get方法获取文件  
这个方法接收一个路径，路径可以是本地路径也可是hdfs路径，或者一个http:，https，如果上传的是一个文件夹，则指定recurisize参数为True。上传的文件使用SparkFiles.get(filename)的方式进行获取。


```python
# 新建addFile.py文件，内容如下：
from pyspark import SparkFiles, SparkConf, SparkContext
import os
import numpy as np
```


```python
tempdir = "H:/PS"
path = os.path.join(tempdir, "num_data")
with open(path, "w") as f:
    f.write("100")
conf = SparkConf()
conf.set("master", "spark://hadoop-maste:7077")
context = SparkContext(conf=conf)
```


```python
context.addFile(path)
rdd = context.parallelize(np.arange(10))


def fun(iterable):
    with open(SparkFiles.get("num_data")) as f:
        value = int(f.readline())
        return [x * value for x in iterable]
```


```python
print(rdd.mapPartitions(fun).collect())
context.stop()
```

    [0, 100, 200, 300, 400, 500, 600, 700, 800, 900]
    

hdfs路径下文件，新建hdfs_addFiles.py文件，内容如下：


```python
from pyspark import SparkFiles, SparkConf, SparkContext
import numpy as np
```


```python
conf = SparkConf()
conf.set("master", "spark://hadoop-maste:7077")
context = SparkContext(conf=conf)
```


```python
path = "hdfs://hadoop-mste:9000/datas/num_data"
```


```python
context.addFile(path)
rdd = context.parallelize(np.arange(10))
```


```python
def fun(iterable):
    with open(SparkFiles.get("num_data")) as f:
        value = int(f.readline())
        return [x * value for x in iterable]
```


```python
print(rdd.mapPartitions(fun).collect())
context.stop()
```

运行 spark-submit hdfs_addFile.py
~~~
root@hadoop-maste:~/workspace/sparkcontext spark-submit hdfs_addFile.py
~~~
需要注意的是addFile默认识别本地路径，若是hdfs路径，需要指定hdfs://hadoop-maste:9000协议，uri及端口信息

配置httpd服务器，httpd的配置  http://192.168.0.6.100/num_data  
进入安装http服务器的机器的 /var/www/html/目录，在这个目录中新建mum_data文件，文件内为容为100
然后编写 http_addFile.py文件，在代码中读取新建的httpd服务器上的num_data文件。


```python
# http_addFile.py
from pyspark import SparkConf, SparkContext, SparkFiles
import numpy as np
```


```python
conf = SparkConf()
conf.set("master", "spark://hadoop-maste:7077")
context = SparkContext(conf=conf)
```


```python
path = "http://192.168.0.100/num_data"
context.addFile(path)
rdd = context.parallelize(np.arange(10))


def fun(iterable):
    with open(SparkFiles.get("num_data")) as f:
        value = int(f.readline())
        return [x * value for x in iterable]


print(rdd.mapPartitions(fun).collect())
context.stop()
```

运行 spark-submit http_addFile.py  
从上面三个例子中，可以看出addFile方法的强大之处，借助该方法，在pyspark里任务运行中可以读取几乎任何位置的文件来参与计算
