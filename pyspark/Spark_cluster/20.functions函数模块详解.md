
1. pyspark.sql.functions 这个方法内置了很多有用的方法，熟悉了之后，很多业务问题都可以直接调用解决


2. abs求绝对值，acos求反余弦值，asin求反正弦值，atan反正切值。反余弦的取值范围为0~PI，asin取值范围为-pi/2~pi/2范围之内，反正切范围-pi/2~pi/2之内

~~~python
from pyspark.sql import SparkSession
from pyspark.sql.functions import *
import numpy as np
import pandas as pd

spark = SparkSession.builder.master('local').appName('functions').getOrCreate()
pandas_df = pd.DataFrame(np.linspace(-1, 1, 100))
df = spark.createDataFrame(pandas_df, schema=['id'])
df.select(
    abs(df.id).alias('abs'),
    acos(df.id).alias('acos'),
    asin(df.id).alias('asin'),
    atan(df.id).alias('atan')
).show()
>>> 
+------------------+------------------+-------------------+-------------------+
|               abs|              acos|               asin|               atan|
+------------------+------------------+-------------------+-------------------+
|               1.0| 3.141592653589793|-1.5707963267948966|-0.7853981633974483|
|0.9797979797979798|  2.94024514648864|-1.3694488196937433| -0.775194435903494|
|0.9595959595959596|2.8563590944682753|-1.2855627676733787|-0.7647825277718445|
|0.9393939393939394|2.7916545622584157|-1.2208582354635191|  -0.75415832996718|
|0.9191919191919192|2.7368199162761746| -1.166023589481278|-0.7433177559555577|
| 0.898989898989899|2.6882540265816615|-1.1174576997867647|-0.7322567545525128|
|0.8787878787878788|2.6441125536712304|-1.0733162268763339|-0.7209713239718948|
|0.8585858585858586|2.6033012084423457|-1.0325048816474491|-0.7094575271315536|
|0.8383838383838383| 2.565107755655995| -0.994311428861098|-0.6977115082679478|
|0.8181818181818181|2.5290379152504543|-0.9582415884555576|-0.6857295109062862|
| 0.797979797979798| 2.494732059198814|-0.9239357324039174|-0.6735078972257722|
|0.7777777777777778|2.4619188346815495|-0.8911225078866528|-0.6610431688506868|
|0.7575757575757576|2.4303874876287264|  -0.85959116083383|-0.6483319890872606|
|0.7373737373737373| 2.399970423487821|-0.8291740966929243|-0.6353712066133405|
|0.7171717171717171| 2.370531726762639|-0.7997353999677423|-0.6221578806126109|
| 0.696969696969697|2.3419593256893587|-0.7711629988944623|-0.6086893073274118|
|0.6767676767676767| 2.314159480387587|-0.7433631535926905|-0.5949630479838941|
|0.6565656565656566|2.2870528042596527|-0.7162564774647562|-0.5809769580203017|
|0.6363636363636364|2.2605713275803963|-0.6897750007854997|-0.5667292175235064|
|0.6161616161616161|2.2346562878665277|-0.6638599610716309|-0.5522183627506319|
+------------------+------------------+-------------------+-------------------+
only showing top 20 rows
~~~

3. add_months(start, months) 对时间格式，在start时间上加上months个月

~~~python
spark.createDataFrame([('2018-07-10', ), ('2019-07-10', )],
                      schema=['months']).select(add_months('months', 3)).show()
>>> 
+---------------------+
|add_months(months, 3)|
+---------------------+
|           2018-10-10|
|           2019-10-10|
+---------------------+
~~~

4. approx_count_distinct(col, rsd=None) 用于估计给定的col列有多少非重复数据，返回新的列

~~~python
df = spark.read.csv('./datas/titanic_train.csv', header=True)
df.select(approx_count_distinct(df.Age)).show()
>>>  
|approx_count_distinct(Age)|
+--------------------------+
|                        83|
+--------------------------+
~~~

5. array(*cols) 创建一个Array类型的列

~~~python
df1 = df.select(array([df.Name, df.Sex, df.Age]).alias('info'))
df1.dtypes
>>> [('info', 'array<string>')]
df1.show(truncate=False)
>>> 
+---------------------------------------------------------------------+
|info                                                                 |
+---------------------------------------------------------------------+
|[Braund, Mr. Owen Harris, male, 22]                                  |
|[Cumings, Mrs. John Bradley (Florence Briggs Thayer), female, 38]    |
|[Heikkinen, Miss. Laina, female, 26]                                 |
|[Futrelle, Mrs. Jacques Heath (Lily May Peel), female, 35]           |
|[Allen, Mr. William Henry, male, 35]                                 |
|[Moran, Mr. James, male,]                                            |
|[McCarthy, Mr. Timothy J, male, 54]                                  |
|[Palsson, Master. Gosta Leonard, male, 2]                            |
|[Johnson, Mrs. Oscar W (Elisabeth Vilhelmina Berg), female, 27]      |
|[Nasser, Mrs. Nicholas (Adele Achem), female, 14]                    |
|[Sandstrom, Miss. Marguerite Rut, female, 4]                         |
|[Bonnell, Miss. Elizabeth, female, 58]                               |
|[Saundercock, Mr. William Henry, male, 20]                           |
|[Andersson, Mr. Anders Johan, male, 39]                              |
|[Vestrom, Miss. Hulda Amanda Adolfina, female, 14]                   |
|[Hewlett, Mrs. (Mary D Kingcome) , female, 55]                       |
|[Rice, Master. Eugene, male, 2]                                      |
|[Williams, Mr. Charles Eugene, male,]                                |
|[Vander Planke, Mrs. Julius (Emelia Maria Vandemoortele), female, 31]|
|[Masselmani, Mrs. Fatima, female,]                                   |
+---------------------------------------------------------------------+
only showing top 20 rows
~~~

6. array_contains(col, value) 返回array类型中是否包含value值，示范返回True，否则False，array为null，返回null

~~~python
df1.select(array_contains('info', 'male')).show()
>>>
+--------------------------+
|array_contains(info, male)|
+--------------------------+
|                      true|
|                     false|
|                     false|
|                     false|
|                      true|
|                      true|
|                      true|
|                      true|
|                     false|
|                     false|
|                     false|
|                     false|
|                      true|
|                      true|
|                     false|
|                     false|
|                      true|
|                      true|
|                     false|
|                      null|
+--------------------------+
only showing top 20 rows
~~~

7. asc(col),desc(col) 分别在给定的列上进行升序、降序排列， 需要和排序函数如sort、orderBy函数一起使用

~~~python
df.sort(desc('Age'), asc('Name')).show(5)
>>> 
+-----------+--------+------+--------------------+------+---+-----+-----+----------+-------+-----+--------+
|PassengerId|Survived|Pclass|                Name|   Sex|Age|SibSp|Parch|    Ticket|   Fare|Cabin|Embarked|
+-----------+--------+------+--------------------+------+---+-----+-----+----------+-------+-----+--------+
|        490|       1|     3|"Coutts, Master. ...|  male|  9|    1|    1|C.A. 37671|   15.9| null|       S|
|        148|       0|     3|"Ford, Miss. Robi...|female|  9|    2|    2|W./C. 6608| 34.375| null|       S|
|        166|       1|     3|"Goldsmith, Maste...|  male|  9|    0|    2|    363291| 20.525| null|       S|
|        542|       0|     3|Andersson, Miss. ...|female|  9|    4|    2|    347082| 31.275| null|       S|
|        183|       0|     3|Asplund, Master. ...|  male|  9|    4|    2|    347077|31.3875| null|       S|
+-----------+--------+------+--------------------+------+---+-----+-----+----------+-------+-----+--------+
only showing top 5 rows

df.orderBy(desc('Age'), asc('Name')).show(5)
>>> 
+-----------+--------+------+--------------------+------+---+-----+-----+----------+-------+-----+--------+
|PassengerId|Survived|Pclass|                Name|   Sex|Age|SibSp|Parch|    Ticket|   Fare|Cabin|Embarked|
+-----------+--------+------+--------------------+------+---+-----+-----+----------+-------+-----+--------+
|        490|       1|     3|"Coutts, Master. ...|  male|  9|    1|    1|C.A. 37671|   15.9| null|       S|
|        148|       0|     3|"Ford, Miss. Robi...|female|  9|    2|    2|W./C. 6608| 34.375| null|       S|
|        166|       1|     3|"Goldsmith, Maste...|  male|  9|    0|    2|    363291| 20.525| null|       S|
|        542|       0|     3|Andersson, Miss. ...|female|  9|    4|    2|    347082| 31.275| null|       S|
|        183|       0|     3|Asplund, Master. ...|  male|  9|    4|    2|    347077|31.3875| null|       S|
+-----------+--------+------+--------------------+------+---+-----+-----+----------+-------+-----+--------+
only showing top 5 rows
~~~

8. ascii(col) 计算String列的第一个字符的ascii值


9. avg 计算均值

~~~python
df.select(avg(df.Age), avg('Age')).show(5)
>>> 
+-----------------+-----------------+
|         avg(Age)|         avg(Age)|
+-----------------+-----------------+
|29.69911764705882|29.69911764705882|
+-----------------+-----------------+
~~~

10. bin 计算指定列的二进制字符串表示的值

~~~python
df.select(bin(df.Age),bin('Age')).show(5)
>>> 
+--------+--------+
|bin(Age)|bin(Age)|
+--------+--------+
|   10110|   10110|
|  100110|  100110|
|   11010|   11010|
|  100011|  100011|
|  100011|  100011|
+--------+--------+
only showing top 5 rows
~~~

11. broadcast(df) 标记一个足够小的DataFrame，在join操作的时候使用broadcast的方法来进行来连接

~~~python
df1 = spark.range(1, 10, 2, 3)
df2 = spark.range(1, 15, 3, 3)
broadcast(df1)
>>> 
DataFrame[id: bigint]
~~~

12. bround(col, scale=0) 对指定的列数值类型数据进行四舍五入，scale指定小数点后的第几位

~~~python
df = spark.createDataFrame([(2.412, ), (3.141, )], schema=['num'])
df.show()
df.select(bround(df.num, 1)).show()
>>> 
+-----+
|  num|
+-----+
|2.412|
|3.141|
+-----+

+--------------+
|bround(num, 1)|
+--------------+
|           2.4|
|           3.1|
+--------------+
~~~

13. cbrt(col) 计算列的立方根

~~~python
df.select(cbrt(df.num)).show()
>>> 
+------------------+
|         CBRT(num)|
+------------------+
|  1.34109363455578|
|1.4644997845718493|
+------------------+
~~~

14. ceil(col) 向上取整

~~~python
df = spark.createDataFrame([(2.14, None), (None, 3.14), (None, None)],
                           schema=['num', 'age'])
df.select(ceil(df.num)).show()>>> 
>>>
+---------+
|CEIL(num)|
+---------+
|        3|
|     null|
|     null|
+---------+
~~~

15. coalesce(*col) 在多列数据中， 返回第一列不为null的数据

~~~python
df = spark.createDataFrame([(2.14, None), (None, 3.14), (None, None)],
                           schema=['num', 'age'])
df.select(coalesce(df.num, df.age)).show()
>>> 
+------------------+
|coalesce(num, age)|
+------------------+
|              2.14|
|              3.14|
|              null|
+------------------+
~~~

16. col(col) 根据列名返回Column对象和column(col) 方法功能相同

~~~python
df.select(col('age'), col('num')).show()
>>> 
+----+----+
| age| num|
+----+----+
|null|2.14|
|3.14|null|
|null|null|
+----+----+
~~~

17. collect_list(col) 将某列的数据以一个list集合的方式返回，list可以包含重复数据

~~~python
import math
df = spark.createDataFrame([(1.234, None), (math.e, 2.235), (math.pi, None)],
                           schema=['num', 'age']).repartition(3)
df.select(collect_list('num')).show(truncate=False)
>>> 
+---------------------------------------------+
|collect_list(num)                            |
+---------------------------------------------+
|[1.234, 2.718281828459045, 3.141592653589793]|
+---------------------------------------------+
~~~

18. collect_set(col) 将某列的数据以set集合的方式返回，set中不包含重复的数据

~~~python
df.select(collect_set('num')).show(truncate=False)
>>> 
+---------------------------------------------+
|collect_set(num)                             |
+---------------------------------------------+
|[1.234, 3.141592653589793, 2.718281828459045]|
+---------------------------------------------+
~~~

19. concat(*cols) 将多列的数据拼接到一起

~~~python
df.select(concat('num', 'age')).show(truncate=False)
>>> 
+----------------------+
|concat(num, age)      |
+----------------------+
|2.7182818284590452.235|
|null                  |
|null                  |
+----------------------+
~~~

20. concat_ws(sep, *cols) 使用指定的分割符将多列数据拼接在一起

~~~python
df.select(concat_ws('->', 'num', 'age')).show(truncate=False)
>>> 
+------------------------+
|concat_ws(->, num, age) |
+------------------------+
|2.718281828459045->2.235|
|3.141592653589793       |
|1.234                   |
+------------------------+
~~~

21. conv(col, fromBase, toBase) 将数值类型的列从fromBase进制转换到toBase进制

~~~python
df = spark.createDataFrame([(12, None), (10, 2.235), (16, None)],
                           schema=['num', 'age']).repartition(3)
df.select(conv('num', 10, 16)).show(truncate=False)
>>> 
+-----------------+
|conv(num, 10, 16)|
+-----------------+
|C                |
|A                |
|10               |
+-----------------+
~~~

22. corr(col1, col2) 计算两列的相关系数

~~~python
df = spark.createDataFrame([(12, 12), (10, 2), (16, 9)],
                           schema=['num', 'age']).repartition(3)
df.select(corr('num', 'age')).show(truncate=False)
>>> 
+------------------+
|corr(num, age)    |
+------------------+
|0.5315540694532027|
+------------------+
~~~

23. cos(col) 计算col列的余弦值

~~~python
sc = spark.sparkContext
rdd = sc.parallelize([(12, 12), (10, 2), (16, 9)], 3)
df = spark.createDataFrame(rdd, schema=['num', 'age'])
df.select(cos('num')).show(truncate=False)
>>> 
+-------------------+
|COS(num)           |
+-------------------+
|0.8438539587324921 |
|-0.8390715290764524|
|-0.9576594803233847|
+-------------------+
~~~

24. count(col) 是一个聚合函数， 用于返回一个集合中的数据的数量

~~~python
rdd = sc.parallelize([(12, 12), (10, 2), (16, 9)], 3)
df = spark.createDataFrame(rdd, schema=['num', 'age'])
df.agg(count('num')).show(truncate=False)
>>> 
+----------+
|count(num)|
+----------+
|3         |
+----------+
~~~

25. countDistinct(col, *col)计算指定的列中不重复的数据的条数

~~~python
rdd = sc.parallelize([(12, 12), (12, 12), (16, 9)], 3)
df = spark.createDataFrame(rdd, schema=['num', 'age'])
df.agg(countDistinct('num')).show(truncate=False)
>>> 
+-------------------+
|count(DISTINCT num)|
+-------------------+
|2                  |
+-------------------+
~~~

26. covar_pop(col1, col2) 计算两列的总体协方差【除以N】

~~~python
rdd = sc.parallelize([(12, 12), (12, 12), (16, 9)], 3)
df = spark.createDataFrame(rdd, schema=['num', 'age'])
df.select(covar_pop('num', 'age')).show(truncate=False)
>>> 
+-------------------+
|covar_pop(num, age)|
+-------------------+
|-2.6666666666666665|
+-------------------+
~~~

27. covar_samp(col1, col2) 计算两列的样本协方差【除以N-1】

~~~python
rdd = sc.parallelize([(12, 12), (12, 12), (16, 9)], 3)
df = spark.createDataFrame(rdd, schema=['num', 'age'])
df.select(covar_samp('num', 'age')).show(truncate=False)
>>> 
+--------------------+
|covar_samp(num, age)|
+--------------------+
|-4.0                |
+--------------------+
~~~

28. crc32(col) 由CRC检验值

~~~python
rdd = sc.parallelize([('12', 12), ('12', 12), ('16', 9)], 3)
df = spark.createDataFrame(rdd, schema=['num', 'age'])
df.select(crc32(df.num)).show(truncate=True)
>>> 
+----------+
|crc32(num)|
+----------+
|1330857165|
|1330857165|
|1212055764|
+----------+
~~~

29. create_map(*cols) 使用指定的列生成map

~~~python
df1 = df.select(create_map(df.num, df.age))
df1.show(truncate=False)
>>> 
+-------------+
|map(num, age)|
+-------------+
|[12 -> 12]   |
|[12 -> 12]   |
|[16 -> 9]    |
+-------------+

df1.dtypes
>>> [('map(num, age)', 'map<string,bigint>')]
~~~

30. current_date() 返回当前的日期作为DataType列

~~~python
df.select(current_date(), df.num).show()
>>> 
+--------------+---+
|current_date()|num|
+--------------+---+
|    2019-07-12| 12|
|    2019-07-12| 12|
|    2019-07-12| 16|
+--------------+---+
~~~

31. current_timestmp 返回当前的时间戳作为TimestampType类型的列

~~~python
df.select(current_timestamp(), df.num).show()
>>> 
+--------------------+---+
| current_timestamp()|num|
+--------------------+---+
|2019-07-12 17:15:...| 12|
|2019-07-12 17:15:...| 12|
|2019-07-12 17:15:...| 16|
+--------------------+---+
~~~

32. date_add(start, days) 在开始日期start上增加days天

~~~python
rdd = sc.parallelize([('2019-07-10', 12), ('2019-10-10', 12),
                      ('2019-12-31', 9)], 3)
df = spark.createDataFrame(rdd, schema=['date', 'age'])
df1 = df.select(date_add(df.date, 3), df.age).show(truncate=False)
>>> 
+-----------------+---+
|date_add(date, 3)|age|
+-----------------+---+
|2019-07-13       |12 |
|2019-10-13       |12 |
|2020-01-03       |9  |
+-----------------+---+
~~~

33. date_fromat(date, format) 将日期转换成format格式，现在SimpleDateFormat离所有的日期格式都支持

~~~python
df1 = df.select(date_format(df.date, 'yyyy/dd/MM'), df.age).show(truncate=False)
>>> 
+-----------------------------+---+
|date_format(date, yyyy/dd/MM)|age|
+-----------------------------+---+
|2019/10/07                   |12 |
|2019/10/10                   |12 |
|2019/31/12                   |9  |
+-----------------------------+---+
~~~

34. date_sub(start, days) 在开始时间的基础上减去days天

~~~python
df1 = df.select(date_sub(df.date, 3), df.age).show(truncate=False)
>>> 
+-----------------+---+
|date_sub(date, 3)|age|
+-----------------+---+
|2019-07-07       |12 |
|2019-10-07       |12 |
|2019-12-28       |9  |
+-----------------+---+
~~~

35. date_trunc(format, timestamp) 按照指定的format对时间进行裁剪

~~~python
df = spark.createDataFrame([('1991-07-10 17:25:23', )],['t'])
df.select(date_trunc('year', df.t).alias('year')).show()
>>> 
+-------------------+
|               year|
+-------------------+
|1991-01-01 00:00:00|
+-------------------+
df.select(date_trunc('month', df.t).alias('month')).show()
>>> 
+-------------------+
|              month|
+-------------------+
|1991-07-01 00:00:00|
+-------------------+
~~~
**format的可选项有：year、yyyy、yy、month、mon、mm、day、dd、hour、minute、second、week、quater**

36. datediff(end, start) 返回两个日期的时间差

~~~python
df = spark.createDataFrame([('2019-10-01', '2019-10-16')], ['s', 'e'])
df.select(datediff(df.s, df.e)).show()
>>> 
+--------------+
|datediff(s, e)|
+--------------+
|           -15|
+--------------+
~~~

37. dayofmonth(col) 返回一个月中的第几天，dayofweek(col)返回一周中的第几天，dayofyear(col)一年中的第几天

~~~python
df.select(dayofmonth(df.s), dayofweek(df.s), dayofyear(df.s)).show()
>>> 
+-------------+------------+------------+
|dayofmonth(s)|dayofweek(s)|dayofyear(s)|
+-------------+------------+------------+
|            1|           3|         274|
+-------------+------------+------------+
~~~

38. decode(col, charset) 对指定的列按照指定的字符集进行解码

~~~python
df.select(decode(df.s, 'utf-8')).show()
>>> 
+----------------+
|decode(s, utf-8)|
+----------------+
|      2019-10-01|
+----------------+
~~~

39. degrees(col) 将弧度值转换为角度值

~~~python
df = spark.createDataFrame([(math.pi/2,),(math.pi/4,)],['t'])
df.select(degrees(df.t)).show()
>>> 
+----------+
|DEGREES(t)|
+----------+
|      90.0|
|      45.0|
+----------+
~~~

40. encode(col, charset) 指定的列按照charset进行编码

~~~python
df = spark.createDataFrame([('2019-10-01', '2019-10-16')], ['s', 'e'])
df.select(encode(df.s, 'utf-8')).show()
>>> 
+--------------------+
|    encode(s, utf-8)|
+--------------------+
|[32 30 31 39 2D 3...|
+--------------------+
~~~

41. exp(col) 对给定的列进行e的value次方计算

~~~python
rdd = sc.parallelize([('12', 1), ('12', 2), ('16', 3)], 3)
df = spark.createDataFrame(rdd, schema=['num', 'age'])
df.select(exp('age')).show()
>>> 
+------------------+
|          EXP(age)|
+------------------+
| 2.718281828459045|
|  7.38905609893065|
|20.085536923187668|
+------------------+
~~~

42. explode(col) 将给定的col中每个返回一个新写的行对象

~~~python
from pyspark.sql import Row
df = spark.createDataFrame([Row(a=1, intlist=[1, 2, 3], mapfield={'a': 'b'})])
df.select(explode(df.intlist).alias('anInt')).show()
>>> 
+-----+
|anInt|
+-----+
|    1|
|    2|
|    3|
+-----+

df.select(explode(df.mapfield).alias('key', 'value')).show()
>>> 
+---+-----+
|key|value|
+---+-----+
|  a|    b|
+---+-----+
~~~

43. explode_outer(col) 对于给定的array或map中的每个元素，返回新的row行，和explode不一样的地方是，若array或map是空的或者为None，会返回Null

~~~python
df = spark.createDataFrame([(1, ['foo', 'bar'], {'x': 1.0}),
                            (2, [], {}),
                            (3, None, None)],
                           ('id', 'an_array', 'a_map'))
df.select(explode_outer(df.an_array)).show()
>>> 
+----+
| col|
+----+
| foo|
| bar|
|null|
|null|
+----+
df.select(explode_outer(df.a_map)).show()
>>> 
+----+-----+
| key|value|
+----+-----+
|   x|  1.0|
|null| null|
|null| null|
+----+-----+
~~~

44. expm1(col) 计算给定的列的e的value次方再减去1

~~~python
from pyspark.sql.functions import *

rdd = sc.parallelize([('12', 1), ('12', 2), ('16', 3)], 3)
df = spark.createDataFrame(rdd, schema=['num', 'age'])
df.select(expm1('age')).show()
>>> 
+------------------+
|        EXPM1(age)|
+------------------+
| 1.718281828459045|
|  6.38905609893065|
|19.085536923187668|
+------------------+
~~~

45. expr(str) 解析传入的表达式，来对列数据做相关的处理，表达式中可以使用大部分的functions包括udf自定义的函数

~~~python
df.select(expr('expm1(age)')).show()
>>> 
+--------------------------+
|EXPM1(CAST(age AS DOUBLE))|
+--------------------------+
|         1.718281828459045|
|          6.38905609893065|
|        19.085536923187668|
+--------------------------+
~~~

46. factorial(col) 计算给定列的阶乘

~~~python
df.select(factorial('age')).show()
>>> 
+--------------+
|factorial(age)|
+--------------+
|             1|
|             2|
|             6|
+--------------+
~~~

47. first(col, ingnorenulls=False) 返回组中的第一条数据

~~~python
df.groupBy('num').agg(first('num')).show()
>>> 
+---+-----------------+
|num|first(num, false)|
+---+-----------------+
| 16|               16|
| 12|               12|
+---+-----------------+
~~~

48. floor(col) 向下取整

~~~python
df = spark.createDataFrame([(2.235, ), (3.141, )], schema=['num'])
df.select(floor(df.num)).show()
>>> 
+----------+
|FLOOR(num)|
+----------+
|         2|
|         3|
+----------+
~~~

49. format_number(col, d)使用指定位数的小数点位数d， 格式化col秀英的数值类型

~~~python
df.select(format_number(df.num, 10)).show()
>>> 
+----------------------+
|format_number(num, 10)|
+----------------------+
|          2.2350000000|
|          3.1410000000|
+----------------------+
~~~

50. format_string(format, *cols)使用给定的格式格式化给定的列

~~~python
rdd = sc.parallelize([('12', 1), ('12', 2), ('16', 3)], 3)
df = spark.createDataFrame(rdd, schema=['num', 'age'])
df.select(format_string('%s--%d', df.num, df.age)).show()
>>> 
+-------------------------------+
|format_string(%s--%d, num, age)|
+-------------------------------+
|                          12--1|
|                          12--2|
|                          16--3|
+-------------------------------+
~~~

51. format_json(col, schema, options={}) 这个方法可以解析包含json格式的数据，可以把json字符集转换为StructType、ArrayType等类型。使用指定的schema用于解析json格式的数据

~~~python
from pyspark.sql.types import *
data = [(1, '''[{'age': 27}]'''), (2, '''[{'age': 25}]''')]
schema = ArrayType(StructType([StructField('age', IntegerType())]))
df = spark.createDataFrame(data, ('key', 'value'))
df.select(from_json(df.value, schema).alias('json')).show()
>>> 
+------+
|  json|
+------+
|[[27]]|
|[[25]]|
+------+
~~~

52. from_unixtime(timestamp, format='yyyy-mm-dd HH:mm:ss') 将时间戳转换为当前时区的指定格式的时间格式

~~~python
spark.conf.set('spark.sql.session.timeZone', 'China/Beijing')
time_df = spark.createDataFrame([(1563120000, )], ['unix_time'])
time_df.select(from_unixtime('unix_time').alias('ts')).show()
spark.conf.unset('spark.sql.session.timeZone')
>>> 
+-------------------+
|                 ts|
+-------------------+
|2019-07-14 16:00:00|
+-------------------+
~~~

53. get_json_object(col, path) 从含json数据的列中抽取出指定的字段，通过“$.name”的形式去除json中的字段

~~~python
data = [("1", '''{"f1": "tom", "f2": "jerry"}'''), ("2", '''{"f1": "hei"}''')]
df = spark.createDataFrame(data, ("key", "json"))
df.select(df.key, get_json_object(df.json, "$.f1"),
          get_json_object(df.json, "$.f2")).show()
>>> 
+---+---------------------------+---------------------------+
|key|get_json_object(json, $.f1)|get_json_object(json, $.f2)|
+---+---------------------------+---------------------------+
|  1|                        tom|                      jerry|
|  2|                        hei|                       null|
+---+---------------------------+---------------------------+
~~~

55. greatest(*cols) 返回给定列中的最大值

~~~python
rdd = sc.parallelize([(12, 1), (12, 2), (16, 3)], 3)
df = spark.createDataFrame(rdd, schema=['num', 'age'])
df.select(greatest(df.age, df.num)).show()
>>> 
+------------------+
|greatest(age, num)|
+------------------+
|                12|
|                12|
|                16|
+------------------+
~~~

56. hash(*col) 指定的列的hash值

~~~python
rdd = sc.parallelize([(12, 1), (12, 2), (16, 3)], 3)
df = spark.createDataFrame(rdd, schema=['num', 'age'])
df.select(greatest(df.age, df.num)).show()
>>> 
+------------------+
|greatest(age, num)|
+------------------+
|                12|
|                12|
|                16|
+------------------+
~~~

57. hour(col) 计算给时间列的小时，并返回生成新的DataFrame

~~~python
df = spark.createDataFrame([('2019-07-15 12:25:30',)], ['t'])
df.select(hour('t')).show()
>>> 
+-------+
|hour(t)|
+-------+
|     12|
+-------+
~~~
