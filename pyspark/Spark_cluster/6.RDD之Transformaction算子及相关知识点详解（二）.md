
- 5.cogroup(other, numPartitions=None)返回两个RDD中以相同key作为键的新的RDD

~~~python
rdd1 = sc.parallelize([('a',33),('b',22),('c',55)],3)
rdd2 = sc.parallelize([('a',12),('b',23),('c',56)],3)
rdd3 = rdd1.cogroup(rdd2,3)
result = rdd3.collect()

for data in result:
    print(data[0], ':')
    for i in list(data[1]):
        for j in i:
           print(j)
>>> b :22 23 a :33 12 c :55 56
~~~

- 6.combineByKey(createCombiner, mergeValue,mergeCombiners,numPartitions=None,partitionFunc=<function portable_hash>)这个方法指定方法合并不同分区上相同的key的数据到一个list中去，createCombiner用于创建只有一个的list，mergeValue将同一个分区上的具有相同的key的值追加到创建的list中，mergeCombiners用于将不同分区上的list按key合并，numPartitions指定分区数
    
~~~python
rdd = sc.parallelize([('a', 33), ('b', 22), ('c', 55), ('a', 12), ('b', 23),
                      ('c', 56)], 3)

def li(x):
    return [x]

def merge(li, x):
    li.append(x)
    return li

def combine(li1, li2):
    li1.extend(li2)
    return li1

rdd1 = rdd.combineByKey(li, merge, combine, 3)
rdd1.collect()
>>> [('b', [22, 23]), ('a', [33, 12]), ('c', [55, 56])]
~~~
这个方法类似于groupByKey和reduceByKey方法

- 7.groupByKey(numPartitions=None,partitionFunc=<function portable_hash>)这个方法和combineByKey方法类似，但是少了很多参数，该方法不是mapside预聚合的方法，因此使用该方法会又较昂贵的网络IO的开销，因为会在集群件产生大量的shuffle传输。性能没有reduceByKey和combineByKey方法好
    
    
~~~python
rdd = sc.parallelize([('a',33),('b',22),('c',55),('a',12),('b',23),('c',56)],3)
rdd1 = rdd.groupByKey(3)
rdd1.collect()
>>> [('b', <pyspark.resultiterable.ResultIterable at 0x2459c7bc3c8>),
>>> ('a', <pyspark.resultiterable.ResultIterable at 0x2459c7bc438>),
>>> ('c', <pyspark.resultiterable.ResultIterable at 0x2459c7bc470>)]
sorted([(x ,sorted(y)) for (x,y) in rdd1.collect()])
>>> [('a', [12, 33]), ('b', [22, 23]), ('c', [55, 56])]
~~~

- 8.reduceByKey(func, numPartitions=None,partitionFunc=<function portable_hash>)这个方法类似于groupByKey，却是map端预聚合的方法，性能较好。这个方法和aggregateByKey及上面的combineByKey性能类似
    
~~~python
from operator import add
rdd = sc.parallelize([('a',33),('b',22),('c',55),('a',12),('b',23),('c',56)],3)
rdd1 = rdd.reduceByKey(add, 3)
rdd1.collect()
>>> [('b', 45), ('a', 45), ('c', 111)]
~~~
也可以自定义函数，把相同的key的值追加到一个list中返回

~~~python
def append(x, y):
    li = list()
    li.append(x)
    li.append(y)
    return li

rdd1 = rdd.reduceByKey(append, 3)
rdd1.collect()
>>> [('b', [22, 23]), ('a', [33, 12]), ('c', [55, 56])]
~~~

- 9.aggregateByKey(zeroValue,seqFunc,combFunc,numPartitions=None,partitionFunc=<function portable_hash>)，该方法需要指定聚合的初始值，分区上的聚合函数seqFunc，最终的聚合函数combFunc
    
    
~~~python
from operator import add
rdd = sc.parallelize([('a',33),('b',22),('c',55),('a',12),('b',23),('c',56)],3)
rdd.aggregateByKey(0, add, add, 3).collect()
>>> [('b', 45), ('a', 45), ('c', 111)] 
~~~

**按照key聚合的方法，除了groupByKey之外，其余都是mapside端预聚合的方法，他们会带来的网络传输的量小，所以他们的性能比groupByKey强大；cogroup也是按照key聚合，把相同的key做键，值作为list**

- 10.context()返回创建该RDD的SparkContext

~~~python
rdd = sc.parallelize(range(10),3)
rdd.context
>>> SparkContext Version v2.3.2 Master local AppName myapp
~~~

- 11.distinct(numPartitions=None)返回RDD中不存在重复值的新的RDD，即去重，可以指定分区数

~~~python
rdd = sc.parallelize([1,1,1,3,3,3,2,2,2],3)
sorted(rdd.distinct(3).collect())
>>> [1,2,3]
~~~

- 12.filter(f)该方法返回包含满足条件的新的RDD


~~~python
rdd = sc.parallelize([1,1,1,1,2,2,2,2,3,3,3,3],3)
rdd.filter(lambda x: x>=2).collect()
>>> [2, 2, 2, 2, 3, 3, 3, 3] 
~~~

- 13.map(f, preservesPartitioning=False)将方法f应用到RDD中的每一个元素之上，返回新的RDD，第二个参数preservesPartitioning用于指定是否保存原分区，该方法效率较低，是因为对于元素中的每一条记录都会调用方法，在调用方法的过程中会产生中间的临时对象从而影响性能，这也是为什么说贵贱使用mapPartitions方法的原因

~~~python
rdd = sc.parallelize(range(10),3)
rdd1 = rdd.map(lambda x: x**3)
rdd1.collect()
>>> [0, 1, 8, 27, 64, 125, 216, 343, 512, 729]
~~~

- 14.flatMap(f, preservesPartitioning=False)首先应用f函数于每个元素上，然后扁平化，所谓的扁平化即将多个list合并程一个大的list，第二个参数preservesPartitioning用于指定是否保留分区

~~~python
rdd = sc.parallelize([1,2,3],3)
rdd1 = rdd.flatMap(lambda x:[x, x**2,x**3])
rdd1.collect()
>>> [1, 1, 1, 2, 4, 8, 3, 9, 27]
~~~

- 15.flatMapValues(f)这个方法会保留原始RDD的分区数，该方法将f方法应用到RDD中的key，value键值对，并进行扁平化，不会改变key值

~~~python
rdd = sc.parallelize([('a',[1,2,3,4,5]),('b',[11,22,33,44,55])],3)
rdd1 = rdd.flatMapValues(lambda x: x)
rdd1.collect()
>>>[('a', 1),('a', 2),('a', 3),('a', 4),('a', 5),
    ('b', 11),('b', 22),('b', 33),('b', 44),('b', 55)]
~~~

- 16.foldByKey(zeroValue,func,numPartitions=None,protitionFunc=<function portable_hash>)按键值对数据聚合，类似于reduceByKey和aggregateByKey方法，但是该方法提供了一个zeroValue起始值用于聚合


~~~python
rdd = sc.parallelize([('a',[1,2,3,4,5]),('b',[11,22,33,44,55])],3)
rdd1 = rdd.flatMapValues(lambda x:x)
rdd2 = rdd1.foldByKey(0,add,3)
rdd2.collect()
>>> [('b', 165), ('a', 15)]
rdd3 = rdd1.foldByKey(1, add, 3)
rdd3.collect()
>>> [('b', 166), ('a', 16)]
~~~
--------------
- groupBy(f,numPartitions=None,partitionFunc=<function portable_hash>)按照给定的f函数对RDD分组
    

~~~python
rdd = sc.parallelize(range(20),3)
rdd1 = rdd.groupBy(lambda x: x%3==0)
sorted([(x,sorted(y)) for (x,y) in rdd1.collect()])
>>> [(False, [1, 2, 4, 5, 7, 8, 10, 11, 13, 14, 16, 17, 19]),
 (True, [0, 3, 6, 9, 12, 15, 18])]
~~~
------------
groupWith(other,\*ohters)这个方法是cogroup的另一个名字，支持多个rdd按键聚合

~~~python
x = sc.parallelize([('a',1),('b',2)],3)
y = sc.parallelize([('a',11),('b',22)],3)
z = sc.parallelize([('a',111)],3)
o = sc.parallelize([('b',234)],3)
rdd = x.groupWith(y,z,o)
result = rdd.collect()
[(x, map(list,y)) for (x,y) in result]
>>> [('a',[[1],[11],[111],[]]), ('b',[[2],[23],[],[234]])]
~~~

- 17.intersection(other)该方法返回两个RDD的交集

~~~python
x = sc.parallelize(range(10),3)
y = sc.parallelize(range(5, 15),3)
rdd = x.intersection(y)
sorted(rdd.collect())
>>> [5,6,7,8,9]
~~~
------------
subsract(other,numPartitions=None)计算连个RDD的差集，在self RDD里而没有在other RDD里

~~~python
x = sc.parallelize(range(10),3)
y = sc.parallelize(range(5,15),3)
rdd = x.subtract(y)
sorted(rdd.collect())
>>> [0, 1, 2, 3, 4]
~~~
--------------
union(other)返回两个RDD的合集

~~~python
x = sc.parallelize([('a',1),('b',2),('c',3)],2)
y = sc.parallelize([('c',9),('b',7),('a',7)],2)
rdd = x.union(y)
sorted(rdd.collect())
>>> [('a', 1), ('a', 7), ('b', 2), ('b', 7), ('c', 3), ('c', 9)]
~~~

- 18.join(other, numPartitions=None)该方法返回两个RDD关于key的关联数据。该方法将在集群之上 执行hash连接操作

~~~python
x = sc.parallelize([('a',1),('b',2)],3)
y = sc.parallelize([('a',13),('a',24)],3)
x.join(y,3).collect()
>>> [('a', (1, 13)), ('a', (1, 24))]
~~~
---------------
fullOuterJoin(other, numPartiions=None)全外连接操作，类似于关系型数据库中表的关联操作，该操作用于key，value结构的RDD中按相同的key进行连接，若其中一个RDD有key而另一个没有，则另一个返回None

~~~python
x = sc.parallelize([('a',1),('b',2)],3)
y = sc.parallelize([('a',11),('c',22)],3)
x.fullOuterJoin(y,3).collect()
>>> [('b', (2, None)), ('a', (1, 11)), ('c', (None, 22))]
~~~
leftOuterJoin(other, numPartitions=None)取决于“左边”的RDD，左边的RDD有key而右边没有则返回None，右边有而左边没有则不取

~~~python
x = sc.parallelize([('a',1),('b',2)],3)
y = sc.parallelize([('a',11),('c',22)],3)
x.leftOuterJoin(y,3).collect()
>>> [('b', (2, None)), ('a', (1, 11))]
~~~
rightOuterJoin(other, numPartiions=None)取决于“右边”的RDD，右边的RDD有key而左边的RDD没有的返回None，右边有而左边没有的则不取值

~~~python
x = sc.parallelize([('a',1),('b',2)],3)
y = sc.parallelize([('a',11),('c',22)],3)
x.rightOuterJoin(y,3).collect()
>>> [('a', (1, 11)), ('c', (None, 22))]
~~~

- 19.keyBy(f)使用给定的f函数为RDD中的每个元素创建一个key值，形成一个二元tuple

~~~python
rdd = sc.parallelize(range(3,15),3)
rdd1 = rdd.keyBy(lambda x: str(x) + '_hello')
rdd1.collect()
~~~

- 20.keys()返回tuple中的键值

~~~python
rdd = sc.parallelize([(1,2),(2,3),(3,4)],3)
rdd.keys().collect()
>>> [1, 2, 3]
~~~

- 21.getCheckpointFile()，如果该RDD设置了检查点，返回RDD检查点对应的文件位置，否则返回None

- 22.glom()该方法会将每个partition上的元素放入一个list中，返回一个新的RDD

~~~python
rdd = sc.parallelize(range(20),5)
rdd.glom().collect()
>>> [[0, 1, 2, 3], [4, 5, 6, 7], [8, 9, 10, 11], [12, 13, 14, 15], [16, 17, 18, 19]]
~~~

- 23.mapParitions(f, perservesPartitioning=False)该方法将应用到每一个分区，而一个分区中可能包含几百个甚至上千万条的记录，这些记录公用中间变量，减少方法调用次数同时减少了中间变量的生成，从而提升性能。第二个参数仍然使用于指定是否保留分区数

~~~python
rdd = sc.parallelize(range(50),3)
rdd.mapPartitions(lambda x: [i ** 2 for i in x]).collect()
>>> [0,1,4,9,...,2401]
~~~

- 24.mapPartitionsWithIndex(f, perservesPartitioning=False)该方法同mapPartitions类似，但是该方法在传递给f的参数为两个分别为分区索引index和该分区对应的值。第二个参数仍然是用于指定是否保留分区

~~~python
rdd = sc.parallelize(range(10, 100),3)
rdd.mapPartitionsWithIndex(lambda index,values:(index,sum(values))).collect()
>>> [0, 735, 1, 1635, 2, 2535]
~~~
---------------------

repartitionAndSortWithinPartitions(numPartitions=None, partitionFunc=<function portable_hash>,ascending=True,keyfunc=<function <lambda>>)使用给定的分区器对RDD重新分区，在每个分区内按照keyfunc给出的条件进行升序排列
    
~~~python
rdd = sc.parallelize(range(1, 10), 3).map(lambda x: (x, x**2))
rdd.repartitionAndSortWithinPartitions(
    2, lambda x: (x % 2 == 0), ascending=False).glom().collect()
>>> [[(9, 81), (7, 49), (5, 25), (3, 9), (1, 1)],
 [(8, 64), (6, 36), (4, 16), (2, 4)]]
~~~

- 25.mapVlues(f)将f函数应用于key,Values类型的RDD数据中的Value之上

~~~python
rdd = sc.parallelize([('a',['abc','def','ghi']),('b',['bb','cc','dd'])],3)
rdd.mapValues(lambda x: '-'.join(x)).collect()
>>> [('a', 'abc-def-ghi'), ('b', 'bb-cc-dd')]
~~~

- 26.pipe(command, env=None, checkCode=False)使用外部的进程例如linux进程命令处理RDD中的每个元素，返回新的RDD.checkCode用于指定是否检查外部shell命令返回的值。默认False不检查

~~~python
rdd = sc.parallelize(['python pyspark','spark hadoop','hadoop'],3)
rdd.pipe('wc').collect()
>>> ['      1       2      15',
 '      1       2      13',
 '      1       1       7']
~~~

- 27.randomSplit(weights,seed=None)使用给定的全重值将RDD随机分成len(weights)个子RDD，seed用于设置随机种子，用于结果的复现。该方法的返回值为RDD列表

~~~python
rdd = sc.parallelize(range(10, 100), 3)
rdds = rdd.randomSplit([1, 4])

for rdd in rdds:
    rdd.collect()
~~~

- 28.sample(withReplacement, fraction, seed=None)该方法用于抽样，withReplacement表示是否有放回，fraction表示抽样比例，seed抽样种子，用于结果复现

~~~python
rdd = sc.parallelize(range(20), 5)
rdd.collect()
rdd.sample(True, 0.5, 123).collect()
>>> [0, 4, 6, 6, 8, 8, 10, 12, 12]
rdd.sample(True, 0.5, 124).collect()
>>> [5, 9, 11, 16, 16]
~~~
----------------
sampleByKey(withReplacement, fraction, seed=None)按key值按比例抽样，withReplacement表示是否有放回，fraction表示抽样比例，seed抽样种子

~~~python
fractions = {'a': 0.5, 'b': 0.1}
rdd = sc.parallelize(fractions.keys(),
                     3).cartesian(sc.parallelize(range(0, 20), 2))
sample = dict(rdd.sampleByKey(False, fractions, 2).groupByKey(3).collect())
[(iter[0], list(iter[1])) for iter in sample.items()]
>>> [('b', [10, 14, 19]), ('a', [1, 4, 9, 10, 12, 16, 19])]
~~~

- 29.setName()方法，为RDD命名，使用RDD上的name方法获取RDD的名字

~~~python
rdd.setName('test')
rdd.name()
>>> 'test'
~~~

- 30.sortBy(keyfunc, ascending=True, numPartitions=None)按自定义方法对数据排序

~~~python
datas = [('b', 33), ('m', 66), ('c', 88)]
sc.parallelize(datas, 3).sortBy(lambda x: x[0]).collect()
>>> [('b', 33), ('c', 88), ('m', 66)]
sc.parallelize(datas, 3).sortBy(lambda x: x[0], ascending=False, numPartitions=3).collect()
>>> [('m', 66), ('c', 88), ('b', 33)]
~~~

- 31.sortByKey(adcending=True, numPartitions=None, keyfunc=<function <lambda>>)该方法只能对key value类型的RDD进行排序
    
~~~python
sc.parallelize(datas, 3).sortByKey().collect()
>>>[('b', 33), ('c', 88), ('m', 66)]
 ~~~
使用numPartitions指定分区数，ascending指定升降序，keyfunc指定对key的处理函数，例如全部大写或小写

~~~python
sc.parallelize(datas, 3).sortByKey(numPartitions=3, ascending=False,
                                  keyfunc=lambda x: x.upper()).collect()
>>> [('m', 66), ('C', 88), ('B', 33)]
~~~

- 32.values()返回RDD中tuple的值

~~~python
rdd = sc.parallelize(range(10), 3).map(lambda x: (x, x**3))
rdd.values().collect()
>>> [0, 1, 8, 27, 64, 125, 216, 343, 512, 729]
~~~

- 33.zip(other)两个RDD中的元素组成的新RDD中的tuple。俗称“拉链操作”，注意两个RDD中的元素和分区数必须相等才能进行zip操作

~~~python
rdd = sc.parallelize(range(1,5),3)
rdd2 = sc.parallelize(['a','b','c','d'],3)
rdd.zip(rdd2).collect()
>>> [(1, 'a'), (2, 'b'), (3, 'c'), (4, 'd')]
~~~
----------------
zipWithIndex()为RDD中单元素的值和索引值zip成tuple类型的值，tuple的第二个元素为索引

~~~python
rdd = sc.parallelize(['a','b','c','d','e','f'],3)
rdd1 = rdd.zipWithIndex()
rdd1.collect()
>>> [('a', 0), ('b', 1), ('c', 2), ('d', 3), ('e', 4), ('f', 5)]
~~~
-----------
zipWithUniqueId()为RDD中的每个元素增加一个唯一的id值，形成tuple，每个分区中的id规律为k，n+k，2n+k；n为分区号，k为RDD的分区数

~~~python
rdd = sc.parallelize(['a','b','c','d','e','f'],3).map(lambda x: x*5)
rdd1 = rdd.zipWithUniqueId().glom()
rdd1.collect()
>>> [[('aaaaa', 0), ('bbbbb', 3)],
     [('ccccc', 1), ('ddddd', 4)],
     [('eeeee', 2), ('fffff', 5)]]
~~~

- 34.StorageLevel位于pyspark.StorageLevel  
它的构造方法如下：  
StorageLevel(useDisk, useMemory, useOffHeap, deserialized, replication=1)  
useDisk指定是否存盘  
useMemory指定是否存内存  
useOffHeap指定外部的堆外内存，例如aullxio内存分布式文件系统  
deserialized指定是否序列化  
replication指定冗余数，默认只存一份

在StorageLevel下定义好了几种存储级别，可以直接调用：  

~~~python
DISK_ONLY = StorageLevel(True, False, False, False, 1)
DISK_ONLY_2 = StorageLevel(True, False, False, False, 2)
MEMORY_AND_DISK = StorageLevel(True, True, False, False, 1)
MEMORY_AND_DISK_2 = StorageLevel(True, True, False, False, 2)
MEMORY_AND_DISK_SER = StorageLevel(True, True, False, False, 1)
MEMORY_AND_DISK_SER_2 = StorageLevel(True, True, False, False, 2)
MEMORY_ONLY = StorageLevel(False, True, False, False, 1)
MEMORY_ONLY_2 = StorageLevel(False, True, False, False, 2)
MEMORY_ONLY_SER = StorageLevel(False, True, False, False, 1)
MEMORY_ONLY_SER_2 = StorageLevel(False, True, False, False, 2)
OFF_HEAP = StorageLevel(True, True, True, False, 1)
~~~
------------
RDD的cache方法默认的存储级别为MEMORY_ONLY，表示只在内存中存一份  
persist方法可以通过关键字参数storageLevel指定存储级别  

------------
通过RDD上的getStorageLevel可以得到RDD存储级别，通过RDD上的is_cached可以返回是否已经缓存  

~~~python
rdd = sc.parallelize(range(10), 3)
rdd.is_cached
>>> True
rdd.collect()
>>> [0, 1, 2, 3, 4, 5, 6, 7, 8, 9]
rdd.getStorageLevel()
>>> StorageLevel(False, False, False, False, 1)
rdd.persist()
>>> PythonRDD[44] at collect at <ipython-input-11-f6b3e4f52bac>:3
rdd.persist(storageLevel=pyspark.StorageLevel,DiSK_ONLY)
rdd.getStorageLevel()
>>> StorageLevel(True, False, False, False, 1)
rdd.unpersist()
rdd.is_cached
>>> False
rdd.getStorageLevel()
>>> StorageLevel(False, False, False, False, 1)
~~~
