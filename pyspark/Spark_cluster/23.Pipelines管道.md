
1. **Pipelines**  
**Pipelines**是构建在DataFrame之上的并且提供了通用的高级API用于帮助开发人员和调试机器学习算法。在MLLIB为机器学习算法提供了标准的API,可以方便的组合多种机器学习算法到单一的Pipelines管道中。  


2. **常用概念**  
  - **DataFrame**：DataFrame是来自SparkSQL的一个数据集，它可以包含多种数据类型，如文本、特征向量、标签和预测值等
  - **Transformer**：Transformer与RDD和DataFrame上的transform操作一样，是将一种类型的DataFrame转换为另外一种类型的DataFrame，例如将一个带有特征的DataFrame转换为一个带有预测值的DataFrame
  - **Estimator**：它代表的是一个算法，可以填入DataFrame类型的数据并且返回一个Transformer对象。它在DataFrame上训练并产生了一个模型
  - **Pipeline**：它将多个Transformer和Estimator串联在一起构造出一个机器学习的工作流
  - **Parameter**：现在所有的Transformer和Estimator共享用一个API用于存储参数


3. **Transformer**是一个抽象，里面包含了特征转换的方法及学习到的模型。每个Transformer都会是西安一个transfor()的方法，用于将一种类型的DataFrame转换为另外一种类型的DataFrame。通常的做法是在DataFarme中追加一到多列
  - 一个特征转换器可以接受一个DataFrame，读取DataFrame上的列，通过定义的方法映射到新的列，并且返回带有新的列的DataFrame对象
  - 一个学习模型可以接受一个DataFrame，读取包含特征向量的列，为每个特征向量预测标签，输入带有标签的新的DataFrame对象

4. **Estimator**  
**Estimator**是一个学习算法的抽象，在Estimator中实现fit方法，这个方法接受DataFrame对象并产生一个模型，而产生的这个模型是一个Transformer类型，也是Model类型。不管是Transformer中的tranform方法还是Estimator中的fit方法，他们都是无状态的，并且每个Transformer和Estimator实例都带有一个唯一ID

5. **Pipeline**
在机器学习领域中，为了从数据中学习到某种模型，运行一个序列的包含多种算法的场景是很常见的。例如简单的文本分类可能会有如下的不同流程：  
 - 拆分句子为一个一个的单词
 - 转换文本中的单词到一个数值向量
 - 用特征向量学习一个模型，并且用于预测对应的标签  
MLlib中用Pipeline去代表这种序列，它是工作流的抽象，它包含一个序列的PipelineStages，这些PipelineStages以特定的顺序运行

6. **Paramenters**  
参数使用ParaMap来包装，参数以键值对形式存储，有两种方式传递参数给算法  
 - 为实例设置参数，例如lr是一个逻辑回归实例，可以调用实例上的方法例如setMaxIter(100)去设置最大的迭代次数为100
 - 另外一种方式是传递ParamMap给fit()或transform()方法，通过这种方式在ParamMap中定义的参数会覆盖之前通过setXX设置的参数
 
这些设置的参数属于特定Estimator或Transformer的实例，例如可以在一个ParaMap中为两个Estimator设置最大迭代次数。lr1、lr2的参数可以这样设置  
ParamMap(lr1.maxIter -> 10, lr2.maxIter -> 20)  
这是非常有用的，例如在一个Pipeline中有两个Estimator需要设置maxIter的时候

7. 一个Estimator、Transformer、Param的具体例子  

~~~python

from pyspark.sql import SparkSession
from pyspark.ml.linalg import Vectors
from pyspark.ml.classification import LogisticRegression

spark = SparkSession.builder.master('local').appName('Pipelines').getOrCreate()
sc = spark.sparkContext
training = spark.createDataFrame([(1.0, Vectors.dense([0.0, 1.1, 0.1])),
                                  (0.0, Vectors.dense([2.0, 1.0, -1.0])),
                                  (0.0, Vectors.dense([2.0, 1.3, 1.0])),
                                  (1.0, Vectors.dense(0.0, 1.2, -0.5))],
                                 ['label', 'features'])

lr = LogisticRegression(maxIter=10, regParam=0.01)
print('LogisticRegression parameters: \n' + lr.explainParams() + '\n')
>>>  

LogisticRegression parameters: 
aggregationDepth: suggested depth for treeAggregate (>= 2). (default: 2)
elasticNetParam: the ElasticNet mixing parameter, in range [0, 1]. For alpha = 0, the penalty is an L2 penalty. For alpha = 1, it is an L1 penalty. (default: 0.0)
family: The name of family which is a description of the label distribution to be used in the model. Supported options: auto, binomial, multinomial (default: auto)
featuresCol: features column name. (default: features)
fitIntercept: whether to fit an intercept term. (default: True)
labelCol: label column name. (default: label)
lowerBoundsOnCoefficients: The lower bounds on coefficients if fitting under bound constrained optimization. The bound matrix must be compatible with the shape (1, number of features) for binomial regression, or (number of classes, number of features) for multinomial regression. (undefined)
lowerBoundsOnIntercepts: The lower bounds on intercepts if fitting under bound constrained optimization. The bounds vector size must beequal with 1 for binomial regression, or the number oflasses for multinomial regression. (undefined)
maxIter: max number of iterations (>= 0). (default: 100, current: 10)
predictionCol: prediction column name. (default: prediction)
probabilityCol: Column name for predicted class conditional probabilities. Note: Not all models output well-calibrated probability estimates! These probabilities should be treated as confidences, not precise probabilities. (default: probability)
rawPredictionCol: raw prediction (a.k.a. confidence) column name. (default: rawPrediction)
regParam: regularization parameter (>= 0). (default: 0.0, current: 0.01)
standardization: whether to standardize the training features before fitting the model. (default: True)
threshold: Threshold in binary classification prediction, in range [0, 1]. If threshold and thresholds are both set, they must match.e.g. if threshold is p, then thresholds must be equal to [1-p, p]. (default: 0.5)
thresholds: Thresholds in multi-class classification to adjust the probability of predicting each class. Array must have length equal to the number of classes, with values > 0, excepting that at most one value may be 0. The class with largest value p/t is predicted, where p is the original probability of that class and t is the class's threshold. (undefined)
tol: the convergence tolerance for iterative algorithms (>= 0). (default: 1e-06)
upperBoundsOnCoefficients: The upper bounds on coefficients if fitting under bound constrained optimization. The bound matrix must be compatible with the shape (1, number of features) for binomial regression, or (number of classes, number of features) for multinomial regression. (undefined)
upperBoundsOnIntercepts: The upper bounds on intercepts if fitting under bound constrained optimization. The bound vector size must be equal with 1 for binomial regression, or the number of classes for multinomial regression. (undefined)
weightCol: weight column name. If this is not set or empty, we treat all instance weights as 1.0. (undefined)

model1 = lr.fit(training)
print('Model1 was fit using parameters:')
print(model1.extractParamMap())
>>> 

Model1 was fit using parameters:
{Param(parent='LogisticRegression_498eaa9f7f35507882b9', name='aggregationDepth', doc='suggested depth for treeAggregate (>= 2)'): 2, Param(parent='LogisticRegression_498eaa9f7f35507882b9', name='elasticNetParam', doc='the ElasticNet mixing parameter, in range [0, 1]. For alpha = 0, the penalty is an L2 penalty. For alpha = 1, it is an L1 penalty'): 0.0, Param(parent='LogisticRegression_498eaa9f7f35507882b9', name='family', doc='The name of family which is a description of the label distribution to be used in the model. Supported options: auto, binomial, multinomial.'): 'auto', Param(parent='LogisticRegression_498eaa9f7f35507882b9', name='featuresCol', doc='features column name'): 'features', Param(parent='LogisticRegression_498eaa9f7f35507882b9', name='fitIntercept', doc='whether to fit an intercept term'): True, Param(parent='LogisticRegression_498eaa9f7f35507882b9', name='labelCol', doc='label column name'): 'label', Param(parent='LogisticRegression_498eaa9f7f35507882b9', name='maxIter', doc='maximum number of iterations (>= 0)'): 10, Param(parent='LogisticRegression_498eaa9f7f35507882b9', name='predictionCol', doc='prediction column name'): 'prediction', Param(parent='LogisticRegression_498eaa9f7f35507882b9', name='probabilityCol', doc='Column name for predicted class conditional probabilities. Note: Not all models output well-calibrated probability estimates! These probabilities should be treated as confidences, not precise probabilities'): 'probability', Param(parent='LogisticRegression_498eaa9f7f35507882b9', name='rawPredictionCol', doc='raw prediction (a.k.a. confidence) column name'): 'rawPrediction', Param(parent='LogisticRegression_498eaa9f7f35507882b9', name='regParam', doc='regularization parameter (>= 0)'): 0.01, Param(parent='LogisticRegression_498eaa9f7f35507882b9', name='standardization', doc='whether to standardize the training features before fitting the model'): True, Param(parent='LogisticRegression_498eaa9f7f35507882b9', name='threshold', doc='threshold in binary classification prediction, in range [0, 1]'): 0.5, Param(parent='LogisticRegression_498eaa9f7f35507882b9', name='tol', doc='the convergence tolerance for iterative algorithms (>= 0)'): 1e-06}

~~~
~~~python

paramMap = {lr.maxIter: 2}
paramMap[lr.maxIter] = 3
paramMap.update({lr.regParam: 0.1, lr.threshold: 0.55})
paramMap2 = {lr.probabilityCol: 'p'}
paramMapCombined = paramMap.copy()
paramMapCombined.update(paramMap2)
model2 = lr.fit(training, paramMapCombined)
print(model2.extractParamMap())
>>> 
{Param(parent='LogisticRegression_498eaa9f7f35507882b9', name='aggregationDepth', doc='suggested depth for treeAggregate (>= 2)'): 2, Param(parent='LogisticRegression_498eaa9f7f35507882b9', name='elasticNetParam', doc='the ElasticNet mixing parameter, in range [0, 1]. For alpha = 0, the penalty is an L2 penalty. For alpha = 1, it is an L1 penalty'): 0.0, Param(parent='LogisticRegression_498eaa9f7f35507882b9', name='family', doc='The name of family which is a description of the label distribution to be used in the model. Supported options: auto, binomial, multinomial.'): 'auto', Param(parent='LogisticRegression_498eaa9f7f35507882b9', name='featuresCol', doc='features column name'): 'features', Param(parent='LogisticRegression_498eaa9f7f35507882b9', name='fitIntercept', doc='whether to fit an intercept term'): True, Param(parent='LogisticRegression_498eaa9f7f35507882b9', name='labelCol', doc='label column name'): 'label', Param(parent='LogisticRegression_498eaa9f7f35507882b9', name='maxIter', doc='maximum number of iterations (>= 0)'): 3, Param(parent='LogisticRegression_498eaa9f7f35507882b9', name='predictionCol', doc='prediction column name'): 'prediction', Param(parent='LogisticRegression_498eaa9f7f35507882b9', name='probabilityCol', doc='Column name for predicted class conditional probabilities. Note: Not all models output well-calibrated probability estimates! These probabilities should be treated as confidences, not precise probabilities'): 'p', Param(parent='LogisticRegression_498eaa9f7f35507882b9', name='rawPredictionCol', doc='raw prediction (a.k.a. confidence) column name'): 'rawPrediction', Param(parent='LogisticRegression_498eaa9f7f35507882b9', name='regParam', doc='regularization parameter (>= 0)'): 0.1, Param(parent='LogisticRegression_498eaa9f7f35507882b9', name='standardization', doc='whether to standardize the training features before fitting the model'): True, Param(parent='LogisticRegression_498eaa9f7f35507882b9', name='threshold', doc='threshold in binary classification prediction, in range [0, 1]'): 0.55, Param(parent='LogisticRegression_498eaa9f7f35507882b9', name='tol', doc='the convergence tolerance for iterative algorithms (>= 0)'): 1e-06}

test = spark.createDataFrame([(1.0, Vectors.dense([-1.0, 1.5, 1.3])),
                              (3.0, Vectors.dense([3.0, 2.0, -0.1])),
                              (1.0, Vectors.dense([0.0, 2.2, 1.5]))],
                             ['label', 'features'])

prediction = model2.transform(test)
result = prediction.select('features', 'label', 'p', 'prediction').collect()
for row in result:
    print('features=%s, label=%s -> prob=%s, prediction=%s' %
          (row.features, row.label, row.p, row.prediction))
>>> 
features=[-1.0,1.5,1.3], label=1.0 -> prob=[0.0350513223375,0.964948677662], prediction=1.0
features=[3.0,2.0,-0.1], label=3.0 -> prob=[0.840662466456,0.159337533544], prediction=0.0
features=[0.0,2.2,1.5], label=1.0 -> prob=[0.0608276805058,0.939172319494], prediction=1.0
~~~

另一文本处理例子：

~~~python
from pyspark.ml import Pipeline
from pyspark.ml.classification import LogisticRegression
from pyspark.ml.feature import HashingTF, Tokenizer

training = spark.createDataFrame([(0, 'a b c d e spark', 1.0), (1, 'b d', 0.0),
                                  (2, 'spark f g h', 1.0),
                                  (3, 'hadoop mapreduce', 0.0)],
                                 ['id', 'text', 'label'])

tokenizer = Tokenizer(inputCol='text', outputCol='words')
hashingTF = HashingTF(inputCol=tokenizer.getOutputCol(), outputCol='features')
lr = LogisticRegression(maxIter=2, regParam=0.001)
pipeline = Pipeline(stages=[tokenizer, hashingTF, lr])
model = pipeline.fit(training)

test = spark.createDataFrame([(4, 'spark i j k'), (5, 'l m n'),
                              (6, 'spark hadoop spark'), (7, 'apache hadoop')],
                             ['id', 'text'])

prediction = model.transform(test)
selected = prediction.select('id', 'text', 'probability', 'prediction')
for row in selected.collect():
    rid, text, prob, prediction = row
    print("('%d, %s') --> prob=%s, prediction=%f" %
          (rid, text, str(prob), prediction))
>>> 

('4, spark i j k') --> prob=[0.263481943245,0.736518056755], prediction=1.000000
('5, l m n') --> prob=[0.555472863599,0.444527136401], prediction=0.000000
('6, spark hadoop spark') --> prob=[0.231923570975,0.768076429025], prediction=1.000000
('7, apache hadoop') --> prob=[0.786513077856,0.213486922144], prediction=0.000000
~~~
