
1. SparkSQL是Spark的四大组件之一，也是Spark数据处理中用的最多的组件  
SparkSQL在SparkCore的基础上对外提供了SQL接口，可以让熟悉SQL的技术人员快速上手。其编程接口为SparkSession

2. SparkSQL所有的内容位于pyspark.sql这个模块下，包括了SparkSession、Column、Row等众多的核心内容，SparkSQL是面向结构话数据的利刃，处理表格如常规的关系型数据库一样，不管是体统的丰富的API还是执行的效率上，都有很大的优势，当然对于非结构化的数据，还得借助RDD来进行基本的处理

3. SparkSQL中的DataFrame是一个类似于Pandas中的DataFrame的表格对象，其低层基于RDD，但是他在RDD的基础上提供了表格的例如comumn、row，索引等信息，因此使用基于DataFrame的API编写除的Spark程序在性能上要优于基于RDD编写出来的程序。还有一个大的优势是，迎合了喝多本来就会SQL的程序员的胃口，使他们非常容易上手使用

4. SparkSQL这个模块之前是使用的基于Hive的SQL解析器，不利于优化，随着项目的发展，Spark团队逐渐抛弃了Hive解析器，开发Spark特有的SQL解析器，使得SparkSQL在灵活性和速度上在上一层楼。在SparkSession的builder上有一个方法叫enableHiveSupport，用于启动对Hive的查询的支持，因此在使用SparkSQL的过程中，可以天然无缝的查询Hive数据仓库中的数据

5. SparkSQL底层使用了Catalyst优化器对SQL语句进行优化，同时底层使用了Tungsten这个项目的成果，Tungsten项目聚集于CPU和Memory的优化，借助Catalyst和Tungsten，使得SparkSQL的执行速度进一步提升
![image.png](attachment:image.png)

6. 普通的RDD和DataFrame可以通过相关的方法进行互相转换，因此使得基于DataFrame的接口功能异常强大而灵活，SparkSQL也提供了用户自定义函数的支持，非常方便的实现自定义的功能，在SQL中使用起来非常方便

7. SparkSQL可以读取几乎所有格式的文件，形成DataFrame，经过业务处理之后，可以非常方便的通过write接口将结果写到文件、关系型数据库、非关系型数据库

### SparkSQL编程入口SparkSession
1. 要编写SparkSQL程序，必须通过SparkSession对象    
pyspark.sql.SparkSession(sparkContext, sparkSession=None)  
在spark1.x之前的版本中，SparkSQL程序的编程入口是  
pyspark.sql.SQLContext(sparkContext, sparkSession=None, sqlContext=None)  
还有一个pyspark.sql.HiveContext(sparkContext, hiveContext=None),用于整合Hive数据仓库的，读取Hive表格需要借助HiveContext  


2. 在pyspark这个类中，有builder，通过builder去构建SparkSession实例，用法如下

~~~python
from pyspark.sql import SparkSession

spark = SparkSession.builder.master("spark://hadoop-maste:7077") \
                    .appName('test').config("spark.xxxx.conf", 
                                            "some-value").getOrCreate()
~~~
- master指定spark集群的地址（‘local’：本地；）  
- appName用于设置app的名称  
- config中以key，value的形式进行一些设置  
- config可以以链式编程的方式多次条用，每次调用可以设置一组key，value配置，而且conf中还可以传入一个关键字参数conf，指定外部的SparkConf配置对象  
- getOrCreate(),若存在sparkSession实例直接返回，否则实例化一个sparkSession返回  
- enableHiveSupport启用对hive的支持

这些配置可以通过SparkSession上的conf属性来获取，这个conf实际上就是RuntimeConfig对象，通过conf上的get方法获取对应的配置和通过spark.sparkContext.getConf().get获取配置是类似的

3. 创建DataFrame的几种常见的方式  
createDataFrame(data, schema=None, samplingRatio=None, verifySchema=True)这个方法。可以使用RDD或者python list获取pandas.DataFrame来创建  
当shcema被指定的时候，每个列的类型将会通过列来进行推断  
samplingRatio指定用于类型推断样本的比例  
verifySchema验证每个每行的数据类型是否符合schema的定义

a.使用二值tuple创建DataFrame，注意对比有schma和没有指定schema的区别
~~~python
l = [('tom',14),('jerry',20)]
spark.createDataFrame(l).collect()
>>> [Row(_1='tom', _2=14), Row(_1='jerry', _2=20)]
spark.createDataFrame(l, schema=['name', 'age']).collect()
>>> [Row(name='tom', age=14), Row(name='jerry', age=20)]
~~~

b.通过使用dict字典创建DataFrame，通过字典不用指定schema，因为字典中的key就是schema  

~~~python
d = [{'name':'tom',"age":14},{'name':'jerry',"age":20}]
spark.createDataFrame(d,schema=['name',"age"]).collect()
>>> [Row(name=14, age='tom'), Row(name=20, age='jerry')]
~~~

c.通过RDD来创建DataFrame  

~~~python
r = [('tom',14),('jerry',20)]

rdd = SparkContext.parallelize(r, 3)
df = spark.createDataFrame(rdd)
df1 = spark.createDataFrame(rdd, schema=['name','age'])
df.collect()
df1.collect()
~~~

d.通过在RDD上构建Row对象来构建DataFrame  

~~~python
from pyspark.sql import Row


sc = spark.sparkContext
Person = Row('name', 'age')
l = [("tom",14),("jerry",20)]
rdd = sc.parallelize(l,3)
person = rdd.map(lambda r: Person(*r))
df2 = spark.createDataFrame(person)
df2.collect()
>>> [Row(name='tom', age=14), Row(name='jerry', age=20)]
~~~

e.通过传入RDD和schema信息来构建DataFrame  

~~~python
from pyspark.sql.types import *


schema = StructType([StructField("name", StringType(),True),
                    StructField("age", IntegerType(),True)])
l = [('tom', 14), ('jerry',20)]
rdd = sc.parallelize(l, 3)
df = spark.createDataFrame(rdd, schema)
df.collect()
>>> [Row(name='tom', age=14), Row(name='jerry', age=20)]
~~~

f.通过Pandas中DataFrame来创建DataFrame 

~~~python
import pandas as pd
pandas_df = pd.DataFrame([{
    'name': 'tom',
    "age": 14
}, {
    'name': 'jerry',
    'age': 20
}])
pandas_df.head()
spark.createDataFrame(pandas_df).collect()
>>> [Row(age=14, name='tom'), Row(age=20, name='jerry')]
~~~

g.schema还可以用字符串“名字：类型” 多个列用逗号分隔表示

~~~python
l = [('tom',14), ('jerry',20)]
rdd = sc.parallelize(l, 3)
spark.createDataFrame(rdd, 'a:string, b:int').collect()
>>> [Row(a='tom', b=14), Row(a='jerry', b=20)]
~~~
通过schema还可以指定字段类型，当指定的字段类型和自动推断出来的类型不一致时，会报错  

~~~python
rdd = rdd.map(lambda row: row[1])
spark.createDataFrame(rdd, 'int').collect()
>>> [Row(value=14), Row(value=20)]
# 实际类型int类型，却指定boolean
spark.createDataFrame(rdd, 'boolean').collect()
>>> field value: BooleanType can not accept object 14 in type <class 'int'>
~~~

4. DataFrame和RDD一样可以通过读取文件，或者通过transformation转化得到。在sparksession的read模块中有非常多的读取文件的方法  
通过read模块可以读取csv/jdbc/json/partquet/text等格式的文件

~~~python
dir(spark.read)
>>> ['__class__','__delattr__','__dict__','__dir__','__doc__','__eq__','__format__','__ge__','__getattribute__','__gt__','__hash__','__init__','__init_subclass__','__le__','__lt__','__module__','__ne__','__new__','__reduce__','__reduce_ex__','__repr__','__setattr__','__sizeof__','__str__','__subclasshook__','__weakref__','_df','_jreader','_set_opts','_spark','csv','format','jdbc','json','load','option','options','orc','parquet','schema','table','text']
~~~

5. newSession()这个方法会返回一个新的SparkSession，他们公用一个sparkcontext，但时注册的临时表，UDF这些是彼此隔离的。  

~~~python
spark1 = spark.newSession()
spark1 == spark
>>> False
conf = spark.conf
conf1 = spark1.conf
conf == conf1
>>> False
context = spark.sparkContext
context1 = spark1.sparkContext
context == context1
>>> True
udf = spark.udf
udf1 = spark1.udf
udf == udf1
>>> False
~~~

6. range(start, end=None, step=1, numPartitions=None)该方法创建的DataFrame，列标签为id，类型为pysprk.sql.types.LongType,numPartitions用于指定分区数  

~~~python
df = spark.range(100, 1000, 37, 3)
df.rdd.getNumPartitions()
df.collect()
>>> [Row(id=100),Row(id=137),Row(id=174),Row(id=211),Row(id=248),Row(id=285),Row(id=322),Row(id=359),Row(id=396),Row(id=433),Row(id=470),Row(id=507),Row(id=544),Row(id=581),Row(id=618),Row(id=655),Row(id=692),Row(id=729),Row(id=766),Row(id=803),Row(id=840),Row(id=877),Row(id=914),Row(id=951),Row(id=988)]
~~~

可以用DataFrame上的show()方法，显示效果通Pandas中的show方法类似

~~~python
df.show(5)
>>> +---+
| id|
+---+
|100|
|137|
|174|
|211|
|248|
+---+
only showing top 5 rows
~~~

7. read该方法返回DataFrameReader对象，通过这个对象可以读取外部文件来创建DataFrame

~~~python
spark.read
>>> <pyspark.sql.readwriter.DataFrameReader at 0x14d28bc1978>
~~~

8. readStream该方法返回DataFrameReader对象，该对象用于读取stream数据

~~~python
spark.readStream
>>> <pyspark.sql.streaming.DataStreamReader at 0x14d28bc1b00>
~~~

9. sparkContext返回底层的sparkContext对象

~~~python
sc = spark.sparkContext
sc.master
>>> 'spark://hadoop-maste:7077'
~~~

10. sql(sqlQuery),使用SQL语句从表格或者视图中查询数据

~~~python
df = spark.range(10, 18, 2, 3)
df.show(5)
>>> +---+
| id|
+---+
| 10|
| 12|
| 14|
| 16|
+---+
## ----------------
df.createOrReplaceTempView('test')
spark.sql('select * from test where id >= 10').show()
>>> +---+
| id|
+---+
| 10|
| 12|
| 14|
| 16|
+---+
~~~

11. stop()停止底层sparkcontext


12. streams 返回StreamingQueryManager，该对象管理着在该sparksession中所有StreamingQuerry对象

~~~python
spark.streams
>>> <pyspark.sql.streaming.StreamingQueryManager at 0x14d2bf4aac8>
~~~

13. table(tableName),使用一个表格的名字，返回该表格对应的DataFrame对象

~~~python
df = spark.range(20)
df.createOrReplaceTempView('a')
spark.sql('select * from a where id > 10').show()
>>> 
+---+
| id|
+---+
| 11|
| 12|
| 13|
| 14|
| 15|
| 16|
| 17|
| 18|
| 19|
+---+
df1 = spark.table('a')
df1.collect() == df1.collect()
>>> True
~~~

14. udf返回UDFRegistration用于UDF函数的注册

~~~python
spark.udf
>>> <pyspark.sql.udf.UDFRegistration at 0x14d28c1d358>
~~~

15. pyspark.sql.UDFRegistration(sqlConext),这个类正是用来注册自定义函数的类。这个类里面有一个最主要的方法，register(name, f, returnType=StringType),第一个参数为自定义函数的名字，第二个参数f表示要传入自定义的函数，函数里面定义了处理的逻辑，第三个关键字参数returnType用于定义返回值的类型，默认为StrinType。下面定义一个求n次方的方法，方法名字叫pow1(m, n),接收两个参数m,n.要在pow方法中实现m的n次方计算，返回DoubleType

~~~python
from pyspark.sql.types import *


def pow1(m, n):
    return float(m) ** float(n)

udf = spark.udf
udf.register('pow1', pow1, returnType=DoubleType())
df = spark.range(0, 10, 2, 3)
df.createOrReplaceTempView('a')
spark.sql('select pow1(id, 2) from a').show()
>>> 
+-----------+
|pow1(id, 2)|
+-----------+
|        0.0|
|        4.0|
|       16.0|
|       36.0|
|       64.0|
+-----------+
spark.sql('select pow1(id, 3) from a').show()
>>> 
+-----------+
|pow1(id, 3)|
+-----------+
|        0.0|
|        8.0|
|       64.0|
|      216.0|
|      512.0|
+-----------+
~~~
